{
  "hash": "6057ee4b2712e9f0013f4668536168a2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multilayer Perceptron\"\n---\n\n\n\n## Midterm Proposal presentation\n\n-  A group of 3-4 students.\n-  Let me know your group members next Tuesday. \n-  The presentation will be on 10/22.\n   -  Problem statement\n   -  Data description\n   -  1-2 references\n-  Each group has 15 minutes to present.\n-  Homework 1 is due on 10/15.\n\n## Recap of the Last Lecture\n\n\n::: {.hidden}\n$$\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n$$\n:::\n\n\n\nA general procedure for supervised learning:\n\n1.  Gather a dataset $\\mathcal{D} = \\{(\\boldsymbol{x}_1, y_1), \\ldots, (\\boldsymbol{x}_n, y_n)\\}$.\n2.  Choose an appropriate **loss function** $L$ for your dataset and problem.\n3.  Choose a **hypothesis class** $\\mathcal{H}$.\n    -  Not too simple (underfitting), nor too complicated (overfitting).\n4.  Add a **regularization term** to the loss function to better control the complexity of the model.\n5.  Find the model that minimizes the **(regularized) empirical risk function**.\n6.  Use cross-validation to select the hyperparameters (if any).\n7.  Estimate the generalization error of the final model using the test dataset.\n\n## Nonlinear Hypothesis Class\n\n-  A simple and useful hypothesis class is the linear hypothesis class, for example:\n   -  Linear regression: $\\mathcal{H} = \\{h(\\boldsymbol{x}) = \\boldsymbol{x}^T\\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}$\n   -  Logistic regression: $\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\} = \\R^p$\n-  To extend linear models to represent nonlinear functions of $\\boldsymbol{x}$, we can apply the linear model not to $\\boldsymbol{x}$ itself but to a transformed input $\\phi(\\boldsymbol{x})$, where $\\phi$ is a nonlinear transformation.\n-  That is,\n    $$\n    \\mathcal{H} = \\{ h(\\boldsymbol{x}) = \\phi(\\boldsymbol{x})^T \\boldsymbol{\\beta}, \\phi: \\R^p \\to \\R^d, \\boldsymbol{\\beta}\\in \\R^d\\}.\n    $$\n-  The function $\\phi$ is usually called a **feature map** and usually we choose $d \\gg p$.\n\n## Choice of Feature Map\n\nThere are three common ways to choose the feature map $\\phi$:\n\n1.  Kernel method\n    -   Instead of explicitly specifying $\\phi$, we can define a symmetric function $k(\\boldsymbol{x}, \\boldsymbol{x}')$ called a **kernel**, which corresponds to the dot product of some feature map $k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\phi(\\boldsymbol{x})^T \\phi(\\boldsymbol{x}^{\\prime})$.\n2.  Manual engineering (hand-crafted features)\n    -   Choose a feature map $\\phi$ manually, e.g., $\\phi(x) = [x, x^2, \\exp(x), \\sin(x)]$.\n    -   A good feature map requires human effort for each separate task, with practitioners specializing in different domains.\n3.  Deep Learning\n    -   Parametrize the feature map $\\phi$ with a deep neural network, $\\phi(\\boldsymbol{x}) = \\phi(\\boldsymbol{x}; \\boldsymbol{\\theta})$.\n    -   The parameter $\\boldsymbol{\\theta}$ is learned from the data, i.e., we learn the feature map from the data.\n\n## Outline\n\n-  Kernel Methods\n   -  Kernel Ridge Regression\n-  Multilayer Perceptron (MLP)\n   -  Basic Structure\n   -  Activation Function\n-  Training an MLP\n   -  Gradient-based Learning\n   -  Back-propagation\n   -  Computational Graph\n-  Example\n\n## Regression with a Feature Map\n\n-  Consider the regression model with a given feature map $\\phi: \\R^p \\to \\R^d$, i.e.,\n   $$\n   y = f(\\boldsymbol{x}) = \\phi(\\boldsymbol{x})^T \\boldsymbol{\\beta}.\n   $$ \n-  Given a training dataset $\\{(\\boldsymbol{x}_i, y_i)\\}_{i=1}^n$, let $\\boldsymbol{z_i} = \\phi(\\boldsymbol{x}_i)$ and $\\boldsymbol{Z} = [\\boldsymbol{z}_1, \\boldsymbol{z}_2, \\ldots, \\boldsymbol{z}_n]^T \\in \\R^{n \\times d}$.\n-  Then the model parameter $\\boldsymbol{\\beta}$ can be estimated by minimizing the regularized empirical risk function:\n    \\begin{align*}\n    \\hat{\\boldsymbol{\\beta}} & = \\argmin_{\\boldsymbol{\\beta} \\in \\R^d} \\|\\boldsymbol{y} - \\boldsymbol{Z}\\boldsymbol{\\beta}\\|^2 + \\alpha \\|\\boldsymbol{\\beta}\\|^2_2 = (\\boldsymbol{Z}^T \\boldsymbol{Z} + \\alpha I_d)^{-1} \\boldsymbol{Z}^T \\boldsymbol{y}\\\\\n    & \\stackrel{\\textcolor{red}{\\text{(*)}}}{=} \\boldsymbol{Z}^T (\\boldsymbol{Z} \\boldsymbol{Z}^T + \\alpha I_n)^{-1} \\boldsymbol{y}.\n    \\end{align*}\n-   The prediction for a new input $\\boldsymbol{x}^{\\star}$ is\n    $$\n    \\widehat{y^{\\star}} = \\phi(\\boldsymbol{x}^{\\star})^T \\hat{\\boldsymbol{\\beta}} = \\phi(\\boldsymbol{x}^{\\star})^T\\boldsymbol{Z}^T (\\boldsymbol{Z} \\boldsymbol{Z}^T + \\alpha I_n)^{-1} \\boldsymbol{y}.\n    $$\n\n\n::: aside\nSee appendix for the proof of equality [(*)]{style=\"color: red\"}.\n:::\n\n## Regression with a Feature Map\n\n-   Note that\n    $$\n    \\boldsymbol{Z} \\boldsymbol{Z}^T = \\begin{bmatrix} \\boldsymbol{z}_1^T \\\\ \\boldsymbol{z}_2^T \\\\ \\vdots \\\\ \\boldsymbol{z}_n^T \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{z}_1 & \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_n \\end{bmatrix} = \\begin{bmatrix}\n    \\boldsymbol{z}_1^T \\boldsymbol{z}_1 & \\boldsymbol{z}_1^T \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_1^T \\boldsymbol{z}_n \\\\\n    \\boldsymbol{z}_2^T \\boldsymbol{z}_1 & \\boldsymbol{z}_2^T \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_2^T \\boldsymbol{z}_n \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\boldsymbol{z}_n^T \\boldsymbol{z}_1 & \\boldsymbol{z}_n^T \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_n^T \\boldsymbol{z}_n\n    \\end{bmatrix} = \\left[\\phi(\\boldsymbol{x}_i)^T\\phi(\\boldsymbol{x}_j)\\right]_{i,j=1}^n\n    $$\n    and \n    $$\n    \\phi(\\boldsymbol{x}^{\\star})^T\\boldsymbol{Z}^T = \\begin{bmatrix} \\phi(\\boldsymbol{x}^{\\star})^T \\boldsymbol{z}_1 & \\phi(\\boldsymbol{x}^{\\star})^T \\boldsymbol{z}_2 & \\cdots & \\phi(\\boldsymbol{x}^{\\star})^T \\boldsymbol{z}_n \\end{bmatrix} = \\left[\\phi(\\boldsymbol{x}^{\\star})^T\\phi(\\boldsymbol{x}_i)\\right]_{i=1}^n.  \n    $$\n-   **BIG NEWS**: we don't need to know the feature map $\\phi$ explicitly, but only its inner product $\\phi(\\boldsymbol{x}_i)^T\\phi(\\boldsymbol{x}_j)$.\n-   That is, if we can [find a function $k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime})$ such that $k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\phi(\\boldsymbol{x})^T\\phi(\\boldsymbol{x}^{\\prime})$ for some $\\phi$]{style=\"color: red\"}, then we don't need to know $\\phi$ explicitly.\n-   The function $k$ is called a **kernel function** and the model is called a **kernel ridge regression**.\n-   **Mercer Condition**: If the function $k(\\cdot, \\cdot)$ is symmetric and positive definite, then there exist a $\\phi$ such that $k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\phi(\\boldsymbol{x})^T\\phi(\\boldsymbol{x}^{\\prime})$.\n  \n\n## Kernel Method vs Deep Learning\n\n-  Both kernel method and deep learning utilize nonlinear feature map to obtain a nonlinear hypothesis class.\n-  Kernel method use a kernel function (the inner product of the feature map) to [implicitly define the feature map]{style=\"color: blue\"}. Some common kernel functions include:\n   -  Linear kernel: $k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\boldsymbol{x}^T\\boldsymbol{x}^{\\prime} \\Rightarrow \\phi(\\boldsymbol{x}) = \\boldsymbol{x}$.\n   -  Polynomial kernel: $k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = (\\boldsymbol{x}^T\\boldsymbol{x}^{\\prime} + c)^d$.\n   -  Radial basis function (RBF) kernel: $k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\exp\\left(-\\gamma\\|\\boldsymbol{x} - \\boldsymbol{x}^{\\prime}\\|^2\\right)$.[^rbf]\n-  Deep learning uses a deep neural network to [parametrize the feature map]{style=\"color: blue\"}.\n\n[^rbf]: See the appendix for the feature map of the RBF kernel.\n\n## Example: Kernel Regression\n\nGenerate a dataset from the model $y = \\sin(2\\pi x) + x + \\epsilon$, where $\\epsilon \\sim N(0, 0.5^2)$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(100)\nn = 100\nX = rng.uniform(low=0, high=3, size=n)\ny = np.sin(2 * np.pi * X) + X + rng.normal(loc=0, scale=0.5, size=n)\nplt.scatter(X, y, c=\"black\", s=20)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](04-mlp_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n--- \n\nCompare four different models:\n\n-   Linear kernel (linear ridge regression, $\\alpha = 1$)\n-   RBF kernel with $\\gamma = 1$ and $\\gamma = 100$ (both with $\\alpha = 1$)\n-   RBF kernel with hyperparameters $\\alpha$ and $\\gamma$ selected by cross-validation.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\n\n# Ridge regression\nmod1 = KernelRidge(alpha = 1, kernel='linear').fit(X.reshape(-1, 1), y)\n\n# Kernel ridge regression with RBF kernel and different gamma (bandwidth)\nmod2 = KernelRidge(alpha = 1, kernel='rbf', gamma=1).fit(X.reshape(-1, 1), y)\nmod3 = KernelRidge(alpha = 1, kernel='rbf', gamma=100).fit(X.reshape(-1, 1), y)\n\n# Use cross-validation to select the hyperparameters\nmod4 = GridSearchCV(\n    KernelRidge(kernel=\"rbf\", gamma=0.1),\n    param_grid={\"alpha\": [1, 0.1, 0.01, 0.001], \"gamma\": np.logspace(-2, 2, 5)},\n).fit(X.reshape(-1, 1), y)\n```\n:::\n\n\n\n---\n\n-   Small value of $\\gamma$ leads to underfitting and large value of $\\gamma$ leads to overfitting.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-mlp_files/figure-revealjs/unnamed-chunk-3-3.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Multilayer Perceptron (MLP)\n\n-  It is also called a **feedforward neural network** because information flows through the function being evaluated from $\\boldsymbol{x}$, through the intermediate computations used to define $f$ , and finally to the output $y$.\n-  When **feedback** connections are included, they are called **recurrent neural networks**.\n\n![](images/lec04/perceptron.png){fig-align=\"center\" width=30%}\n\n## Multilayer Perceptron (MLP)\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n![](images/lec04/neural_network-0.png){fig-align=\"center\" width=100%}\n:::\n\n::: {.column width=\"60%\"}\n-   Input: $\\boldsymbol{x} = [x_1, x_2, \\ldots, x_p]^T \\in \\R^p$.\n-   Hidden Units:\n    \\begin{align*}\n    h_i & = \\sigma\\left(\\sum_{j=1}^p w_{ij} x_j + b_i\\right), \\quad i = 1, \\ldots, m, \\\\ \n        & = \\sigma\\left(\\boldsymbol{w}_i^T \\boldsymbol{x} + b_i\\right), \\quad i = 1, \\ldots, m, \\\\\n    \\boldsymbol{w}_i & = [w_{i1}, w_{i2}, \\ldots, w_{ip}]^T \\in \\R^p, \\quad b_i \\in \\R.\n    \\end{align*}\n-   Output: \n    \\begin{align*}\n    y & = \\beta_0 + \\sum_{j=1}^m \\beta_{j} h_j = \\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{h},\\\\\n    \\boldsymbol{h} & = [h_{1}, h_{2}, \\ldots, h_{m}]^T \\in \\R^m, \\\\\n    \\boldsymbol{\\beta} & = [\\beta_{1}, \\beta_{2}, \\ldots, \\beta_{m}]^T \\in \\R^m, \\quad \\beta_0 \\in \\R.\n    \\end{align*}\n:::\n\n::::\n\n## Feature Map in MLP\n\n-   The relationship between the input $\\boldsymbol{x}$ and the hidden unit $\\boldsymbol{h}$ is\n    $$\n    \\boldsymbol{h} = \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right),\n    $$\n    where\n    $$\n    \\boldsymbol{W} = \\begin{bmatrix} \\boldsymbol{w}_1^T \\\\ \\boldsymbol{w}_2^T \\\\ \\vdots \\\\ \\boldsymbol{w}_m^T \\end{bmatrix} \\in \\R^{m \\times p}, \\quad \\boldsymbol{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix} \\in \\R^m.\n    $$\n-   The function $\\sigma: \\R \\to \\R$ is called an **activation function** and is applied element-wise to the vector $\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}$.\n-   Tha map $\\boldsymbol{x} \\mapsto \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right)$ can be viewed as a feature map parametrized by $\\boldsymbol{W}$ and $\\boldsymbol{b}$.\n-   That is, we replace the linear predictor $\\beta_0 + \\boldsymbol{x}^T \\boldsymbol{\\beta}$ with $\\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{h} = \\beta_0 + \\boldsymbol{\\beta}^T \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right)$.\n\n-   Finally, we can link the predictor $\\beta_0 + \\boldsymbol{\\beta}^T \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right)$ to the output $y$ (or more specifically $\\E(Y \\mid \\boldsymbol{x})$) using a link function, e.g., identity, logit, or softmax. \n\n\n## Activation function\n\n-   The main role of the activation function is to introduce nonlinearity into the model.\n-   If $\\sigma(x) = x$, then the MLP is equivalent to a linear model since\n    \\begin{align*}\n    \\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{h} & = \\beta_0 + \\boldsymbol{\\beta}^T \\sigma(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}) = \\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{\\beta}^T \\boldsymbol{b}\\\\\n    & = \\left(\\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{b}\\right) + \\left(\\boldsymbol{\\beta}^T \\boldsymbol{W}\\right) \\boldsymbol{x} = \\tilde{\\beta}_0 + \\tilde{\\boldsymbol{\\beta}}^T \\boldsymbol{x}.\n    \\end{align*}\n-   From this perspective, $\\sigma$ can be any nonlinear function.\n-   However, the choice of activation function has a crucial impact on training, especially when the neural network is deep, i.e., it has many hidden layers.\n-   Common choices of activation functions include:\n    -   Rectified Linear Unit (ReLU): $\\sigma(x) = \\max(0, x)$\n    -   Logistic: $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$\n    -   Hyperbolic Tangent (tanh): $\\sigma(x) = \\tanh(x)$\n\n## Activation Functions\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-mlp_files/figure-revealjs/unnamed-chunk-4-5.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## More Hidden Layers\n\n-   Increasing the number of hidden layers is straightforward and it allows the model to learn more complex functions.\n-   The MLP is also called a **fully connected neural network (FCNN)** because we have connections between all the units in adjacent layers.\n\n![](images/lec04/neural_network-1.png){fig-align=\"center\" width=50%}\n\n## The general form of an FCNN\n\n-   An $L$-layer FCNN (the $L$th layer is the output layer) can be written recursively as\n    $$\n    f^{(L)}(\\boldsymbol{x}) = \\boldsymbol{W}^{(L)}\\boldsymbol{h}^{(L-1)} +  \\boldsymbol{b}^{(L)} \\in \\R^k,\n    $$\n    where\n    $$\n    \\boldsymbol{h}^{(l)} = \\sigma^{(l)}\\left(\\boldsymbol{W}^{(l)} \\boldsymbol{h}^{(l-1)} + \\boldsymbol{b}^{(l)}\\right), \\quad l = 1, \\ldots, L-1,\n    $$\n    and $\\boldsymbol{h}^{(0)} = \\boldsymbol{x} \\in \\R^p$.\n-   The function $\\sigma^{(l)}$ is the activation function for the $l$-th layer. Typically, we use the same activation function for all layers.\n-   The parameter of the model is $\\theta = \\{\\boldsymbol{W}^{(1)}, \\ldots, \\boldsymbol{W}^{(L)}, \\boldsymbol{b}^{(1)}, \\ldots, \\boldsymbol{b}^{(L)}\\}$.\n-   Denote the number of nodes in the $l$th layer by $m_l$ ($m_0 = p$ and $m_{L} = k$), i.e., $\\boldsymbol{h}^{(l)} \\in \\R^{m_l}$.\n-   Then $\\boldsymbol{W}^{(l)} \\in \\R^{m_l \\times m_{l-1}}$ and $\\boldsymbol{b}^{(l)} \\in \\R^{m_l}$ and the total number of parameters is\n    $$\n        \\sum_{i=1}^{L} m_l\\cdot m_{l-1} + m_l = \\sum_{i=1}^{L} m_l(m_{l-1} + 1).\n    $$\n\n\n# Back-propagation\n\n## Training a Neural Network\n\n-   Suppose now we have a dataset $\\mathcal{D} = \\{(\\boldsymbol{x}_1, y_1), \\ldots, (\\boldsymbol{x}_n, y_n)\\}$ and we have chosen a hypothesis class $\\mathcal{H}^{(L)}$ consisting of $L$-layer FCNNs.\n-   Based on the dataset, we can also specify an appropriate loss function $\\ell$.\n-   Following the ERM principle, we want to find the model $\\hat{f} \\in \\mathcal{H}^{(L)}$ that minimizes the empirical risk function\n    $$\n    \\hat{f} = \\argmin_{f \\in \\mathcal{H}^{(L)}} \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f(\\boldsymbol{x}_i)).\n    $$ \n-   Since $\\mathcal{H}^{(L)}$ is parametrized by the weights and biases, we need to find the optimal weights and biases that minimize the empirical risk function.\n-   Let $J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f_{\\theta}(\\boldsymbol{x}_i))$. The goal is to find $\\theta$ that minimizes $J(\\theta)$. \n\n## Gradient Descent\n\n-   The gardient descent algorithm is a simple and widely used optimization algorithm for finding the minimum of a function.\n-   Let $g: \\R^d \\to \\R$ be a differentiable function. The gradient of $g$ at $\\boldsymbol{x} \\in \\R^d$ is the vector of partial derivatives of $g$ at $\\boldsymbol{x}$:\n    $$\n    \\nabla g(\\boldsymbol{x}) = \\left[\\frac{\\partial}{\\partial x_1} g(\\boldsymbol{x}), \\ldots, \\frac{\\partial}{\\partial x_d} g(\\boldsymbol{x})\\right]^T.\n    $$\n-   Consider the first-order Taylor expansion of $g$ at $\\boldsymbol{x}$:\n    $$\n    g(\\boldsymbol{x} + \\boldsymbol{\\epsilon}) = g(\\boldsymbol{x}) + \\boldsymbol{\\epsilon} ^T\\nabla g(\\boldsymbol{x})  + O(\\|\\boldsymbol{\\epsilon}\\|^2).\n    $$\n-   When $\\|\\boldsymbol{\\epsilon}\\|$ is small, we can approximate $g(\\boldsymbol{x} + \\boldsymbol{\\epsilon})$ by $g(\\boldsymbol{x}) + \\boldsymbol{\\epsilon} ^T\\nabla g(\\boldsymbol{x})$.\n-   Taking $\\boldsymbol{\\epsilon} = -\\eta \\nabla g(\\boldsymbol{x})$ for some small $\\eta > 0$, we have\n    $$\n    g(\\boldsymbol{x} - \\eta \\nabla g(\\boldsymbol{x})) \\approx g(\\boldsymbol{x}) - \\eta \\|\\nabla g(\\boldsymbol{x})\\|^2 \\leq g(\\boldsymbol{x}).\n    $$\n-   That is, if we move in the **opposite** direction of the gradient **by an appropriate distance**, we can decrease the value of $g$. This is called the **gradient descent** algorithm.\n\n---\n\n\n![](images/lec04/GD_algorithm.png){fig-align=\"center\" width=100%}\n\n-   You can also stop the algorithm when the gradient is small enough, i.e., $\\|\\nabla g(\\boldsymbol{x})\\| < \\epsilon$ for some small $\\epsilon > 0$.\n\n## Example: Gradient Descent\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-mlp_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Gradient Descent\n\n-   The gradient descent/ascent algorithm is the most simple optimization algorithm for finding the minimum/maximum of a function.\n-   It is a **first-order optimization** algorithm that uses only the first-order derivative information.\n-   The extra parameter $\\eta$ is called the **learning rate**, which controls the step size of the algorithm.\n-   Learning rate is a crucial hyperparameter in the gradient descent algorithm as it determines the convergence rate and the stability of the algorithm.\n-   If the learning rate is too small, the algorithm may converge very slowly. If it is too large, the algorithm may diverge.\n-   There are also second-order optimization algorithms that use the second-order derivative information (the Hessian matrix), e.g., Newton's method, IRWLS.\n-   Modern deep learning frameworks use variants of gradient descent algorithms, e.g., Adam, RMSprop, Adagrad, etc.\n\n## Train a Neural Network using GD\n\n-   Recall that the goal is to find the optimal weights and biases that minimize the empirical risk function $J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f_{\\theta}(\\boldsymbol{x}_i))$.\n-   To apply the gradient descent algorithm, we need to compute the gradient of $J(\\theta)$ with respect to the weights and biases.\n-   For simplicity, we assume that $L = 2$ (one hidden layer, $m_0 = p$, $m_1 = m$, $m_2 = 1$), i.e.,\n    $$\n    f_{\\theta}(\\boldsymbol{x}) = \\boldsymbol{W}^{(2)}\\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right) + \\boldsymbol{b}^{(2)}.\n    $$\n-   The parameters are $\\theta = \\{\\boldsymbol{W}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{b}^{(1)}, \\boldsymbol{b}^{(2)}\\}$ where $\\boldsymbol{W}^{(1)} \\in \\R^{m \\times p}$, $\\boldsymbol{W}^{(2)} \\in \\R^{1 \\times m}$, $\\boldsymbol{b}^{(1)} \\in \\R^m$, and $\\boldsymbol{b}^{(2)} \\in \\R$.\n\n--- \n\n-   MSE loss:\n    $$\n    J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_{\\theta}(\\boldsymbol{x}_i))^2, \\quad \\nabla J(\\theta) = -\\frac{2}{n}\\sum_{i=1}^n(y_i - f_{\\theta}(\\boldsymbol{x}_i)) \\nabla f_{\\theta}(\\boldsymbol{x}_i).\n    $$\n-   Cross-entropy loss (classification):\n    \\begin{align*}\n    J(\\theta) & = -\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\log p_{\\theta}(\\boldsymbol{x}_i) + (1 - y_i) \\log\\left(1 - p_{\\theta}(\\boldsymbol{x}_i) \\right)\\right], \\qquad p_{\\theta}(\\boldsymbol{x}_i) = \\frac{1}{1+\\exp(-f_{\\theta}(\\boldsymbol{x}_i))}\\\\\n    & = -\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\log\\left(\\frac{p_{\\theta}(\\boldsymbol{x}_i)}{1 - p_{\\theta}(\\boldsymbol{x}_i)}\\right) + \\log\\left(1 - p_{\\theta}(\\boldsymbol{x}_i) \\right)\\right]\\\\ \n    & = -\\frac{1}{n} \\sum_{i=1}^n \\left[y_i f_{\\theta}(\\boldsymbol{x}_i) - \\log\\left(1 + \\exp(f_{\\theta}(\\boldsymbol{x}_i)) \\right)\\right]\\\\ \n    \\nabla J(\\theta) & = -\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\nabla f_{\\theta}(\\boldsymbol{x}_i) - \\frac{\\exp(f_{\\theta}(\\boldsymbol{x}_i))}{1 + \\exp(f_{\\theta}(\\boldsymbol{x}_i))}\\nabla f_{\\theta}(\\boldsymbol{x}_i) \\right]\\\\\n    & = -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - p_{\\theta}(\\boldsymbol{x}_i)\\right)\\nabla f_{\\theta}(\\boldsymbol{x}_i). \n    \\end{align*}\n-   To compute $\\nabla J(\\theta)$, we need to compute $\\nabla f_{\\theta}(\\boldsymbol{x}_i)$ for all $i = 1, \\ldots, n$.\n\n## Chain Rule\n\nWrite \n\\begin{align*}\nf_{\\theta}(\\boldsymbol{x}) & = \\boldsymbol{W}^{(2)}\\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right) + \\boldsymbol{b}^{(2)} = \\boldsymbol{W}^{(2)}\\boldsymbol{h}^{(1)} + \\boldsymbol{b}^{(2)}\\\\\n\\boldsymbol{h}^{(1)} & = \\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right), \n\\end{align*}\nand we have \n\\begin{align*}\n\\nabla_{\\boldsymbol{b}^{(2)}} f_{\\theta}(\\boldsymbol{x}) & = 1\\\\\n\\nabla_{\\boldsymbol{W}^{(2)}} f_{\\theta}(\\boldsymbol{x}) & = \\boldsymbol{h}^{(1)} = \\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right) \\in \\R^m\\\\\n\\nabla_{\\boldsymbol{b}^{(1)}} f_{\\theta}(\\boldsymbol{x}) & = \\left[\\nabla_{\\boldsymbol{b}^{(1)}} \\boldsymbol{h}^{(1)}\\right] \\left(\\boldsymbol{W}^{(2)}\\right)^T \\in \\R^m\\\\\n\\nabla_{\\boldsymbol{W}^{(1)}} f_{\\theta}(\\boldsymbol{x}) & = \\boldsymbol{W}^{(2)}\\nabla_{\\boldsymbol{W}^{(1)}} \\boldsymbol{h}^{(1)} = \\sum_{k=1}^m W_k^{(2)} \\nabla_{\\boldsymbol{W}^{(1)}} h_k^{(1)}\\in \\R^{m \\times p}\\\\\n\\nabla_{\\boldsymbol{b}^{(1)}} \\boldsymbol{h}^{(1)} & = \\text{diag}\\left(\\sigma^{\\prime}(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)})\\right) \\in \\R^{m \\times m}\\\\\n\\nabla_{\\boldsymbol{W}^{(1)}} h_k^{(1)} & \\in \\R^{m \\times p}.\n\\end{align*}\n\n## Computational Graph\n\nA graphical representation of computation:\n\n-   Nodes indicate variables (scalar, vector, matrix, etc.)\n-   Edges indicate operations (addition, multiplication, function evaluation, etc.)\n\n![](images/lec04/comp_graph-0.png){fig-align=\"center\" width=30%}\n\n## Computational Graph for a Neural Network\n\n-   The computational graph for a neural network is the following\n\n![](images/lec04/comp_graph-1.png){fig-align=\"center\" width=100%}\n\n-   The square nodes represent unknown parameters.\n-   The process of transforming the input $\\boldsymbol{x}$ to the output $y$ following the arrows is called **forward propagation**.\n\n## Back-propagation\n\n-   The process of computing the gradient of the loss function with respect to the parameters is called **back-propagation**.\n-   The back-propogation algorithm is simply an application of the chain rule to compute the gradient of the loss function with respect to the parameters.\n-   To each edge in the computational graph, we associate a gradient that represents the derivative of the starting node with respect to the variable at the end node.\n\n\n![](images/lec04/comp_graph-2.png){fig-align=\"center\" width=100%}\n\n\n## Example\n\nConsider the same dataset as the previous example. We want to build a regression model using neural networks.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-mlp_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\nSetup an MLP with one hidden layer with 10 units and the logistic activation function.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport keras\nfrom keras import layers\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(units = 10, activation = \"sigmoid\", \n                     input_shape = (1,), name = \"hidden_layer\"),\n        layers.Dense(units = 1, activation = None, \n                     name = \"output_layer\")\n    ],\n    name=\"Shallow_MLP\"\n)\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"Shallow_MLP\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ hidden_layer (Dense)            │ (None, 10)             │            20 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ output_layer (Dense)            │ (None, 1)              │            11 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 31 (124.00 B)\n Trainable params: 31 (124.00 B)\n Non-trainable params: 0 (0.00 B)\n```\n\n\n:::\n:::\n\n\n\n---\n\nUse the Adam optimizer with a learning rate of 0.3 and train the model for 1000 epochs (iterations).\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-line-numbers=\"2,3,5\"}\nimport time \nopt = keras.optimizers.Adam(learning_rate=0.3)\nmodel.compile(loss=\"mean_squared_error\", optimizer=opt)\nt0 = time.time()\nmodel.fit(X.reshape(-1, 1), y, batch_size=n, epochs=1000, verbose=0) \nMLP_time = time.time() - t0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-mlp_files/figure-revealjs/unnamed-chunk-10-3.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Some questions\n\n-   ReLU v.s. Sigmoid? Which is better?\n-   How to choose the number of hidden units? The more the better?\n-   How to choose the number of hidden layers? The deeper the better?\n-   It seems that kernel regresion gives a slightly better result than MLP does.\n    -   In the previous example, kernel regression takes ~0.01 second to train, while MLP takes ~9.5 seconds.\n\n# Appendix\n\n## Proof of (*) in P.6\n\nShow that for any $\\boldsymbol{Z} \\in \\R^{n \\times d}$, $\\boldsymbol{y} \\in \\R^n$, and $\\alpha > 0$, we have\n$$\n(\\boldsymbol{Z}^T \\boldsymbol{Z} + \\alpha I_d)^{-1} \\boldsymbol{Z}^T \\boldsymbol{y} = \\boldsymbol{Z}^T (\\boldsymbol{Z} \\boldsymbol{Z}^T + \\alpha I_n)^{-1} \\boldsymbol{y}.\n$$\n\n**Proof**:\n\n1.  The left hand side is the solution to the equation $\\boldsymbol{Z}^T \\boldsymbol{Z} \\boldsymbol{\\beta}+\\alpha \\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{y}$.\n2.  Rearranging the terms gives $\\boldsymbol{\\beta}=\\boldsymbol{Z}^T\\left[\\frac{1}{\\alpha}(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{\\beta})\\right]$.\n3.  Define $\\boldsymbol{b}=\\frac{1}{\\alpha}(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{\\beta})$, and then $\\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{b}$.\n4.  Substituting $\\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{b}$ into $\\boldsymbol{b}=\\frac{1}{\\alpha}(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{\\beta})$, we have\n$$\n\\boldsymbol{b}=\\frac{1}{\\alpha}\\left(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{Z}^T \\boldsymbol{b}\\right).\n$$\n5.  Rearranging the terms gives $\\left(\\boldsymbol{Z}\\boldsymbol{Z}^{T}+\\alpha I_n\\right) \\boldsymbol{b}=\\boldsymbol{y}$, which yields $\\boldsymbol{b}=\\left(\\boldsymbol{Z} \\boldsymbol{Z}^T+\\alpha I_n\\right)^{-1} \\boldsymbol{y}$\n6.  Substituting into $\\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{b}$ gives $\\boldsymbol{\\beta}=\\boldsymbol{Z}^T\\left(\\boldsymbol{Z} \\boldsymbol{Z}^T+\\alpha I_n\\right)^{-1} \\boldsymbol{y}$.\n\n## Feature map of RBF kernel\n\nConsider the univariate RBF kernel $k(x, y) = \\exp\\left(-(x - y)^2\\right)$. We have\n\\begin{align*}\nk(x, y) & = \\exp\\left(-(x - y)^2\\right) = \\exp\\left(-x^2 + 2xy - y^2\\right) = \\exp(-x^2)\\exp(2xy)\\exp(-y^2)\\\\\n& = \\exp(-x^2)\\left(\\sum_{m=0}^{\\infty} \\frac{2^mx^my^m}{m!}\\right)\\exp(-y^2)\\\\\n& = \\exp(-x^2) \\left(1, \\sqrt{\\frac{2^1}{1!}}x, \\sqrt{\\frac{2^2}{2!}}x^2, \\sqrt{\\frac{2^3}{3!}}x^3, \\ldots \\right)^T\\\\\n& \\qquad \\left(1, \\sqrt{\\frac{2^1}{1!}}y, \\sqrt{\\frac{2^2}{2!}}y^2, \\sqrt{\\frac{2^3}{3!}}y^3, \\ldots \\right)\\exp(-y^2)\n\\end{align*}\n\nHence the feature map corresponding to the RBF kernel is \n$$\n\\phi(x) = \\exp(-x^2)\\left(1, \\sqrt{\\frac{2^1}{1!}}x, \\sqrt{\\frac{2^2}{2!}}x^2, \\sqrt{\\frac{2^3}{3!}}x^3, \\ldots \\right)^T.\n$$",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}