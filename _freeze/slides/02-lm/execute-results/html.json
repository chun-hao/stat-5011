{
  "hash": "9e62bead4e1f6eede7939798c43576c3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Generalized Linear Models\"\n---\n\n\n## Outline\n\n\n::: {.hidden}\n$$\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n$$\n:::\n\n\n\n-  Classical Linear Model\n   -  Ordinary Least Squares (OLS) Estimation\n   -  Maximum Likelihood (ML) Estimation\n   -  Penality and Regularization\n-  Generalized Linear Models\n   -  Logistic regression\n   -  Multinomial Regression\n-  Non-linear Models\n   -  Generalized Additive Models (GAM)\n   -  Projection Pursuit Regression (PPR)\n\n## Classical Linear Model\n\n-  Given $p$ covariate variables $x_1, \\ldots, x_p$ and a response variable $y$, the classical linear model assumes that the relationship between the $x$'s and $y$ is linear:\n   $$\n   y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p.\n   $$\n\n-  Denote $\\boldsymbol{x} = (1, x_{1}, \\ldots, x_{p})^T$ and $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^T$, the model can be written as\n   $$\n   y = \\boldsymbol{x}^T\\boldsymbol{\\beta}.\n   $$\n-  Suppose now we have $n$ samples $(\\boldsymbol{x}_1, y_1), \\ldots (\\boldsymbol{x}_p, y_p)$ and we believe that the linear model above is a reasonable approximation of the relationship between the $\\boldsymbol{x}_i$'s and $y_i$.\n-  The goal is to estimate the **model parameter** $\\boldsymbol{\\beta}$. \n\n## Ordinary Least Squares (OLS) Estimation\n\n-  The most common method to estimate $\\boldsymbol{\\beta}$ is the **ordinary least squares (OLS)** estimation, that is, we find $\\boldsymbol{\\beta}$ that minimizes the sum of squared residuals:\n   $$\n   \\hat{\\boldsymbol{\\beta}} = \\argmin_{\\boldsymbol{\\beta}} \\sum_{i=1}^n (y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2.\n   $$\n-  Denote\n   $$\n   \\boldsymbol{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad\n   \\boldsymbol{X} = \\begin{bmatrix} \\boldsymbol{x}_1^T \\\\ \\vdots \\\\ \\boldsymbol{x}_n^T \\end{bmatrix},\n   $$\n   the minimization problem can be written as\n   $$\n   \\hat{\\boldsymbol{\\beta}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2.\n   $$\n\n## Ordinary Least Squares (OLS) Estimation\n\n-  Let $L(\\boldsymbol{\\beta}) = \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2$. This is often called the **loss function**.\n-  Take the gradient of $L(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and set it to zero, we have\n   $$\n   \\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = -2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) = 0.\n   $$\n-  The OLS estimation has a closed-form solution:\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{OLS}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n   $$\n-  To ensure the existence of the inverse, we often assume that $\\boldsymbol{X}^T\\boldsymbol{X}$ is invertible, that is, the columns of $\\boldsymbol{X}$ are linearly independent.\n-  To verify that $\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}}$ is the minimizer, we need to show that the Hessian of $L(\\boldsymbol{\\beta})$ is positive definite:\n   $$\n   \\nabla^2_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = 2\\boldsymbol{X}^T\\boldsymbol{X} \\succ 0.\n   $$\n\n\n## Maximum Likelihood (ML) Estimation\n\n-  Another way to estimate $\\boldsymbol{\\beta}$ is the **maximum likelihood (ML)** estimation.\n-  Suppose that the response variable $y$ is normally distributed with mean $\\boldsymbol{x}^T\\boldsymbol{\\beta}$ and variance $\\sigma^2$:\n   $$\n   y \\mid \\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{x}^T\\boldsymbol{\\beta}, \\sigma^2).\n   $$\n-  Assuming the samples are i.i.d, the likelihood function is\n   $$\n   L(\\boldsymbol{\\beta}, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right).\n   $$\n-  The log-likelihood function is\n   $$\n   \\ell(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2 = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2.\n   $$\n-  The ML estimation is $\\hat{\\boldsymbol{\\beta}}, \\hat{\\sigma}^2 = \\argmax_{\\boldsymbol{\\beta}, \\sigma^2} \\ell(\\boldsymbol{\\beta}, \\sigma^2)$.\n\n## Maximum Likelihood (ML) Estimation\n\n-  Taking the gradient of $\\ell(\\boldsymbol{\\beta}, \\sigma^2)$ with respect to $\\boldsymbol{\\beta}$ and $\\sigma^2$ and set them to zero, we have\n   $$\n   \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sigma^2} \\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) = 0,\n   $$ \n   and\n   $$ \n   \\frac{\\partial}{\\partial \\sigma^2} \\ell(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 = 0.\n   $$\n-  The ML estimation has a closed-form solution: \n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}, \\quad\n   \\hat{\\sigma}^2 = \\frac{1}{n}\\|\\boldsymbol{y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}\\|_2^2.\n   $$\n-  The MLE of $\\boldsymbol{\\beta}$ is the same as the OLS estimation.\n\n## OLS v.s. ML Estimation\n\n-  Compared to the OLS estimation, the ML estimation requires an additional assumption on the distribution of $y$.\n-  In ths case of linear regression, the normality assumption is the most common one.\n-  An equivalent way to express the linear regression under the normality assumption is\n   $$\n   y  = \\boldsymbol{x}^T\\boldsymbol{\\beta} + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2).\n   $$\n-  One of the advantages of the ML estimation is that it provides a way to estimate the variance of the estimated parameter $\\hat{\\boldsymbol{\\beta}}$: \n   \\begin{align*}\n   \\var(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}) & = \\var((\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}) \\\\\n   & = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\var(\\boldsymbol{y})\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\\\\n   & = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T(\\sigma^2 I)\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\\\\n   & = \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}.\n   \\end{align*}\n\n## Useful Properties of MLE\n\n-  Under the normality assumption, the MLE has the following properties:\n   -  **Unbiasedness**: $\\E(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}) = \\boldsymbol{\\beta}$.\n   -  **Normality**: $\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1})$.\n   -  **Prediction Intervals**: for a given $\\boldsymbol{x}^{\\star}$, the predicted value is $y^{\\star} = \\boldsymbol{x}^{\\star T}\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}$ and the prediction interval is\n      $$\n      y^{\\star} \\pm t_{n-p-1, 1-\\alpha/2} \\hat{\\sigma} \\sqrt{1 + \\boldsymbol{x}^{\\star T}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{x}^{\\star}}.\n      $$\n   -  We can also derive the confidence intervals and hypothesis tests for $c^T\\boldsymbol{\\beta}$ for any $c$.\n\n## What if the samples are not i.i.d?\n\n-  If the samples are not i.i.d, we can model the joint distribution of the samples:\n   $$\n   \\boldsymbol{y} \\mid \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{X}\\boldsymbol{\\beta}, \\sigma^2 W^{-1})\n   $$\n   where $W$ is an $n\\times n$ covariance matrix describing the dependence between the samples.\n-  Consider the transformation $\\widetilde{\\boldsymbol{y}} = W^{1/2}\\boldsymbol{y}$ and $\\widetilde{\\boldsymbol{X}} = W^{1/2}\\boldsymbol{X}$, the model becomes\n   $$\n   \\widetilde{\\boldsymbol{y}} \\mid \\widetilde{\\boldsymbol{X}} \\sim \\mathcal{N}(\\widetilde{\\boldsymbol{X}}\\boldsymbol{\\beta}, \\sigma^2 I).\n   $$\n-  Therefore the MLE for $\\boldsymbol{\\beta}$ is\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = (\\widetilde{\\boldsymbol{X}}^T\\widetilde{\\boldsymbol{X}})^{-1}\\widetilde{\\boldsymbol{X}}^T\\widetilde{\\boldsymbol{y}} = (\\boldsymbol{X}^TW\\boldsymbol{X})^{-1}\\boldsymbol{X}^TW\\boldsymbol{y}\n   $$\n   which is called the **weighted least squares** estimation.\n\n## Penalized Likelihood Estimation\n\n-  However, in practice, the MLE not be the best choice.\n-  For example, when $X$ contains columns that are close to **collinear** or if the number of covariates $p$ is large, computing $(X^TX)^{-1}$ will become numerically unstable.\n-  One of the most common ways to address this issue is to add **penalization** or **regularization**.\n-  The idea is to add a penalty term to the log-likelihood function, i.e.,\n   $$\n   -\\ell(\\boldsymbol{\\beta}, \\sigma^2) + \\lambda \\cdot \\text{pen}(\\boldsymbol{\\beta}).\n   $$\n-  That is, we are looking for the $\\boldsymbol{\\beta}$ that minimizes the negaive log-likelihood and the penalty. \n-  The extra term $\\lambda$ is a **hyperparameter** that controls the trade-off between the likelihood and the penalty.\n\n## Ridge Regression\n\n-  One of the most common penalization methods is the **ridge regression**.\n-  The penalty term is the $L_2$ norm of $\\boldsymbol{\\beta}$:\n   $$\n   \\text{pen}(\\boldsymbol{\\beta}) = \\|\\boldsymbol{\\beta}\\|_2^2 = \\sum_{j=0}^p \\beta_j^2.\n   $$\n-  The ridge estimator is\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2.\n   $$\n-  Let $L_{\\lambda}(\\boldsymbol{\\beta}) = \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2$ and set the gradient to zero \n   $$\n   \\nabla_{\\boldsymbol{\\beta}} L_{\\lambda}(\\boldsymbol{\\beta}) = -2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + 2\\lambda \\boldsymbol{\\beta} = 0.\n   $$\n-  Hence the ridge estimator is\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\lambda I)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n   $$\n\n## Ridge Regression\n\n-  Typically, penalization of the intercept is not desired in ridge regression so that $\\beta_0$ should be excluded from the penalty term. \n-  A simple way to achieve this is to center all covariates and the responses so that $\\bar{y} = 0$ and $\\bar{\\boldsymbol{x}} = 0$ which automatically results in $\\hat{\\beta}_0 = 0$.\n-  A second approach is to use the following penalty term: \n   $$\n   \\text{pen}(\\boldsymbol{\\beta}) = \\sum_{j=1}^p \\beta_j^2 = \\boldsymbol{\\beta}^TK\\boldsymbol{\\beta},\n   $$\n   where $K = \\text{diag}(0, 1, \\ldots, 1)$.\n-  The ridge estimator is\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\lambda K)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n   $$\n\n## Ridge v.s. OLS \n\n-  The ridge estimator is **biased** and the OLS is **unbiased**.\n-  However, one can show that the ridge estimator has a smaller variance than the OLS estimator.\n-  When choosing an appropriate smoothing parameter $\\lambda$, the ridge estimator can have a smaller **mean squared error (MSE)** than the OLS estimator.\n-  The ridge estimator is **shrinkage** estimator that shrinks the coefficients towards zero (large value of $\\lambda$ yields a stronger shrinkage).\n-  The ridge estimator is particularly useful when the covariates are collinear or when the number of covariates is large.\n-  The choice of $\\lambda$ is often done using cross-validation.\n\n\n## Least Absolute Shrinkage and Selection Operator (LASSO)\n\n-  Another common penalization method is the **least absolute shrinkage and selection operator (LASSO)**.\n-  The penalty term is the $L_1$ norm of $\\boldsymbol{\\beta}$:\n   $$\n   \\text{pen}(\\boldsymbol{\\beta}) = \\sum_{j=1}^p |\\beta_j|.\n   $$\n-  The LASSO estimator is\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{LASSO}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\sum_{j=1}^p |\\beta_j|.\n   $$\n\n## LASSO Estimation\n\n-  Note that the objective function of the LASSO estimator is not differentiable, due to the absolute value term.\n-  No closed-form solution for the LASSO estimator is available. However, it can be solved using the **Least Angle Regression**[^efron] or the **coordinate descent**[^friedman] algorithm.\n-  One of the key properties of the LASSO estimator is that it produces **sparse** solutions, i.e., some of the estimated coefficients are exactly zero.\n-  This is particularly useful for **variable selection**, i.e., to identify the important covariates.\n\n[^efron]: Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. *The Annals of Statistics*, 32(2), pages 407–499.\n[^friedman]: Friedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. *Journal of Statistical Software*, 33(1), pages 1–22.\n\n\n## Ridge v.s. LASSO ($L_2$ penalty v.s. $L_1$ penalty)\n\n![](images/lec02/regularization.png){fig-align=\"center\" width=80%}\n\n::: aside\nFigure 3.11 of ESL\n:::\n\n## Elastic Net\n\n-  The **elastic net**[^zou] is a combination of the ridge and the LASSO.\n-  The penalty term is a combination of the $L_1$ and $L_2$ norm of $\\boldsymbol{\\beta}$:\n   $$\n   \\text{pen}(\\boldsymbol{\\beta}) = \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2.\n   $$\n-  The elastic net estimator is\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{EN}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2.\n   $$\n-  Through the choice of $\\lambda_1$ and $\\lambda_2$, the elastic net can be used to achieve the benefits of both the ridge and the LASSO.\n\n[^zou]: Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. *Journal of the Royal Statistical Society Series B: Statistical Methodology*, 67(2), pages 301-320.\n\n## Example - Sparse features\n\n-  We generate a synthetic dataset with 50 samples and 10 features.\n-  Only 5 out of the 10 features are informative.\n-  We fit the linear regression, Ridge, Lasso, and Elastic Net models to the data.\n\n::: {#6f517b34 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.datasets import make_regression\n\nX, y, true_coef = make_regression(n_samples = 50, n_features = 10, \n                                  n_informative = 5, noise = 5,\n                                  coef = True, random_state = 42)\n\nlm = LinearRegression().fit(X, y)\nridge = Ridge(alpha=1.0).fit(X, y)\nlasso = Lasso(alpha=1.0).fit(X, y)\nenet = ElasticNet(alpha=1.0, l1_ratio=0.5).fit(X, y)\n```\n:::\n\n\n## Example - Sparse features\n\n::: {#2d982287 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=2}\n|   True Coef |   Linear |   Ridge |   LASSO |   ElasticNet |\n|------------:|---------:|--------:|--------:|-------------:|\n|      57.078 |   56.306 |  55.626 |  55.459 |       40.091 |\n|       0.000 |    0.173 |   0.471 |   0.000 |        1.685 |\n|       0.000 |   -0.185 |   0.073 |  -0.000 |        1.556 |\n|      35.610 |   33.877 |  33.447 |  33.189 |       25.617 |\n|       0.000 |    0.702 |   1.655 |   0.000 |        8.187 |\n|      60.577 |   60.568 |  59.158 |  59.634 |       38.185 |\n|       0.000 |    1.586 |   1.838 |   0.650 |        4.251 |\n|      64.592 |   64.964 |  63.242 |  63.704 |       37.886 |\n|       0.000 |   -0.440 |  -0.428 |  -0.000 |       -1.682 |\n|      98.652 |   99.563 |  96.871 |  98.682 |       61.042 |\n:::\n:::\n\n\n# Generalized Linear Models\n\n## Beyond Normality \n\n-  When the responce variable is not real-valued, the classical linear model is not appropriate.\n-  For example:\n   -  Binary responces: $y \\in \\{0, 1\\}$.\n   -  Count data: $y \\in \\{0, 1, 2, \\ldots\\}$.\n   -  Multinomial responces: $y \\in \\{1, 2, \\ldots, K\\}$.\n-  In these cases, neither the OLS estimation nor the normality assumption is appropriate.\n-  Generalized Linear Model (GLM) is a generalization of the classical linear model that allows for non-normal responces.\n-  The key is to find a reasonable distribution to model $y$.\n  \n\n## Binary Responces: Logistic regression\n\n-  When $y$ is binary, we can use the Bernoulli distribution\n   $$\n   Y \\mid \\boldsymbol{x} \\sim \\text{Ber}(p(\\boldsymbol{x})),\n   $$\n-  That is, $\\P(Y = 1 \\mid \\boldsymbol{x}) = p(\\boldsymbol{x})$ and $\\P(Y = 0 \\mid \\boldsymbol{x}) = 1 - p(\\boldsymbol{x})$ and the expectation is $\\E(Y \\mid \\boldsymbol{x}) = p(\\boldsymbol{x})$.\n-  The logistic regression model assumes\n   $$\n   p(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T\\boldsymbol{\\beta})}.\n   $$\n-  Equivalently, we can write\n   $$\n   \\log\\left(\\frac{p(\\boldsymbol{x})}{1 - p(\\boldsymbol{x})}\\right) = \\boldsymbol{x}^T\\boldsymbol{\\beta}.\n   $$\n-  That is, the log-odds of the event $\\{Y = 1\\}$ is linear in $\\boldsymbol{x}$.\n\n## ML Estimation for Logistic Regression\n\n-  Given $n$ samples $(\\boldsymbol{x}_1, y_1), \\ldots (\\boldsymbol{x}_n, y_n)$, the likelihood function is\n   $$\n   L(\\boldsymbol{\\beta}) = \\prod_{i=1}^n p(\\boldsymbol{x}_i)^{y_i} (1 - p(\\boldsymbol{x}_i))^{1 - y_i}.\n   $$\n-  The log-likelihood function is\n   $$\n   \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n y_i \\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)).\n   $$\n-  Hence the MLE of $\\boldsymbol{\\beta}$ is\n   $$\n   \\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = \\argmax_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}).\n   $$\n-  The negative log-likelihood is also called the **cross-entropy** loss, i.e., maximizing the likelihood is equivalent to minimizing the cross-entropy loss.\n\n## Cross-Entropy Loss\n\n-  In Information Theory, the cross-entropy is defined as\n   $$\n   H(p, q) = -\\sum_{x} p(x) \\log(q(x)) = -\\E_p[\\log(q(X))],\n   $$\n   where $p(x)$ and $q(x)$ are two discrete probability distributions.\n-  Large value of cross-entropy indicates that the two distributions are different.\n-  In the case of logistic regression, we want to measure the discrepancy between the data $[y_i, 1-y_i]$ and the model $[p(\\boldsymbol{x}_i), 1 - p(\\boldsymbol{x}_i)]$.\n-  Hence the cross-entropy is\n   $$\n   -\\sum_{i=1}^n y_i \\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)).\n   $$\n\n\n## Exponential family\n\n-  Recall that an exponential family is a family of distributions\n   $$\n   f(x \\mid \\theta) = h(x)\\exp(\\theta^T T(x) - \\psi(\\theta))\n   $$\n   where $\\theta \\in \\R^k$ and $T(x) = [T_1(x), \\ldots, T_k(x)]^T$.\n-  The parameter $\\theta$ is called the **natural parameter** or the **canonical parameter** and $T(x)$ is the **sufficient statistic**.\n-   Two useful properties (from Bartlett's identities):\n    +   $\\E(T(X)) = \\nabla_{\\theta}\\psi(\\theta)$\n    +   $\\var(T(X)) = \\text{Hess}(\\psi(\\theta)) = \\nabla^2_{\\theta} \\psi(\\theta)$.\n-   That is, the relationship between the parameter $\\theta$ and the expectation $\\E(T(X))$ determined by $\\nabla \\psi$.\n\n\n## Examples\n\n-  Normal disribution: \n   $$\n   f(x \\mid \\mu, \\sigma^2) = \\exp\\left(-\\frac{1}{2\\sigma^2} x^2 + \\frac{\\mu}{\\sigma^2}x - \\frac{\\mu^2}{2\\sigma^2}\\right), \\quad x \\in \\R\n   $$\n    +   $\\theta = \\left(-\\frac{1}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}\\right)$, $T(x) = (-x^2, x)$, $\\psi(\\theta) = -\\frac{\\theta_2^2}{4\\theta_1} = \\frac{\\mu^2}{2\\sigma^2}$\n-  Bernoulli distribution:\n   $$\n   f(x \\mid p) = p^x(1-p)^{1-x} = \\exp\\left(x\\log\\frac{p}{1-p} + \\log(1-p)\\right), \\quad x \\in \\{0, 1\\}\n   $$\n    +   $\\theta = \\log\\frac{p}{1-p}$, $T(x) = x$, $\\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta})$\n-  Poisson distribution:\n   $$\n   f(x \\mid \\lambda) = \\frac{\\lambda^x e^{\\lambda}}{x!}= \\frac{1}{x!}\\exp(x\\log\\lambda - \\lambda), \\quad x = 0, 1, 2, \\ldots\n   $$\n    +   $\\theta = \\log\\lambda$, $T(x) = x$, $\\psi(\\theta) = \\exp(\\theta) = \\lambda$\n\n## Generalized Linear Model (GLM)\n\n-   Let $Y$ be univariate, $\\boldsymbol{x} \\in \\R^p$, and $\\boldsymbol{\\beta} \\in \\R^p$.\n-   A GLM is assuming $Y \\mid \\boldsymbol{x} \\sim F_{\\theta}$, where $\\theta = \\boldsymbol{x}^T\\boldsymbol{\\beta}$ and $F_\\theta$ has the density function\n    $$\n    f(y \\mid \\theta) = h(y)\\exp(\\theta\\cdot y - \\psi(\\theta)).\n    $$\n-   Therefore\n    \\begin{align*}\n    \\E(Y \\mid \\boldsymbol{x}) & = \\frac{d}{d\\theta}\\psi(\\theta) = \\psi^{\\prime}(\\boldsymbol{x}^T\\boldsymbol{\\beta})\n    \\end{align*}\n-   Equivalently,\n    $$\n    g(\\E(Y \\mid \\boldsymbol{x})) = \\boldsymbol{x}^T\\boldsymbol{\\beta} \n    $$\n    where $g$ is the inverse of $\\psi^{\\prime}$.\n-   The function $g$ is called the **link function**.\n\n## Logistic Regression\n\n-  For Bernoulli distributions, we have \n   $$\n   \\theta = \\log\\frac{p}{1-p}, \\quad \\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta}).\n   $$\n-  Thus, $\\psi^{\\prime}(\\theta) = \\frac{e^{\\theta}}{1 + e^{\\theta}}$ and $g(p) = (\\psi^{\\prime})^{-1}(p) = \\log\\frac{p}{1-p}$. $\\psi^{\\prime}$ is called the **logistic function** and $g$ is called the **logit function**.\n-   Putting altogether, we have\n    $$\n    g(\\E(Y \\mid \\boldsymbol{x})) = \\log\\left(\\frac{p(\\boldsymbol{x})}{1 - p(\\boldsymbol{x})}\\right) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n    $$\n    or equivalently\n    $$\n    \\P(Y = 1 \\mid \\boldsymbol{x}) = \\psi^{\\prime}(\\boldsymbol{x}^T\\boldsymbol{\\beta}) = \\frac{\\exp(\\boldsymbol{x}^T\\boldsymbol{\\beta})}{1 + \\exp(\\boldsymbol{x}^T\\boldsymbol{\\beta})}.\n    $$\n    \n## Remarks\n\n-   The link function $g = (\\psi^{\\prime})^{-1}$ is sometimes called the **canonical link** function, since it is derived from the canonical representation of an exponential family.\n-   All we need for a link function is that it is invertible and matches the range of $\\E(Y \\mid \\boldsymbol{x})$ and $\\boldsymbol{x}^T\\boldsymbol{\\beta}$.\n-   For example, in the Bernoulli linear model, we could have used the **probit link** function\n    $$\n    g(u) = \\Phi^{-1}(u): [0, 1] \\to \\R\n    $$\n    where $\\Phi$ is the CDF of the standard normal distribution.\n-   This is called the **probit regression**.\n\n## Multinomial Regression\n\n-  Multinomial regression is a generalization of Logistic regression to categorical variables with more than two categories.\n-  Suppose $Y$ is a categorical variable with $K$ categories, $Y \\in \\{1, 2, \\ldots, K\\}$.\n-  A more useful representation is to use the **one-hot encoding**:\n   $$\n   Y = [0, 0, \\ldots, 1, \\ldots, 0]^T\n   $$\n   where the $k$-th element is 1 and the rest are 0.\n-  The multinomial regression model assumes\n   $$\n   Y \\mid \\boldsymbol{x} \\sim \\text{Multi}(1, [p_1(\\boldsymbol{x}), p_2(\\boldsymbol{x}), \\ldots, p_K(\\boldsymbol{x})]^T)\n   $$\n   where $p_k(\\boldsymbol{x}) = \\P(Y = 1_k \\mid \\boldsymbol{x})$ and $1_k$ is the one-hot encoding of the $k$-th category.\n\n## Multinomial Distribution\n\n-  The probability mass function of the $\\text{Multi}(m,p)$ is\n   $$\n   f(x \\mid p) = \\frac{n!}{x_1!\\cdots x_K!}\\prod_{k=1}^K p_k^{x_k} = \\frac{n!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^K x_k\\log p_k\\right)\n   $$\n   where $x = [x_1, x_2, \\ldots, x_K]^T$, $\\sum_{k=1}^K x_k = m$, and $\\sum_{k=1}^K p_k = 1$.\n-  Note that $p_K = 1 - p_1 - \\ldots - p_{K-1}$ and therefore\n   \\begin{align*}\n   f(x \\mid p) & = \\frac{n!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^{K-1} x_k\\log p_k + \\left(m - \\sum_{k=1}^{K-1} x_k\\right)\\log(1 - p_1 - \\ldots - p_{K-1})\\right)\\\\\n   & = \\frac{n!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^{K-1} x_k\\log\\frac{p_k}{p_K} + m\\log(1 - p_1 - \\ldots - p_{K-1})\\right).\n   \\end{align*}\n\n\n\n## Softmax Function\n\n-  The canonical parameter is $\\theta = [\\log\\frac{p_1}{p_K}, \\ldots, \\log\\frac{p_{K-1}}{p_K}]^T$ and therefore $p_i = p_K\\exp(\\theta_i)$.\n-  Using the relationship $p_K = 1 - \\sum_{k=1}^{K-1} p_k$, we have\n   $$\n   p_K = 1 - p_K\\sum_{k=1}^{K-1} \\exp(\\theta_k) \\quad \\Rightarrow \\quad p_K = \\frac{1}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}.\n   $$\n-  Hence (assume $m = 1$ for simplicity)\n   \\begin{align*}\n   \\psi(\\theta) & = - \\log(1 - p_1 - \\ldots - p_{K-1})\n   = - \\log(1 - p_Ke^{\\theta_1} - \\ldots - p_Ke^{\\theta_{K-1}})\\\\\n   & = - \\log\\left(1 - \\frac{\\sum_{k=1}^{K-1} \\exp(\\theta_k)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}\\right)\n   = \\log\\left(1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)\\right).\n   \\end{align*}\n-  Taking the derivative, we have the **softmax function**:\n   $$\n   \\nabla_{\\theta}\\psi(\\theta) = \\left[\\frac{\\exp(\\theta_1)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}, \\ldots, \\frac{\\exp(\\theta_{K-1})}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}\\right].\n   $$\n\n\n## Multinomial Regression\n\n-  The multinomial regression model is given by\n   \\begin{align*}\n      \\theta_i & = \\boldsymbol{x}^T\\boldsymbol{\\beta}_i, \\\\\n      p_i(\\boldsymbol{x}) & = \\frac{\\exp(\\theta_i)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_i)}, \\quad i = 1, 2, \\ldots, K-1,\n   \\end{align*}\n   where $\\boldsymbol{\\beta}_i \\in \\R^p$.\n-  In fact, a more common representation is \n   \\begin{align*}\n      \\tilde{\\theta}_i & = \\boldsymbol{x}^T\\tilde{\\boldsymbol{\\beta}}_i, \\\\\n      p_i(\\boldsymbol{x}) & = \\frac{\\exp(\\tilde{\\theta}_i)}{\\sum_{k=1}^{K} \\exp(\\tilde{\\theta}_i)}, \\quad i = 1, 2, \\ldots, K.\n   \\end{align*}\n-  The equivalence is due to the transformation $\\theta_i = \\tilde{\\theta}_i - \\tilde{\\theta}_K$ and $\\boldsymbol{\\beta}_i = \\tilde{\\boldsymbol{\\beta}}_i - \\tilde{\\boldsymbol{\\beta}}_K$.\n-  We can also write\n   $$\n   [p_1(\\boldsymbol{x}), \\ldots, p_K(\\boldsymbol{x})] = \\texttt{softmax}(\\boldsymbol{x}^T\\boldsymbol{\\beta}_1, \\ldots, \\boldsymbol{x}^T\\boldsymbol{\\beta}_K).\n   $$\n\n\n## Quick Summary\n\n-  A GLM is \n   $$\n      g(\\E(Y \\mid X = x)) = x^T\\beta \\Leftrightarrow \\E(Y \\mid X = x) = g^{-1}(x^T\\beta).\n   $$\n-  The link function $g$ connects the conditional expectation and the linear predictor and is chosen based on the distribution of $Y$.\n-  Examples:\n   -  Logistic regression: $g(p) = \\log\\left(\\frac{p}{1-p}\\right)$, $g^{-1}(x) = $\n   -  Linear regression: $g(\\mu) = \\mu$.\n   -  Multinomial regression: softmax function.\n   -  There are other choices and the above are called the **canonical link functions**.\n\n## Beyond Linearity\n\n-  Up to now, we have assumed that a linear relationship between the features and the (transformed) conditional expectation:\n   $$\n      g(\\E(Y \\mid \\boldsymbol{x})) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n   $$\n-  However, this is a strong assumption and may not be appropriate in many cases.\n-  To remove this assumption, we can consider\n   $$\n      g(\\E(Y \\mid \\boldsymbol{x})) = f(\\boldsymbol{x})\n   $$\n   where $f: \\R^p \\to \\R$ is an unknown function.\n-  The problem is now to estimate the function $f$.\n-  Depending on the restrictions on $f$, we can use different methods to estimate $f$.\n\n## Generalized Additive Models (GAM)\n\n-  An additive model assumes that the unknown function $f$ is a sum of univariate functions:\n   $$\n   f(\\boldsymbol{x}) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p).\n   $$\n-  Therefore, the model is\n   $$\n   g(\\E(Y \\mid \\boldsymbol{x})) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p).\n   $$\n-  The functions $f_j: \\R \\to \\R$ are unknown and need to be estimated.\n-  Nonparametric methods can be used to estimate the functions $f_j$, for example, kernel smoothing, splines, etc.\n-  The GAM is a generalization of the linear model that allows for non-linear relationships between the features and the conditional expectation.\n\n## Projection Pursuit Regression (PPR)\n\n-  The **projection pursuit regression (PPR)**[^ppr] model assumes:\n   $$\n   f(\\boldsymbol{x}) = \\sum_{m=1}^M f_m(\\boldsymbol{x}^T\\boldsymbol{\\omega}_m),\n   $$\n   where $\\boldsymbol{\\omega}_m \\in \\R^p$ are unknown unit vectors and $f_m: \\R \\to \\R$ are unknown functions.\n-  The scalar variable $V_m = \\boldsymbol{x}^T\\boldsymbol{\\omega}_m$ is the projection of $\\boldsymbol{x}$ onto the unit vector $\\boldsymbol{\\omega}_m$, and we seek $\\boldsymbol{\\omega}_m$ so that the model fits well, hence the name “projection pursuit.”\n-  If $M$ is taken arbitrarily large, for appropriate choice of $f_m$ the PPR model can approximate any continuous function in $\\R^p$ arbitrarily well, i.e., the PPR is a *universal approximator*. \n-  However, this model is not widely used due to the difficulty in estimating the functions $f_m$.\n\n[^ppr]: Friedman, J. H., & Tukey, J. W. (1974). A projection pursuit algorithm for exploratory data analysis. *IEEE Transactions on computers*, 100(9), 881-890. \n\n# MLE for Logistic Regression\n\n## The log-likelihood function\n\nIn order to find the MLE, we need to simplify the log-likelihood function:\n\\begin{align*}\n\\ell(\\boldsymbol{\\beta}) & = \\log \\left(\\prod_{i=1}^n p(\\boldsymbol{x}_i)^{y_i}(1-p(\\boldsymbol{x}_i))^{1-y_i}\\right) \\\\\n& = \\sum_{i=1}^n y_i \\log p(\\boldsymbol{x}_i) + (1-y_i)\\log(1-p(\\boldsymbol{x}_i))\\\\\n& = \\sum_{i=1}^n \\left[y_i \\log \\left(\\frac{p(\\boldsymbol{x}_i)}{1-p(\\boldsymbol{x}_i)}\\right) + \\log(1-p(\\boldsymbol{x}_i))\\right]\\\\\n& = \\sum_{i=1}^n \\left[y_i\\boldsymbol{x}_i^T\\boldsymbol{\\beta} - \\log(1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}))\\right]\n\\end{align*}\n\n## Gradient and Hessian\n\nNow we compute the gradient and the Hessian of the log-likelihood function:\n\n\\begin{align*}\n    \\nabla \\ell(\\boldsymbol{\\beta}) & = \\sum_{i=1}^n \\left[y_i \\boldsymbol{x}_i - \\frac{\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})}{1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})}\\boldsymbol{x}_i\\right] = \\sum_{i=1}^n (y_i - p(\\boldsymbol{x}_i))\\boldsymbol{x}_i\\\\\n    & = X^T(\\boldsymbol{y}-\\mathbf{p})\\\\\n    \\nabla^2 \\ell(\\boldsymbol{\\beta}) & = -\\sum_{i=1}^n p(\\boldsymbol{x}_i)(1-p(\\boldsymbol{x}_i))\\boldsymbol{x}_i\\boldsymbol{x}_i^T = -X^TWX\n\\end{align*}\nwhere\n$$\n\\mathbf{p} = [p(\\boldsymbol{x}_1), \\ldots, p(\\boldsymbol{x}_n)]^T, \\quad W= \\diag(\\mathbf{p})\\diag(1-\\mathbf{p}), \\quad X = \\begin{bmatrix} \\boldsymbol{x}_1^T \\\\ \\vdots \\\\ \\boldsymbol{x}_n^T \\end{bmatrix}\n$$\n\n## Iteratively Reweighted Least Squares (IRWLS)\n\nThere is no analytic solution for the MLE of the logistic regression. However, the Newton-Raphson method can be used to find the MLE. The Newton-Raphson method is an iterative method that updates the parameter $\\boldsymbol{\\beta}$ as follows:\n\n\\begin{align*}\n\\boldsymbol{\\beta}^{(t+1)} & = \\boldsymbol{\\beta}^{(t)} - \\left[\\nabla^2 \\ell(\\boldsymbol{\\beta}^{(t)})\\right]^{-1} \\nabla \\ell\\left(\\boldsymbol{\\beta}^{(t)}\\right) \\\\\n& = \\boldsymbol{\\beta}^{(t)}+\\left(X^T W^{(t)}X\\right)^{-1} X^T\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right) \\\\\n& = \\left(X^T W^{(t)} X\\right)^{-1} X^T W^{(t)}\\left[X \\boldsymbol{\\beta}^{(t)}+\\left(W^{(t)}\\right)^{-1}\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right)\\right] \\\\\n& = \\left(X^T W^{(t)} X\\right)^{-1} X^T W^{(t)} \\mathbf{z}^{(t)}\n\\end{align*}\nwhere\n\\begin{align*}\n\\mathbf{z}^{(t)} & =X \\boldsymbol{\\beta}^{(t)}+\\left(W^{(t)}\\right)^{-1}\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right), \\quad \\mathbf{p}^{(t)} = [p^{(t)}(\\boldsymbol{x}_1), \\ldots, p^{(t)}(\\boldsymbol{x}_n)]^T\\\\\np^{(t)}(\\boldsymbol{x}_i) & = \\frac{\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}^{(t)})}{1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}^{(t)})}, \\quad\nW^{(t)} = \\diag(\\mathbf{p}^{(t)})\\diag(1-\\mathbf{p}^{(t)})\n\\end{align*}\n\n",
    "supporting": [
      "02-lm_files"
    ],
    "filters": [],
    "includes": {}
  }
}