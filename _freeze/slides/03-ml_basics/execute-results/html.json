{
  "hash": "71c1370adf279d377d657a7763240eab",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning Basics\"\n---\n\n\n## Recap of the Last Lecture\n\n\n::: {.hidden}\n$$\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n$$\n:::\n\n\n\n-  Relationship between likelihood and loss function\n   -  Normal likelihood $\\leftrightarrow$ squared error loss\n   -  Multinomial/Binomial likelihood $\\leftrightarrow$ cross-entropy loss\n-  Penalization/Regularization: $L_1$ and $L_2$ regularization\n-  Link function: a function that connects the conditional mean $\\E(Y \\mid \\boldsymbol{x})$ and the linear predictor $\\boldsymbol{x}^T \\boldsymbol{\\beta}$:\n   -  Real-valued response: identity link $\\E(Y \\mid \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}$\n   -  Binary response: logit link $\\E(Y \\mid \\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}$\n   -  Multinomial response: softmax link \n      $$\n      \\E(Y \\mid \\boldsymbol{x}) = \\left[\\frac{\\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta_1})}{\\sum_{k=1}^K \\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta}_k)}, \\ldots, \\frac{\\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta_K})}{\\sum_{k=1}^K \\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta}_k)}\\right]^T\n      $$\n\n## Outline\n\n-  Empirical Risk Minimization (ERM)\n   -  A general framework for machine learning\n   -  Decomposition of the generalization error of a model\n-  Vapnik-Chervonenkis (VC) Theory\n   -  Measuring the complexity of a set of models\n   -  Providing an upper bound for the generalization error\n-  Validation of a trained model\n   -  Estimating the generalization error\n   -  $k$-fold cross-validation\n   -  Cross-validation for hyperparameter tuning \n\n## Different Types of Learning\n\n\nThere are many types of learning:\n\n-  Supervised Learning\n-  Unsupervised Learning\n-  Reinforcement Learning\n-  Semi-supervised Learning\n-  Active Learning\n-  Online Learning\n-  Transfer Learning\n-  Multi-task Learning\n-  Federated Learning, etc.\n\n## Supervised Learning\n\n-  The dataset consists of pairs $(x_i, y_i)$, $x_i \\in \\mathcal{X}$, $y_i \\in \\mathcal{Y}$, where $x_i$ is called the **feature** and $y_i$ is the associated **label**.\n   -  $\\mathcal{X} \\subseteq \\R^p$ is called the feature space (usually $\\mathcal{X} = \\R^p$)\n   -  $\\mathcal{Y} \\subseteq \\R^K$ is called the label space\n-  The goal is to learn a function $f: \\mathcal{X} \\to \\mathcal{Y}$ that maps the feature to the label.\n-  Examples:\n   -  image/text classification\n   -  prediction\n-  Commonly used models:\n   -  Linear regression/Logistic regression\n   -  Support vector machine (SVM)\n   -  Neural network, and many others\n\n## General Framework of Supervised Learning\n\n-  In this course, we will mainly focus on supervised learning.\n-  Supervised learning can also be viewed as a **function estimation** problem, i.e., estimating the function $f$ that maps the feature $x$ to the label $y$.\n-  Depending the types of labels, many different models have been developed.\n-  Instead of focusing on individual models, we will discuss a general framework for supervised learning, called **Empirical Risk Minimization (ERM)**.\n\n\n# Empirical Risk Minimization\n\n## Empirical Risk Minimization (ERM)\n\n-  The ERM principle for supervised learning requires:\n   -  A **loss function** $L(y, g(x))$ that measures the discrepancy between the true label $y$ and the predicted label $g(x)$.\n   -  A **hypothesis class** $\\mathcal{G}$ which is a class of functions $g: \\mathcal{X} \\to \\mathcal{Y}$.\n   -  A **training dataset**  $(x_1, y_1), \\ldots, (x_n, y_n)$.\n\n\n## Loss function\n\n-  A loss function $L: \\mathcal{Y} \\times \\R^K \\to \\R$ quantifies how well $\\hat{y}$ approximates $y$:\n   -  smaller values of $L(y, \\hat{y}$) indicate that $\\hat{y}$ is a good approximation of $y$\n   -  typically (but not always) $L(y, y) = 0$ and $L(y, \\hat{y}) \\geq 0$ for all $\\hat{y}$, and $y$\n-  Examples:\n   -  Quadratic loss: $L(y, \\hat{y}) = (y - \\hat{y})^2$ or $L(y, \\hat{y}) = \\|y - \\hat{y}\\|^2$\n   -  Absolute loss: $L(y, \\hat{y}) = |y - \\hat{y}|$\n   -  Cross-Entropy loss: $L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})$ or $L(y, \\hat{y}) = -\\sum_{i=1}^K y_i\\log\\hat{y}_i$\n\n## Risk Function\n\n-  Assume that $(X, Y) \\sim F$ and $F$ is an unknown distribution. \n-  Given a loss function ,the **risk function** of a model $h$ is \n   $$\n   R(h) = \\E_{(X, Y) \\sim F}[L(Y, h(X))].\n   $$\n-  The optimal $h$ is the one that minimizes the risk function: \n   $$\n   h^{\\star} = \\argmin_{h: \\mathcal{X} \\to \\mathcal{Y}} R(h).\n   $$\n-  Denote the **optimal risk** as $R^{\\star} = R(h^{\\star})$.\n-  However, it is impossible to obtain either $h^{\\star}$ or $R^{\\star}$ because:\n   1.  the space of all possible functions $\\{h: \\mathcal{X} \\to \\mathcal{Y}\\}$ is too large, and \n   2.  the data distribution $F$ is unknown.\n\n## Hypothesis Class\n\n-  To make the problem tractable, we restrict the space of functions to a **hypothesis class** $\\mathcal{H}$.\n-  We denote the **best model** in $\\mathcal{H}$ as $h_{\\mathcal{H}}^{\\star} = \\argmin_{h \\in \\mathcal{H}} R(h)$.\n-  Its associated risk is $R_{\\mathcal{H}}^{\\star} = R(h_{\\mathcal{H}}^{\\star})$.\n-  By definition, it is obvious that $R_{\\mathcal{H}}^{\\star} \\geq R^{\\star}$.\n-  Examples:\n   -  Linear regression: $\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\} = \\R^p$\n   -  Logistic regression: $\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\} = \\R^p$\n-  The difference between $R_{\\mathcal{H}}^{\\star}$ and $R^{\\star}$ is called the **approximation error**.\n-  Intuitively, the larger the hypothesis class, the smaller the approximation error.\n\n## Empirical Risk\n\n\n-  Assuming that $(x_1, y_1), \\ldots, (x_n, y_n) \\iid F$, the **empirical risk** is\n   $$\n    R_{\\text{emp}}(h) = \\E_{(X, Y) \\sim \\widehat{F}_n}[L(Y, h(X))]\n           = \\frac{1}{n} \\sum_{i=1}^n L(y_i, h(x_i))\n   $$\n   where $\\widehat{F}_n = \\frac{1}{n}\\sum_{i=1}^n \\delta_{(x_i, y_i)}$ is the empirical distribution of the data.\n-  We choose the $h$ that minimizes the empirical risk function, i.e., the **empirical risk minimizer**:\n   $$\n    \\hat{h}_{n, \\mathcal{H}} = \\argmin_{h \\in \\mathcal{H}} R_{\\text{emp}}(h)\n   $$\n   where $\\mathcal{H}$ is the hypothesis class.\n-  Denote the empirical risk associated with $\\hat{h}_{n, \\mathcal{H}}$ as $\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})$ and this is what we obtain in practice.\n\n## Quick Summary\n\n-  Goal: find the best model $h^{\\star} = \\argmin_h R(h)$, which is impossible since\n   -  the space of all possible functions is too large $\\textcolor{red}{\\rightarrow}$ [restrict to hypothesis class]{style=\"color:red;\"}\n   -  the data distribution is unknown $\\textcolor{red}{\\rightarrow}$ [use empirical data]{style=\"color:red;\"}\n-  We have three models:\n   -  $h^{\\star}$: the best model [(associated risk $R^{\\star} = R(h^{\\star})$)]{style=\"color:blue;\"}\n   -  $h_{\\mathcal{H}}^{\\star}$: the best model in the hypothesis class $\\mathcal{H}$ [(associated risk $R_{\\mathcal{H}}^{\\star} = R(h^{\\star}_{\\mathcal{H}})$)]{style=\"color:blue;\"}\n   -  $\\hat{h}_{n, \\mathcal{H}}$: the empirical risk minimizer, i.e., the **trained model** [(empirical risk, or the **training error**,  $\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})$)]{style=\"color:blue;\"}\n-  We want $\\hat{h}_{n, \\mathcal{H}}$ to be as close as possible to $h^{\\star}$ in terms of the risk function $R(h)$.\n\n## Error Decomposition\n\n-  Goal: $R(\\hat{h}_{n,\\mathcal{H}}) - R^{\\star} = R(\\hat{h}_{n,\\mathcal{H}}) - R(h^{\\star}) = 0$.\n-  Decomposition:\n   \\begin{align*}\n   R(\\hat{h}_{n,\\mathcal{H}}) - R^{\\star} & = \\underbrace{R(\\hat{h}_{n,\\mathcal{H}}) - R_{\\mathcal{H}}^{\\star}}_{\\textcolor{blue}{\\text{estimation error}}} \\quad + \\underbrace{R_{\\mathcal{H}}^{\\star} - R^{\\star}}_{\\textcolor{blue}{\\text{approximation error}}}\\\\\n   & = \\underbrace{R(\\hat{h}_{n,\\mathcal{H}}) - R(h_{\\mathcal{H}}^{\\star})}_{\\textcolor{blue}{\\text{estimation error}}} \\quad + \\quad \\underbrace{R(h_{\\mathcal{H}}^{\\star}) - R(h^{\\star})}_{\\textcolor{blue}{\\text{approximation error}}}\\\\\n   \\end{align*}\n-  The approximation error comes from the [use of a hypothesis class $\\mathcal{H}$]{style=\"color:red;\"}.\n   -  Larger $\\mathcal{H}$ $\\rightarrow$ smaller approximation error\n-  The estimation error comes from the [use of empirical data]{style=\"color:red;\"}.\n   -  More data $\\rightarrow$ smaller estimation error\n\n## Error Decomposition {.auto-stretch}\n\n![](./images/lec03/diagram.png){fig-align=\"center\" width=100%}\n\n## Example\n\n-  Linear Regression:\n   -   $\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}$\n   -   $L(y, h(\\boldsymbol{x})) = (y - h(\\boldsymbol{x}))^2$\n-  Logistic Regression:\n   -   $\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\}$\n   -   $L(y, h(\\boldsymbol{x})) = -y \\log(h(\\boldsymbol{x})) - (1 - y) \\log(1 - h(\\boldsymbol{x}))$\n-  (Linear) Support Vector Machine:\n   -   $\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{w}^T \\boldsymbol{x} + b, \\boldsymbol{w} \\in \\R^p, b \\in \\R\\}$\n   -   $L(y, h(\\boldsymbol{x})) = \\max(0, 1 - y \\cdot h(\\boldsymbol{x}))$\n\n## Maximum Likelihood (ML) v.s. ERM\n\n-  In fact, the ML principle is a special case of the ERM principle.\n-  That is, specifying a likelihood function gives a loss function, i.e., use the negative log-likelihood as the loss function.\n-  ML:\n   -  Stronger assumptions\n   -  Stronger guarantees (consistency, asymptotic normality, etc.)\n   -  Allow us to do more things (e.g., hypothesis testing and confidence intervals)\n   -  Linear regression and logistic regression are ML and hence ERM\n-  ERM:\n   -  More flexible and practical, but weaker guarantees\n   -  Usually provide only a point estimate\n   -  SVM is ERM but not ML\n\n\n\n\n## Constructing Learning Algorithms\n\n-  Following the ERM principle, we need to specify a **loss function** and a **hypothesis class** in order to construct a learning algorithm.\n-  The choice of the loss function is based on the types of labels and the problem.\n-  The choice of the hypothesis class is more challenging:\n   -  Smaller $\\mathcal{H}$ $\\rightarrow$ larger approximation error, smaller estimation error, and less overfitting\n   -  Larger $\\mathcal{H}$ $\\rightarrow$ smaller approximation error, larger estimation error, more overfitting, and requires more data\n-  Next, we will discuss:\n   -  how to measure the \"size\" (capacity/complexity) of a hypothesis class\n   -  how to choose an appropriate hypothesis class\n\n\n# Vapnik-Chervonenkis (VC) Theory\n\n## Complexity v.s. Dimension\n\n-  Let $\\mathcal{H}$ be a parametric hypothesis class ,e.g., $\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}$.\n-  An intuitive way to measure the complexity of $\\mathcal{H}$ is to count the number of unknown parameters, i.e., the dimension of $\\mathcal{H}$.\n-  In this case, the dimension of $\\dim(\\mathcal{H}) = p$.\n\n![](03-ml_basics_files/figure-revealjs/cell-2-output-1.png){width=792 height=431 fig-align='center'}\n\n\n## Shattering\n\nA hypothesis class $\\mathcal{H}$ is said to **shatter** a set of points $S = \\{x_1, \\ldots, x_n\\}$ if for all possible binary labelings (0/1) of these points, there exists a function $h \\in \\mathcal{H}$ that can perfectly separate the points.\n\n![](./images/lec03/shatter.png){width=100%}\n\n::: aside\nImage Source: Figure 7.6 of ESL\n:::\n\n\n## Shattering\n\n**Definition** (Restriction of $\\mathcal{H}$ to $S$) Let $\\mathcal{H}$ be a class of functions from $\\mathcal{X}$ to $\\{0,1\\}$ and let $S = \\{x_1, \\ldots, x_n\\} \\subset \\mathcal{X}$. The restriction of $\\mathcal{H}$ to $S$ is the set of functions from $S$ to $\\{0, 1\\}$ that can be derived from $\\mathcal{H}$. That is,\n$$\n   \\mathcal{H}_S = \\{(h(x_1), \\ldots, h(x_n)): h \\in \\mathcal{H}\\}\n$$\n\n**Definition** (Shattering) A hypothesis class $\\mathcal{H}$ shatters a finite set $S \\subset \\mathcal{X}$ if the restriction of $\\mathcal{H}$ to $S$ is the set of all functions from $S$ to $\\{0, 1\\}$. That is, $|\\mathcal{H}_S| = 2^{|S|}$.\n\n## Vapnik-Chervonenkis (VC) Dimension\n\n**Definition** (VC-dimension) The VC-dimension of a hypothesis class $\\mathcal{H}$, denoted $\\text{VC-dim}(\\mathcal{H})$, is the maximal size of a set $S \\subset \\mathcal{X}$ that can be shattered by $\\mathcal{H}$. If $\\mathcal{H}$ can shatter sets of arbitrarily large size we say that $\\mathcal{H}$ has **infinite VC-dimension**.\n\n-  One can show that for linear models, e.g., $\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\beta_0 + \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}$, the VC-dimension is $p+1$ (the same as the number of parameters).\n-  However, for nonlinear models, the calculation of the VC-dimension is often challenging.\n\n## Example (Infinite VC-dimension)\n\n-  Let $\\mathcal{H} = \\{h: h(x) = \\mathbb{I}(\\sin(\\alpha x) > 0), \\alpha > 0\\}$. Then $\\text{VC-dim}(\\mathcal{H}) = \\infty$.\n-  Proof: \n   -  For any $n$, let $x_1 = 2\\pi 10^{-1}, \\ldots, x_n = 2\\pi 10^{-n}$.\n   -  Then the parameter $\\alpha = \\frac{1}{2}\\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right)$ can perfectly separate the points, where $y_i \\in \\{0, 1\\}$ is any labeling of the points.\n\n![](./images/lec03/sin_shatter.png){fig-align=\"center\" width=80%}\n\n::: aside\nImage Source: Figure 7.5 of ESL\n:::\n\n\n## Goodness-of-fit v.s. Generalization ability\n\n-  Goodness-of-fit: how well the model fits the data.\n-  Generalization ability: how well the model generalizes to unseen data.\n-  Recall that for an ERM $\\hat{h}_{n, \\mathcal{H}}$, we have \n   -  training error (error on training data) $\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})$\n   -  generalization error (error on unseen data) $R(\\hat{h}_{n, \\mathcal{H}}) = \\E_{(X, Y) \\sim F} \\left[L(Y, \\hat{h}_{n,\\mathcal{H}}(X)) \\mid \\mathcal{T}\\right]$, where $\\mathcal{T}$ denotes the training dataset.\n-  We can write\n   \\begin{align*}\n      R(\\hat{h}_{n, \\mathcal{H}}) & = \\hat{R}_n + \\left(R(\\hat{h}_{n, \\mathcal{H}}) - \\hat{R}_n\\right)\\\\ \n      \\textcolor{blue}{\\text{generalization error}} & = \\textcolor{blue}{\\text{training error}} + \\textcolor{blue}{\\text{generalization gap}}\n   \\end{align*}\n\n\n## Overfitting and Underfitting\n\n-  To have low generalization error, we need to have both **low training error** and **small generalization gap**.\n   -  Large training error $\\rightarrow$ underfitting\n   -  Low training error but large generalization gap $\\rightarrow$ overfitting\n\n![](./images/lec03/overfitting.png){fig-align=\"center\" width=70%}\n\n::: aside\nImage Source: Figure 5.3 in DL\n:::\n\n## VC Inequality\n\n-  The VC theory provides an upper bound for the generalization gap, known as the **VC inequality**: with probability at least $1 - \\delta$\n   $$\n   R(h) \\leq R_{\\text{emp}}(h)+\\varepsilon \\sqrt{1+\\frac{4 R_{\\text{emp}}(h)}{\\varepsilon}}, \\quad \\varepsilon = O\\left(\\frac{d - \\log \\delta}{n}\\right)\n   $$\n   simultaneously for all $h \\in \\mathcal{H}$, where $\\text{VC-dim}(\\mathcal{H}) = d < \\infty$.\n-  The generalization gap increases as \n   -  the VC-dimension increases\n   -  the samples size $n$ decreases\n-  This upper bound is only a loose bound and does not work for models with infinite VC-dimension.\n\n## Regularized ERM\n\n-  To prevent overfitting, we can add a **regularization term** to the empirical risk:\n   $$\n   R_{\\text{reg}}(h) = R_{\\text{emp}}(h) + \\lambda \\Omega(h)\n   $$\n   where $\\Omega(h)$ is a regularization term and $\\lambda$ is the regularization parameter.\n-  Typically, $\\Omega(h)$ measures the smoothness or complexity of the model $h$.\n\n![](./images/lec03/Regularization.svg){fig-align=\"center\" width=30%}\n\n::: aside\nImage Source: <https://en.wikipedia.org/wiki/Regularization_(mathematics)>\n:::\n\n\n## Regularized ERM\n\n-  For example, $\\Omega(h) = \\|h^{\\prime}(x)\\|_2^2 = \\int \\left(h^{\\prime}(x)\\right)^2dx$. ($L_2$ Regularization)\n-  If $h(x) = \\beta_0 + \\beta_1 x$, then $h^{\\prime}(x) = \\beta_1$ and $\\Omega(h) = \\beta_1^2$.\n-  The $L_1$ regularization is $\\Omega(h) = \\|h^{\\prime}(x)\\|_1 = \\int |h^{\\prime}(x)|dx$.\n-  If $h(x) = \\beta_0 + \\beta_1 x$, then $h^{\\prime}(x) = \\beta_1$ and $\\Omega(h) = |\\beta_1|$.\n-  Using $L_1$ gives you [sparsity]{style=\"color:red;\"}; using $L_2$ gives you [smoothness/insensitivity]{style=\"color:red;\"}:\n   -  Consider a linear model $h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}$.\n   -  A good model should not be too sensitive to the input, i.e., small changes in the input should not lead to large changes in the output.\n   -  That is, if $\\boldsymbol{x} \\approx \\tilde{\\boldsymbol{x}}$ , then $|\\boldsymbol{x}^T \\boldsymbol{\\beta} - \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}|$ should be small.\n   -  Note that\n      $$\n      |\\boldsymbol{x}^T \\boldsymbol{\\beta} - \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}| = |(\\boldsymbol{x} - \\tilde{\\boldsymbol{x}})^T \\boldsymbol{\\beta}| \\leq \\|\\boldsymbol{x} - \\tilde{\\boldsymbol{x}}\\|_2 \\|\\boldsymbol{\\beta}\\|_2\n      $$\n\n## Bias-Variance Tradeoff\n\n-  Adding a reguralization term often increases the bias but reduces the variance of the model.\n-  This tradeoff is known as the **bias-variance tradeoff**.\n\n![](./images/lec03/bias_variance.png){fig-align=\"center\" width=70%}\n\n::: aside\nImage Source: Figure 5.6 in DL\n:::\n\n## Quick Summary\n\n-  The VC dimension measures the complexity of a hypothesis class.\n-  The VC inequality provides an upper bound for the generalization gap, provided that the VC dimension is finite.\n-  The bound is often criticized for being too loose and does not work for models with infinite VC dimension.\n-  Example of infinite VC dimension: \n   -  Neural Networks\n   -  Kernel methods (e.g., kernel SVM, kernel regression)\n   -  $K$-nearest neighbors (with small $K$, say $K = 1$)\n\n## Double Descent Curve\n\n![](images/lec03/double_descent.png){width=100%}\n\n::: aside\nBelkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning practice and the classical biasâ€“variance trade-off. *Proceedings of the National Academy of Sciences*, 116(32), 15849-15854.\n:::\n\n# Validation \n\n## Estimating the Generalization Error\n\n-  Although the VC inequality provides an upper bound for the generalization gap, it is often too loose.\n-  In order to have a more accurate insight into the model's generalization ability, we need to estimate the generalization error.\n-  To achieve this, we need to have an extra dataset, called the **validation dataset** $\\mathcal{V} = \\{(\\tilde{x}_i, \\tilde{y}_i)\\}_{i=1}^m$.\n-  The generalization error is then estimated as\n   $$\n   \\hat{R}_{\\text{gen}} = \\frac{1}{m} \\sum_{i=1}^m L(\\tilde{y}_i, \\hat{h}_{n, \\mathcal{H}}(\\tilde{x}_i)).\n   $$\n-  Assuming the $\\mathcal{V}$ and $\\mathcal{T}$ (training dataset) are i.i.d, $\\hat{R}_{\\text{gen}}$ is an unbiased estimate of the generalization error, i.e., $\\E[\\hat{R}_{\\text{gen}} \\mid \\mathcal{T}] = R(\\hat{h}_{n, \\mathcal{H}})$.\n\n## $k$-fold Cross-Validation (CV)\n\n-  In practice, we often do not have an extra validation dataset and hence we need to use the training dataset to estimate the generalization error.\n-  One common method is the $k$-fold cross-validation:\n   1.  Split the training dataset $\\mathcal{T}$ into $k$ equal-sized folds.\n   2.  For each fold $i = 1, \\ldots, k$, train the model on the remaining $k-1$ folds and evaluate the model on the $i$th fold.\n   3.  Average the $k$ validation errors to obtain the estimated generalization error.\n\n![](images/lec03/k_fold_CV.png){fig-align=\"center\" width=80%}\n\n## $k$-fold Cross-Validation\n\n-  When $k = n$, it is called the **leave-one-out cross-validation (LOOCV)**, i.e., train the model on $n-1$ samples and evaluate on the remaining one.\n-  Choice of $k$?\n   -  Larger $k$ $\\rightarrow$ [low bias, high variance]{style=\"color:blue;\"} (the model is trained on a larger dataset and validated on a smaller dataset)\n   -  Smaller $k$ $\\rightarrow$ [high bias, low variance]{style=\"color:blue;\"} (the model is trained on a smaller dataset and validated on a larger dataset)\n   -  $k = 5$ or $k = 10$ are common choices.\n\n## CV for Hyperparameter Tuning\n\n-  In practice, the models often have hyperparameters that need to be tuned, e.g., the regularization parameter $\\lambda$.\n-  We can use CV to choose the best hyperparameters:\n   1.  For each hyperparameter value, perform $k$-fold CV to estimate the generalization error.\n   2.  Choose the hyperparameter value that minimizes the CV error.\n-  However, the CV error after the **selection** will overestimate the generalization error.\n-  Such bias is known as the **selection bias**.\n  \n## CV for Hyperparameter Tuning\n\n-  To avoid the selection bias, we first split the dataset into two parts: the **training dataset** and the **test dataset**.\n-  The test dataset should not be used in the neither the traing process nor hyperparameter tuning process.\n-  The training dataset is further split into $k$-folds for CV.\n-  After all the processes, including training, hyperparameter tuning, model selection, etc., we evaluate the final model on the test dataset to estimate the generalization error.\n\n## Example: Using CV to Choose the Regularization Parameter\n\n::: {#9d42ca37 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import datasets\n\nX, y = datasets.load_diabetes(return_X_y=True)\n\nX /= X.std(axis=0)\n\nalpha_seq = np.logspace(-2, 2, 100)\nreg = LassoCV(alphas = alpha_seq, cv = 5, random_state = 42)\nreg.fit(X, y)\n\nprint(\"best alpha:\", np.round(reg.alpha_, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbest alpha: 0.0774\n```\n:::\n:::\n\n\n## Example: Using CV to Choose the Regularization Parameter\n\n::: {#3a64a38c .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](03-ml_basics_files/figure-revealjs/cell-4-output-1.png){width=819 height=451 fig-align='center'}\n:::\n:::\n\n\n# Appendix\n\n## Proof of Example (Infinite VC-dimension)\n\n-  We need to show that the model $h(x) = \\mathbb{I}(\\sin(\\alpha x) > 0)$ with $\\alpha = \\frac{1}{2}\\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right)$ can perfectly separate the $n$ points.\n-  Consider the $j$th sample $x_j = 2\\pi 10^{-j}$.\n-  If $y_j = 0$, then \n   \\begin{align*}\n      \\alpha x_j & = \\pi 10^{-j} \\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right) = \\pi 10^{-j} \\left(1 + \\sum_{\\{i: y_i = 0\\}} 10^i\\right)\\\\\n      & = \\pi 10^{-j}\\left(1 + 10^j + \\sum_{\\{i: y_i = 0, i \\neq j\\}} 10^i\\right)\\\\\n      & = \\pi \\left(10^{-j} + 1 + \\sum_{\\{i: y_i = 0, i > j\\}} 10^{i-j} + \\sum_{\\{i: y_i = 0, i < j\\}} 10^{i-j}\\right)\n   \\end{align*}\n\n::: aside\nReference: <https://mlweb.loria.fr/book/en/VCdiminfinite.html>\n:::\n\n## Proof of Example (Infinite VC-dimension)\n\n-  For $i>j$, $10^{i-j}$ is even and so is $\\sum_{\\{i: y_i = 0, i > j\\}} 10^{i-j}$, say \n   $$\n   \\sum_{\\{i: y_i = 0, i > j\\}} 10^{i-j} = 2m, \\quad m \\in \\mathbb{N}. \n   $$\n-  Note that \n   $$\n   \\sum_{\\{i: y_i = 0, i < j\\}} 10^{i-j} < \\sum_{i=1}^{\\infty} 10^{-i} = \\sum_{i=0}^{\\infty} 10^{-i} - 1 = \\frac{1}{1-0.1} - 1 = \\frac{1}{9}.  \n   $$\n-  Therefore, $\\alpha x_j = \\pi(1 + 2m +\\epsilon)$, where \n   $$\n   0 < \\epsilon = 10^{-j} + \\sum_{\\{i: y_i = 0, i < j\\}} 10^{i-j} < \\frac{1}{10} + \\frac{1}{9} < 1.\n   $$\n-  Hence $\\sin(\\alpha x_j) < 0$ and $h(x_j) = 0$.\n\n## Proof of Example (Infinite VC-dimension)\n\n-  If $y_j = 1$, then \n   \\begin{align*}\n      \\alpha x_j & = \\pi 10^{-j} \\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right) = \\pi 10^{-j} \\left(1 + \\sum_{\\{i: y_i = 0\\}} 10^i\\right)\\\\\n      & = \\pi \\left(10^{-j} + \\sum_{\\{i: y_i = 0, i > j\\}} 10^{i-j} + \\sum_{\\{i: y_i = 0, i < j\\}} 10^{i-j}\\right)\n   \\end{align*}\n-  Similarly, we have $\\alpha x_j = \\pi(2m +\\epsilon)$, where \n   $$\n   0 < \\epsilon = 10^{-j} + \\sum_{\\{i: y_i = 0, i < j\\}} 10^{i-j} < \\frac{1}{10} + \\frac{1}{9} < 1.\n   $$\n-  Hence $\\sin(\\alpha x_j) > 0$ and $h(x_j) = 1$. \n\n",
    "supporting": [
      "03-ml_basics_files"
    ],
    "filters": [],
    "includes": {}
  }
}