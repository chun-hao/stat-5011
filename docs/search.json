[
  {
    "objectID": "slides/07-implementation.html#popular-deep-learning-frameworks",
    "href": "slides/07-implementation.html#popular-deep-learning-frameworks",
    "title": "Introduction to PyTorch",
    "section": "Popular Deep Learning Frameworks",
    "text": "Popular Deep Learning Frameworks\n\nThere are several deep learning frameworks:\n\nTensorFlow (2015): Developed by Google Brain Team.\nPyTorch (2016): Developed by Facebook AI Research Lab.\nJAX (2018): Developed by Google Brain Team.\nMXNet (2015): Developed by Apache Software Foundation.\nKeras (2015): A high-level API that can run on top of JAX, TensorFlow, or PyTorch.\n\nPyTorch and TensorFlow/Keras are the most popular deep learning frameworks:\n\nPyTorch is more widely used in the research community.\nTensorFlow is more widely used in the industry.\n\nIn today’s lecture, we will focus on PyTorch."
  },
  {
    "objectID": "slides/07-implementation.html#outline",
    "href": "slides/07-implementation.html#outline",
    "title": "Introduction to PyTorch",
    "section": "Outline",
    "text": "Outline\n\nBasic concepts of PyTorch\n\nTensor\nAutograd\n\nMain loop for training models\n\nLoading Data and data preparation\nBuilding a Neural Network\nTraining and Validation Loop\n\nTensorboard Visualization\n\nTracking the training process\nVisualizing the network architecture"
  },
  {
    "objectID": "slides/07-implementation.html#tensor",
    "href": "slides/07-implementation.html#tensor",
    "title": "Introduction to PyTorch",
    "section": "Tensor",
    "text": "Tensor\n\nTensors are multi-dimensional arrays.\n\n0-dim array is called scalar.\n1-dim array is called vector.\n2-dim array is called matrix.\n\nIn PyTorch, everything is a tensor: data, paremeter, gradient, etc. A tensor can be created from a Python list or sequence with the torch.tensor() function.\n\n\nimport torch\n\na = torch.tensor([1]) # scalar\nb = torch.tensor([1, 2, 3]) # vector\nc = torch.tensor([[1, 2], [3, 4]]) # matrix\nd = torch.tensor([[[1, 2], [3, 4]], \n                  [[5, 6], [7, 8]]]) # 3D tensor\n\n\nThe shape attribute is used to get the shape of a tensor\n\n\nprint(d.shape)\n\ntorch.Size([2, 2, 2])"
  },
  {
    "objectID": "slides/07-implementation.html#tensor-vs.-numpy-array",
    "href": "slides/07-implementation.html#tensor-vs.-numpy-array",
    "title": "Introduction to PyTorch",
    "section": "Tensor vs. Numpy array",
    "text": "Tensor vs. Numpy array\n\nPyTorch tensors are very similar to Numpy arrays.\nThe two main differences are\n\nPyTorch tensors can run on GPUs\nPyTorch tensors are better integrated with PyTorch’s autograd.\n\nThe required_grad attribute is used to track the gradient of the tensor.\n\nIf required_grad=True, the gradient of the tensor will be computed during the backpropagation.\n\nWe can easily convert a PyTorch tensor to a Numpy array and vice versa using\n\ntorch.tensor.numpy()\ntorch.tensor.from_numpy()"
  },
  {
    "objectID": "slides/07-implementation.html#autograd",
    "href": "slides/07-implementation.html#autograd",
    "title": "Introduction to PyTorch",
    "section": "Autograd",
    "text": "Autograd\n\nWhen creating tensors with requires_grad=True, it signals to autograd that every operation on them should be tracked.\nWe call the function .backward() on the final tensor to compute the gradient.\n\n\nx = torch.tensor([3.0], requires_grad=True)\ny = x**2\ny.backward()\nprint(x.grad.item()) # dy/dx = 2x and when x = 3, dy/dx = 6\n\n6.0\n\n\n\nIt also works for general multi-dimensional tensors and matrix operations.\n\n\nx = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\ny = torch.trace(torch.matmul(x.t(), x))\ny.backward()\nprint(x.grad) \n\ntensor([[2., 4.],\n        [6., 8.]])\n\n\n\\[\ny = \\operatorname{tr}(X^TX) \\Rightarrow \\frac{\\partial y}{\\partial X} = 2X\n\\]"
  },
  {
    "objectID": "slides/07-implementation.html#object-oriented-programming-oop-in-python",
    "href": "slides/07-implementation.html#object-oriented-programming-oop-in-python",
    "title": "Introduction to PyTorch",
    "section": "Object-oriented programming (OOP) in Python",
    "text": "Object-oriented programming (OOP) in Python\n\nA class is a prototype of objects.\n\n\nclass person:\n    def __init__(self, name, age): # instance constructor\n        self.name = name # attribute\n        self.age = age\n    def get_name(self): # method\n        return self.name\n\njohn = person(\"John\", 20) # john is an instance of the class 'person'\n\n\nA class contains attributes (name and age) and methods (get_name).\nWe can define a subclass that inherits from a parent class.\n\n\nclass student(person):\n    def __init__(self, name, age, major): \n        super().__init__(name, age) # call the parent class constructor\n        self.major = major\n    def get_major(self):\n        return self.major\n\njohn = student(\"John\", 20, \"Stat\")\nprint(f\"{john.get_name()} is majoring in {john.get_major()}.\")\n\nJohn is majoring in Stat."
  },
  {
    "objectID": "slides/07-implementation.html#data-preparation-1",
    "href": "slides/07-implementation.html#data-preparation-1",
    "title": "Introduction to PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nIn the data preparation stage, we need to do the following:\n\nSplit the data into training, validation, and test sets\nSplit the data into mini-batches\n\nA dataset is stored in a Dataset class. Inside the dataset calss, we can\n\ndownload/load the data\npreprocess the data (standardization, transformation, etc.)\n\nTo feed the dataset to our model, we need the DataLoader class, which\n\nshuffles the data indices\nsplits the data into mini-batches"
  },
  {
    "objectID": "slides/07-implementation.html#dataset",
    "href": "slides/07-implementation.html#dataset",
    "title": "Introduction to PyTorch",
    "section": "Dataset",
    "text": "Dataset\n\nfrom torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.tensor(x).float()\n        self.y = torch.tensor(y).float()\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\nYou need to implement three methods in the Dataset class:\n\n__init__: Read data and preprocess\n__getitem__: Return one sample at a time.\n__len__: Return the size of the dataset."
  },
  {
    "objectID": "slides/07-implementation.html#example",
    "href": "slides/07-implementation.html#example",
    "title": "Introduction to PyTorch",
    "section": "Example",
    "text": "Example\n\nimport numpy as np\n\nrng = np.random.default_rng(20241029)\nn = 30\np = 2\n\n# Generate x_1, x_2, ..., x_30 from a normal distribution\nx = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n\n# Generate y from the linear model y = 2 - x_1 + 3*x_2\nbeta = np.array([-1, 3])\ny = 2 + x.dot(beta).reshape(-1,1) + rng.normal(loc=0.0, scale=0.1, size=(n, 1))\n\ntraining_data = MyDataset(x, y)\nprint(\"The number fo samples is\", training_data.__len__())\nprint(\"The first sample is (x_1, x_2, y) =\", training_data.__getitem__(0))\n\nThe number fo samples is 30\nThe first sample is (x_1, x_2, y) = (tensor([-0.7578,  1.2519]), tensor([6.6215]))"
  },
  {
    "objectID": "slides/07-implementation.html#dataloader",
    "href": "slides/07-implementation.html#dataloader",
    "title": "Introduction to PyTorch",
    "section": "DataLoader",
    "text": "DataLoader\n\nThe DataLoader class will:\n\nshuffle the data indices (if shuffle=True)\nsplit the data into mini-batches (if batch_size is specified)\n\n\n\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(training_data, batch_size=10, shuffle=True)\n\n\nThe DataLoader is an iterable. After we iterate over all batches, the data will be shuffled again.\n\n\nx_batch, y_batch = next(iter(train_loader))\nprint(f\"Batch size is {len(x_batch)}\")\nprint(f\"Batch mean of x is {x_batch.mean(axis = 0)}\")\nprint(f\"Batch mean of y is {y_batch.mean(axis = 0)}\")\n\nBatch size is 10\nBatch mean of x is tensor([-0.1499, -0.1101])\nBatch mean of y is tensor([1.7837])"
  },
  {
    "objectID": "slides/07-implementation.html#layers",
    "href": "slides/07-implementation.html#layers",
    "title": "Introduction to PyTorch",
    "section": "Layers",
    "text": "Layers\n\nA neural network model is built by stacking layers.\nPyTorch provides many predefined layers that can be used to build a neural network.\nFor example, torch.nn.Linear is a fully connected layer.\n\n\nlin = torch.nn.Linear(2, 3) # 2 input features and 3 output features\n\n\nThe parameters of the layer can be accessed using the state_dict() method or directly accessing the attributes\n\n\nprint(lin.state_dict())\n\nOrderedDict([('weight', tensor([[-0.5396,  0.4724],\n        [ 0.4363,  0.3894],\n        [-0.6281, -0.3871]])), ('bias', tensor([0.2805, 0.6208, 0.5337]))])\n\n\n\nprint(lin.weight)\n\nParameter containing:\ntensor([[-0.5396,  0.4724],\n        [ 0.4363,  0.3894],\n        [-0.6281, -0.3871]], requires_grad=True)"
  },
  {
    "objectID": "slides/07-implementation.html#available-layers",
    "href": "slides/07-implementation.html#available-layers",
    "title": "Introduction to PyTorch",
    "section": "Available Layers",
    "text": "Available Layers\nSee https://pytorch.org/docs/stable/nn.html"
  },
  {
    "objectID": "slides/07-implementation.html#building-a-neural-network",
    "href": "slides/07-implementation.html#building-a-neural-network",
    "title": "Introduction to PyTorch",
    "section": "Building a Neural Network",
    "text": "Building a Neural Network\n\nThe torch.nn.Module class is the base class for all neural network modules.\nThe torch.nn.Sequential class is a subclass of torch.nn.Module that is used to sequentially stack layers.\nThe following example defines a simple neural network with one hidden layer and ReLU activation function\n\n\nmodel = nn.Sequential(\n    nn.Linear(input_dim, hidden_dim),\n    nn.ReLU(), # activation function\n    nn.Linear(hidden_dim, output_dim)\n)\n\n\nHowever, not all neural networks can be defined using torch.nn.Sequential, for example, ResNet, recurrent networks, etc.\nWe can also define a custom neural network by subclassing torch.nn.Module."
  },
  {
    "objectID": "slides/07-implementation.html#nn.module",
    "href": "slides/07-implementation.html#nn.module",
    "title": "Introduction to PyTorch",
    "section": "nn.Module",
    "text": "nn.Module\nA basic nn.Module subclass is as follows:\n\nclass model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Define the layers\n        self.linear1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Define the forward pass, i.e., how to compute the output from the input\n        x = self.linear1(x)\n        x = self.relu(x)\n        y = self.linear2(x)\n        return y\n\n\nThis model is exactly the same as the previous one.\nIn the __init__ method, we define the layers that will be used by the model.\nIn the forward method, we define how the output \\(y\\) is obtained from the input \\(x\\)."
  },
  {
    "objectID": "slides/07-implementation.html#example-residual-layer",
    "href": "slides/07-implementation.html#example-residual-layer",
    "title": "Introduction to PyTorch",
    "section": "Example: Residual Layer",
    "text": "Example: Residual Layer\n\nRecall that a residual layer is defined as \\(y = x + f(x)\\).\nThe \\(f(x)\\) can be defined with different number/type of layers, activation functions, etc.\nSo you won’t find a predefined residual layer in PyTorch."
  },
  {
    "objectID": "slides/07-implementation.html#loss-function",
    "href": "slides/07-implementation.html#loss-function",
    "title": "Introduction to PyTorch",
    "section": "Loss Function",
    "text": "Loss Function\n\nAfter defining the model and the data, we need to define the loss function.\nThere are many loss functions available in PyTorch:\n\ntorch.nn.MSELoss: Mean Squared Error\ntorch.nn.CrossEntropyLoss: Cross Entropy\ntorch.nn.L1Loss: L1 Loss\ntorch.nn.PoissonNLLLoss: Poisson Negative Log Likelihood\n\nSee https://pytorch.org/docs/stable/nn.html#loss-functions\nYou can also define your own loss function. For example,\n\n\ndef my_MSE(output, target):\n    loss = torch.mean((output - target)**2)\n    return loss"
  },
  {
    "objectID": "slides/07-implementation.html#optimizer",
    "href": "slides/07-implementation.html#optimizer",
    "title": "Introduction to PyTorch",
    "section": "Optimizer",
    "text": "Optimizer\n\nThe optimizer is used to update the parameters of the model.\nThere are many optimizers available in PyTorch:\n\ntorch.optim.SGD: Stochastic Gradient Descent\ntorch.optim.Adam: Adam\n\nSee https://pytorch.org/docs/stable/optim.html\nThere are some important methods in the optimizer:\n\nzero_grad(): Clear the gradient stored in the optimizer\nstep(): Update the parameters\n\n\n\noptim = torch.optim.SGD(model.parameters(), lr=0.05)\n# Compute the gradient \n...\n# Update the parameters\noptim.step()\n# Clear the gradient\noptim.zero_grad()"
  },
  {
    "objectID": "slides/07-implementation.html#training-loop",
    "href": "slides/07-implementation.html#training-loop",
    "title": "Introduction to PyTorch",
    "section": "Training Loop",
    "text": "Training Loop\nHence a standard training loop looks like this:\n\ndef training_loop(dataloader, model, loss_fn, optimizer, n_epochs):\n    for epoch in range(n_epochs):\n        for x_batch, y_batch in dataloader:\n\n            # Compute prediction and loss\n            y_pred = model(x_batch)\n            loss = loss_fn(y_pred, y_batch)\n\n            # Backpropagation\n            loss.backward() # compute the gradient\n            optimizer.step() # update the parameters\n            optimizer.zero_grad() # clear the gradient stored in the optimizer\n\n        # print the training progress\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch+1}, Loss = {loss.item():.3f}\")"
  },
  {
    "objectID": "slides/07-implementation.html#example-1",
    "href": "slides/07-implementation.html#example-1",
    "title": "Introduction to PyTorch",
    "section": "Example",
    "text": "Example\nWe now have all the components (data, model, loss, and optimizer) to train the model.\n\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n\ntraining_loop(train_loader, model, loss_fn, optimizer, 100)\n\nEpoch 1, Loss = 13.321\nEpoch 11, Loss = 1.536\nEpoch 21, Loss = 0.365\nEpoch 31, Loss = 0.067\nEpoch 41, Loss = 0.045\nEpoch 51, Loss = 0.025\nEpoch 61, Loss = 0.853\nEpoch 71, Loss = 0.028\nEpoch 81, Loss = 0.066\nEpoch 91, Loss = 0.050"
  },
  {
    "objectID": "slides/07-implementation.html#quick-summary",
    "href": "slides/07-implementation.html#quick-summary",
    "title": "Introduction to PyTorch",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nWith PyTorch, we train a model using the following steps:\n\nDefine the model using torch.nn.Module\nDefine the loss function (choose from torch.nn or define your own)\nDefine the optimizer (choose from torch.optim)\nWrite a training loop\n\nWe can also write a validation loop to evaluate the model on the validation set.\nThe validation loop is similar to the training loop, but we don’t need to compute the gradient and update the parameters."
  },
  {
    "objectID": "slides/07-implementation.html#tensorboard",
    "href": "slides/07-implementation.html#tensorboard",
    "title": "Introduction to PyTorch",
    "section": "TensorBoard",
    "text": "TensorBoard\nTensorBoard is a visualization tool provided by TensorFlow, but it can also be used with PyTorch."
  },
  {
    "objectID": "slides/07-implementation.html#tracking-the-trainingvalidation-loss",
    "href": "slides/07-implementation.html#tracking-the-trainingvalidation-loss",
    "title": "Introduction to PyTorch",
    "section": "Tracking the training/validation loss",
    "text": "Tracking the training/validation loss\n\nUse the torch.utils.tensorboard.SummaryWriter class to log the training process.\nThe add_scalar method is used to log the scalar value.\n\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter('runs/example')\n\nfor epoch in range(100):\n    running_loss = 0.0\n    for x_batch, y_batch in train_loader:\n        # Compute prediction and loss\n        y_pred = model(x_batch)\n        loss = loss_fn(y_pred, y_batch)\n        # Backpropagation\n        loss.backward() # compute the gradient\n        optimizer.step() # update the parameters\n        optimizer.zero_grad() # clear the gradient stored in the optimizer\n\n        running_loss += loss.item()\n    \n    avg_loss = running_loss / len(train_loader)\n    writer.add_scalar('Loss/Train', avg_loss, epoch + 1)\n    writer.flush()"
  },
  {
    "objectID": "slides/07-implementation.html#view-the-network-architecture",
    "href": "slides/07-implementation.html#view-the-network-architecture",
    "title": "Introduction to PyTorch",
    "section": "View the network architecture",
    "text": "View the network architecture\nUse add_graph to visualize the network architecture.\n\ndataiter = iter(train_loader)\nx, y= next(dataiter)\n\nwriter.add_graph(model, x)\nwriter.flush()"
  },
  {
    "objectID": "slides/07-implementation.html#save-and-load-models",
    "href": "slides/07-implementation.html#save-and-load-models",
    "title": "Introduction to PyTorch",
    "section": "Save and Load Models",
    "text": "Save and Load Models\n\nThe model information is stored in the state_dict attribute.\n\n\nmodel.state_dict()\n\nOrderedDict([('0.model.0.weight',\n              tensor([[-0.4887, -0.8192],\n                      [ 0.6946,  0.5337],\n                      [ 0.1736,  0.0551],\n                      [ 0.4247, -1.0334]])),\n             ('0.model.0.bias', tensor([-0.3327,  0.5050, -0.6019, -0.0285])),\n             ('0.model.2.weight',\n              tensor([[ 0.6055, -0.6635,  0.4054,  0.5843],\n                      [ 0.2425, -0.1477, -0.2659,  0.7719],\n                      [ 0.1666, -0.4641, -0.0790, -0.1267],\n                      [-0.4862, -0.1035, -0.2386, -0.1672]])),\n             ('0.model.2.bias', tensor([ 0.0108, -0.1911, -0.0543, -0.2758])),\n             ('0.model.4.weight',\n              tensor([[ 0.9599,  0.8796, -0.0876,  0.0927],\n                      [ 0.2789,  0.1629,  0.4322,  0.0858]])),\n             ('0.model.4.bias', tensor([0.7170, 1.1826])),\n             ('2.model.0.weight',\n              tensor([[ 0.5355, -0.1691],\n                      [-0.0610,  0.5382],\n                      [-0.7051, -0.3227],\n                      [ 0.4620, -0.4787]])),\n             ('2.model.0.bias', tensor([ 0.0993,  0.0737, -0.2287, -0.5889])),\n             ('2.model.2.weight',\n              tensor([[-0.5853,  0.3475,  0.2194, -0.2362],\n                      [ 0.0463,  0.2101, -0.2726,  0.0574],\n                      [-0.4221, -0.3367, -0.4742,  0.2873],\n                      [-0.1169, -0.0678, -0.4661, -0.0395]])),\n             ('2.model.2.bias', tensor([-0.0908, -0.4833,  0.5289,  0.1900])),\n             ('2.model.4.weight',\n              tensor([[-0.0702, -0.2431, -0.2679, -0.3471],\n                      [ 0.2825,  0.1204, -0.2197, -0.1854]])),\n             ('2.model.4.bias', tensor([ 0.0370, -0.1809])),\n             ('4.weight', tensor([[-1.0629,  2.5723]])),\n             ('4.bias', tensor([0.0272]))])\n\n\n\nWe can save the model to a file and load it back.\n\n\ntorch.save(model.state_dict(), \"my_model.pth\")"
  },
  {
    "objectID": "slides/07-implementation.html#some-available-models",
    "href": "slides/07-implementation.html#some-available-models",
    "title": "Introduction to PyTorch",
    "section": "Some available models",
    "text": "Some available models\n\nIf we want to use the ResNet model structure for image classification and the image size is 200x200, we can use the following code to get the model structure.\nNote that this model is defined for ImageNet, which has 1000 classes. If you want to use it for a different number of classes, you need to change the last layer.\n\n\nimport torchvision.models as models\nfrom torchsummary import summary\n\nresnet = models.resnet18(weights=None) # No weights - random initialization\nsummary(resnet, (3, 200, 200))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 100, 100]           9,408\n       BatchNorm2d-2         [-1, 64, 100, 100]             128\n              ReLU-3         [-1, 64, 100, 100]               0\n         MaxPool2d-4           [-1, 64, 50, 50]               0\n            Conv2d-5           [-1, 64, 50, 50]          36,864\n       BatchNorm2d-6           [-1, 64, 50, 50]             128\n              ReLU-7           [-1, 64, 50, 50]               0\n            Conv2d-8           [-1, 64, 50, 50]          36,864\n       BatchNorm2d-9           [-1, 64, 50, 50]             128\n             ReLU-10           [-1, 64, 50, 50]               0\n       BasicBlock-11           [-1, 64, 50, 50]               0\n           Conv2d-12           [-1, 64, 50, 50]          36,864\n      BatchNorm2d-13           [-1, 64, 50, 50]             128\n             ReLU-14           [-1, 64, 50, 50]               0\n           Conv2d-15           [-1, 64, 50, 50]          36,864\n      BatchNorm2d-16           [-1, 64, 50, 50]             128\n             ReLU-17           [-1, 64, 50, 50]               0\n       BasicBlock-18           [-1, 64, 50, 50]               0\n           Conv2d-19          [-1, 128, 25, 25]          73,728\n      BatchNorm2d-20          [-1, 128, 25, 25]             256\n             ReLU-21          [-1, 128, 25, 25]               0\n           Conv2d-22          [-1, 128, 25, 25]         147,456\n      BatchNorm2d-23          [-1, 128, 25, 25]             256\n           Conv2d-24          [-1, 128, 25, 25]           8,192\n      BatchNorm2d-25          [-1, 128, 25, 25]             256\n             ReLU-26          [-1, 128, 25, 25]               0\n       BasicBlock-27          [-1, 128, 25, 25]               0\n           Conv2d-28          [-1, 128, 25, 25]         147,456\n      BatchNorm2d-29          [-1, 128, 25, 25]             256\n             ReLU-30          [-1, 128, 25, 25]               0\n           Conv2d-31          [-1, 128, 25, 25]         147,456\n      BatchNorm2d-32          [-1, 128, 25, 25]             256\n             ReLU-33          [-1, 128, 25, 25]               0\n       BasicBlock-34          [-1, 128, 25, 25]               0\n           Conv2d-35          [-1, 256, 13, 13]         294,912\n      BatchNorm2d-36          [-1, 256, 13, 13]             512\n             ReLU-37          [-1, 256, 13, 13]               0\n           Conv2d-38          [-1, 256, 13, 13]         589,824\n      BatchNorm2d-39          [-1, 256, 13, 13]             512\n           Conv2d-40          [-1, 256, 13, 13]          32,768\n      BatchNorm2d-41          [-1, 256, 13, 13]             512\n             ReLU-42          [-1, 256, 13, 13]               0\n       BasicBlock-43          [-1, 256, 13, 13]               0\n           Conv2d-44          [-1, 256, 13, 13]         589,824\n      BatchNorm2d-45          [-1, 256, 13, 13]             512\n             ReLU-46          [-1, 256, 13, 13]               0\n           Conv2d-47          [-1, 256, 13, 13]         589,824\n      BatchNorm2d-48          [-1, 256, 13, 13]             512\n             ReLU-49          [-1, 256, 13, 13]               0\n       BasicBlock-50          [-1, 256, 13, 13]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.46\nForward/backward pass size (MB): 51.08\nParams size (MB): 44.59\nEstimated Total Size (MB): 96.13\n----------------------------------------------------------------"
  },
  {
    "objectID": "slides/07-implementation.html#pre-trained-weights",
    "href": "slides/07-implementation.html#pre-trained-weights",
    "title": "Introduction to PyTorch",
    "section": "Pre-trained weights",
    "text": "Pre-trained weights\n\nWe can also use the weights that are trained on the ImageNet dataset.\n\n\nresnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n\n\nThis can be useful when you have a small dataset and you want to use the pre-trained weights to improve the performance."
  },
  {
    "objectID": "slides/07-implementation.html#fine-tuning",
    "href": "slides/07-implementation.html#fine-tuning",
    "title": "Introduction to PyTorch",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nThe last layer of ResNet18 is the fully connected layer with 512 input features and 1000 output features\n\n\nresnet.fc\n\nLinear(in_features=512, out_features=1000, bias=True)\n\n\n\nIf we want to use it for a 10-class classification problem, we need to change the last layer.\n\n\nresnet.fc = nn.Linear(512, 10)\nresnet.fc\n\nLinear(in_features=512, out_features=10, bias=True)\n\n\n\nFor efficiency, we can freeze the weights of the previous layers and only train the last layer.\n\n\n# freeze all layers by setting the requires_grad attribute to False\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# unfreeze the last layer\nfor param in resnet.fc.parameters():\n    param.requires_grad = True"
  },
  {
    "objectID": "slides/07-implementation.html#pytorch-lightning",
    "href": "slides/07-implementation.html#pytorch-lightning",
    "title": "Introduction to PyTorch",
    "section": "PyTorch Lightning",
    "text": "PyTorch Lightning\n\nPyTorch offers a lot of flexibility, but it can be cumbersome to write the training loop, validation loop, etc.\nLightning is a lightweight PyTorch wrapper.\nThe Lightning class (LightningModule) is exactly the same as the PyTorch, except that the LightningModule provides a structure for the research code.\nMore specifically, there are two main classes in Lightning: LightningModule and LightningDataModule."
  },
  {
    "objectID": "slides/07-implementation.html#lightning-vs.-pytorch",
    "href": "slides/07-implementation.html#lightning-vs.-pytorch",
    "title": "Introduction to PyTorch",
    "section": "Lightning vs. PyTorch",
    "text": "Lightning vs. PyTorch\n\nLightning provides a more structured way to write PyTorch code.\nThe Lightning Trainer automates many things, such as:\n\nEpoch and batch iteration\noptimizer.step(), loss.backward(), optimizer.zero_grad() calls\nCalling of model.eval(), enabling/disabling grads during evaluation\nCheckpoint Saving and Loading\nTensorboard (see loggers options)"
  },
  {
    "objectID": "slides/07-implementation.html#references",
    "href": "slides/07-implementation.html#references",
    "title": "Introduction to PyTorch",
    "section": "References",
    "text": "References\n\nPyTorch: https://pytorch.org\nTensorBoard: https://www.tensorflow.org/tensorboard\nLightning: https://www.lightning.ai\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/04-mlp.html#midterm-proposal-presentation",
    "href": "slides/04-mlp.html#midterm-proposal-presentation",
    "title": "Multilayer Perceptron",
    "section": "Midterm Proposal presentation",
    "text": "Midterm Proposal presentation\n\nA group of 3-4 students.\nLet me know your group members next Tuesday.\nThe presentation will be on 10/22.\n\nProblem statement\nData description\n1-2 references\n\nEach group has 15 minutes to present.\nHomework 1 is due on 10/15."
  },
  {
    "objectID": "slides/04-mlp.html#recap-of-the-last-lecture",
    "href": "slides/04-mlp.html#recap-of-the-last-lecture",
    "title": "Multilayer Perceptron",
    "section": "Recap of the Last Lecture",
    "text": "Recap of the Last Lecture\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\nA general procedure for supervised learning:\n\nGather a dataset \\(\\mathcal{D} = \\{(\\boldsymbol{x}_1, y_1), \\ldots, (\\boldsymbol{x}_n, y_n)\\}\\).\nChoose an appropriate loss function \\(L\\) for your dataset and problem.\nChoose a hypothesis class \\(\\mathcal{H}\\).\n\nNot too simple (underfitting), nor too complicated (overfitting).\n\nAdd a regularization term to the loss function to better control the complexity of the model.\nFind the model that minimizes the (regularized) empirical risk function.\nUse cross-validation to select the hyperparameters (if any).\nEstimate the generalization error of the final model using the test dataset."
  },
  {
    "objectID": "slides/04-mlp.html#nonlinear-hypothesis-class",
    "href": "slides/04-mlp.html#nonlinear-hypothesis-class",
    "title": "Multilayer Perceptron",
    "section": "Nonlinear Hypothesis Class",
    "text": "Nonlinear Hypothesis Class\n\nA simple and useful hypothesis class is the linear hypothesis class, for example:\n\nLinear regression: \\(\\mathcal{H} = \\{h(\\boldsymbol{x}) = \\boldsymbol{x}^T\\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}\\)\nLogistic regression: \\(\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\} = \\R^p\\)\n\nTo extend linear models to represent nonlinear functions of \\(\\boldsymbol{x}\\), we can apply the linear model not to \\(\\boldsymbol{x}\\) itself but to a transformed input \\(\\phi(\\boldsymbol{x})\\), where \\(\\phi\\) is a nonlinear transformation.\nThat is, \\[\n\\mathcal{H} = \\{ h(\\boldsymbol{x}) = \\phi(\\boldsymbol{x})^T \\boldsymbol{\\beta}, \\phi: \\R^p \\to \\R^d, \\boldsymbol{\\beta}\\in \\R^d\\}.\n\\]\nThe function \\(\\phi\\) is usually called a feature map and usually we choose \\(d \\gg p\\)."
  },
  {
    "objectID": "slides/04-mlp.html#choice-of-feature-map",
    "href": "slides/04-mlp.html#choice-of-feature-map",
    "title": "Multilayer Perceptron",
    "section": "Choice of Feature Map",
    "text": "Choice of Feature Map\nThere are three common ways to choose the feature map \\(\\phi\\):\n\nKernel method\n\nInstead of explicitly specifying \\(\\phi\\), we can define a symmetric function \\(k(\\boldsymbol{x}, \\boldsymbol{x}')\\) called a kernel, which corresponds to the dot product of some feature map \\(k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\phi(\\boldsymbol{x})^T \\phi(\\boldsymbol{x}^{\\prime})\\).\n\nManual engineering (hand-crafted features)\n\nChoose a feature map \\(\\phi\\) manually, e.g., \\(\\phi(x) = [x, x^2, \\exp(x), \\sin(x)]\\).\nA good feature map requires human effort for each separate task, with practitioners specializing in different domains.\n\nDeep Learning\n\nParametrize the feature map \\(\\phi\\) with a deep neural network, \\(\\phi(\\boldsymbol{x}) = \\phi(\\boldsymbol{x}; \\boldsymbol{\\theta})\\).\nThe parameter \\(\\boldsymbol{\\theta}\\) is learned from the data, i.e., we learn the feature map from the data."
  },
  {
    "objectID": "slides/04-mlp.html#outline",
    "href": "slides/04-mlp.html#outline",
    "title": "Multilayer Perceptron",
    "section": "Outline",
    "text": "Outline\n\nKernel Methods\n\nKernel Ridge Regression\n\nMultilayer Perceptron (MLP)\n\nBasic Structure\nActivation Function\n\nTraining an MLP\n\nGradient-based Learning\nBack-propagation\nComputational Graph\n\nExample"
  },
  {
    "objectID": "slides/04-mlp.html#regression-with-a-feature-map",
    "href": "slides/04-mlp.html#regression-with-a-feature-map",
    "title": "Multilayer Perceptron",
    "section": "Regression with a Feature Map",
    "text": "Regression with a Feature Map\n\nConsider the regression model with a given feature map \\(\\phi: \\R^p \\to \\R^d\\), i.e., \\[\ny = f(\\boldsymbol{x}) = \\phi(\\boldsymbol{x})^T \\boldsymbol{\\beta}.\n\\]\nGiven a training dataset \\(\\{(\\boldsymbol{x}_i, y_i)\\}_{i=1}^n\\), let \\(\\boldsymbol{z_i} = \\phi(\\boldsymbol{x}_i)\\) and \\(\\boldsymbol{Z} = [\\boldsymbol{z}_1, \\boldsymbol{z}_2, \\ldots, \\boldsymbol{z}_n]^T \\in \\R^{n \\times d}\\).\nThen the model parameter \\(\\boldsymbol{\\beta}\\) can be estimated by minimizing the regularized empirical risk function: \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}} & = \\argmin_{\\boldsymbol{\\beta} \\in \\R^d} \\|\\boldsymbol{y} - \\boldsymbol{Z}\\boldsymbol{\\beta}\\|^2 + \\alpha \\|\\boldsymbol{\\beta}\\|^2_2 = (\\boldsymbol{Z}^T \\boldsymbol{Z} + \\alpha I_d)^{-1} \\boldsymbol{Z}^T \\boldsymbol{y}\\\\\n& \\stackrel{\\textcolor{red}{\\text{(*)}}}{=} \\boldsymbol{Z}^T (\\boldsymbol{Z} \\boldsymbol{Z}^T + \\alpha I_n)^{-1} \\boldsymbol{y}.\n\\end{align*}\\]\nThe prediction for a new input \\(\\boldsymbol{x}^{\\star}\\) is \\[\n\\widehat{y^{\\star}} = \\phi(\\boldsymbol{x}^{\\star})^T \\hat{\\boldsymbol{\\beta}} = \\phi(\\boldsymbol{x}^{\\star})^T\\boldsymbol{Z}^T (\\boldsymbol{Z} \\boldsymbol{Z}^T + \\alpha I_n)^{-1} \\boldsymbol{y}.\n\\]\n\n\n\nSee appendix for the proof of equality (*)."
  },
  {
    "objectID": "slides/04-mlp.html#regression-with-a-feature-map-1",
    "href": "slides/04-mlp.html#regression-with-a-feature-map-1",
    "title": "Multilayer Perceptron",
    "section": "Regression with a Feature Map",
    "text": "Regression with a Feature Map\n\nNote that \\[\n\\boldsymbol{Z} \\boldsymbol{Z}^T = \\begin{bmatrix} \\boldsymbol{z}_1^T \\\\ \\boldsymbol{z}_2^T \\\\ \\vdots \\\\ \\boldsymbol{z}_n^T \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{z}_1 & \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_n \\end{bmatrix} = \\begin{bmatrix}\n\\boldsymbol{z}_1^T \\boldsymbol{z}_1 & \\boldsymbol{z}_1^T \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_1^T \\boldsymbol{z}_n \\\\\n\\boldsymbol{z}_2^T \\boldsymbol{z}_1 & \\boldsymbol{z}_2^T \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_2^T \\boldsymbol{z}_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\boldsymbol{z}_n^T \\boldsymbol{z}_1 & \\boldsymbol{z}_n^T \\boldsymbol{z}_2 & \\cdots & \\boldsymbol{z}_n^T \\boldsymbol{z}_n\n\\end{bmatrix} = \\left[\\phi(\\boldsymbol{x}_i)^T\\phi(\\boldsymbol{x}_j)\\right]_{i,j=1}^n\n\\] and \\[\n\\phi(\\boldsymbol{x}^{\\star})^T\\boldsymbol{Z}^T = \\begin{bmatrix} \\phi(\\boldsymbol{x}^{\\star})^T \\boldsymbol{z}_1 & \\phi(\\boldsymbol{x}^{\\star})^T \\boldsymbol{z}_2 & \\cdots & \\phi(\\boldsymbol{x}^{\\star})^T \\boldsymbol{z}_n \\end{bmatrix} = \\left[\\phi(\\boldsymbol{x}^{\\star})^T\\phi(\\boldsymbol{x}_i)\\right]_{i=1}^n.  \n\\]\nBIG NEWS: we don’t need to know the feature map \\(\\phi\\) explicitly, but only its inner product \\(\\phi(\\boldsymbol{x}_i)^T\\phi(\\boldsymbol{x}_j)\\).\nThat is, if we can find a function \\(k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime})\\) such that \\(k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\phi(\\boldsymbol{x})^T\\phi(\\boldsymbol{x}^{\\prime})\\) for some \\(\\phi\\), then we don’t need to know \\(\\phi\\) explicitly.\nThe function \\(k\\) is called a kernel function and the model is called a kernel ridge regression.\nMercer Condition: If the function \\(k(\\cdot, \\cdot)\\) is symmetric and positive definite, then there exist a \\(\\phi\\) such that \\(k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\phi(\\boldsymbol{x})^T\\phi(\\boldsymbol{x}^{\\prime})\\)."
  },
  {
    "objectID": "slides/04-mlp.html#kernel-method-vs-deep-learning",
    "href": "slides/04-mlp.html#kernel-method-vs-deep-learning",
    "title": "Multilayer Perceptron",
    "section": "Kernel Method vs Deep Learning",
    "text": "Kernel Method vs Deep Learning\n\nBoth kernel method and deep learning utilize nonlinear feature map to obtain a nonlinear hypothesis class.\nKernel method use a kernel function (the inner product of the feature map) to implicitly define the feature map. Some common kernel functions include:\n\nLinear kernel: \\(k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\boldsymbol{x}^T\\boldsymbol{x}^{\\prime} \\Rightarrow \\phi(\\boldsymbol{x}) = \\boldsymbol{x}\\).\nPolynomial kernel: \\(k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = (\\boldsymbol{x}^T\\boldsymbol{x}^{\\prime} + c)^d\\).\nRadial basis function (RBF) kernel: \\(k(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\exp\\left(-\\gamma\\|\\boldsymbol{x} - \\boldsymbol{x}^{\\prime}\\|^2\\right)\\).1\n\nDeep learning uses a deep neural network to parametrize the feature map.\n\nSee the appendix for the feature map of the RBF kernel."
  },
  {
    "objectID": "slides/04-mlp.html#example-kernel-regression",
    "href": "slides/04-mlp.html#example-kernel-regression",
    "title": "Multilayer Perceptron",
    "section": "Example: Kernel Regression",
    "text": "Example: Kernel Regression\nGenerate a dataset from the model \\(y = \\sin(2\\pi x) + x + \\epsilon\\), where \\(\\epsilon \\sim N(0, 0.5^2)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(100)\nn = 100\nX = rng.uniform(low=0, high=3, size=n)\ny = np.sin(2 * np.pi * X) + X + rng.normal(loc=0, scale=0.5, size=n)\nplt.scatter(X, y, c=\"black\", s=20)\nplt.show()"
  },
  {
    "objectID": "slides/04-mlp.html#multilayer-perceptron-mlp",
    "href": "slides/04-mlp.html#multilayer-perceptron-mlp",
    "title": "Multilayer Perceptron",
    "section": "Multilayer Perceptron (MLP)",
    "text": "Multilayer Perceptron (MLP)\n\nIt is also called a feedforward neural network because information flows through the function being evaluated from \\(\\boldsymbol{x}\\), through the intermediate computations used to define \\(f\\) , and finally to the output \\(y\\).\nWhen feedback connections are included, they are called recurrent neural networks."
  },
  {
    "objectID": "slides/04-mlp.html#multilayer-perceptron-mlp-1",
    "href": "slides/04-mlp.html#multilayer-perceptron-mlp-1",
    "title": "Multilayer Perceptron",
    "section": "Multilayer Perceptron (MLP)",
    "text": "Multilayer Perceptron (MLP)\n\n\n\n\n\n\n\n\n\nInput: \\(\\boldsymbol{x} = [x_1, x_2, \\ldots, x_p]^T \\in \\R^p\\).\nHidden Units: \\[\\begin{align*}\nh_i & = \\sigma\\left(\\sum_{j=1}^p w_{ij} x_j + b_i\\right), \\quad i = 1, \\ldots, m, \\\\\n    & = \\sigma\\left(\\boldsymbol{w}_i^T \\boldsymbol{x} + b_i\\right), \\quad i = 1, \\ldots, m, \\\\\n\\boldsymbol{w}_i & = [w_{i1}, w_{i2}, \\ldots, w_{ip}]^T \\in \\R^p, \\quad b_i \\in \\R.\n\\end{align*}\\]\nOutput: \\[\\begin{align*}\ny & = \\beta_0 + \\sum_{j=1}^m \\beta_{j} h_j = \\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{h},\\\\\n\\boldsymbol{h} & = [h_{1}, h_{2}, \\ldots, h_{m}]^T \\in \\R^m, \\\\\n\\boldsymbol{\\beta} & = [\\beta_{1}, \\beta_{2}, \\ldots, \\beta_{m}]^T \\in \\R^m, \\quad \\beta_0 \\in \\R.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-mlp.html#feature-map-in-mlp",
    "href": "slides/04-mlp.html#feature-map-in-mlp",
    "title": "Multilayer Perceptron",
    "section": "Feature Map in MLP",
    "text": "Feature Map in MLP\n\nThe relationship between the input \\(\\boldsymbol{x}\\) and the hidden unit \\(\\boldsymbol{h}\\) is \\[\n\\boldsymbol{h} = \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right),\n\\] where \\[\n\\boldsymbol{W} = \\begin{bmatrix} \\boldsymbol{w}_1^T \\\\ \\boldsymbol{w}_2^T \\\\ \\vdots \\\\ \\boldsymbol{w}_m^T \\end{bmatrix} \\in \\R^{m \\times p}, \\quad \\boldsymbol{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix} \\in \\R^m.\n\\]\nThe function \\(\\sigma: \\R \\to \\R\\) is called an activation function and is applied element-wise to the vector \\(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\).\nTha map \\(\\boldsymbol{x} \\mapsto \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right)\\) can be viewed as a feature map parametrized by \\(\\boldsymbol{W}\\) and \\(\\boldsymbol{b}\\).\nThat is, we replace the linear predictor \\(\\beta_0 + \\boldsymbol{x}^T \\boldsymbol{\\beta}\\) with \\(\\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{h} = \\beta_0 + \\boldsymbol{\\beta}^T \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right)\\).\nFinally, we can link the predictor \\(\\beta_0 + \\boldsymbol{\\beta}^T \\sigma\\left(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}\\right)\\) to the output \\(y\\) (or more specifically \\(\\E(Y \\mid \\boldsymbol{x})\\)) using a link function, e.g., identity, logit, or softmax."
  },
  {
    "objectID": "slides/04-mlp.html#activation-function",
    "href": "slides/04-mlp.html#activation-function",
    "title": "Multilayer Perceptron",
    "section": "Activation function",
    "text": "Activation function\n\nThe main role of the activation function is to introduce nonlinearity into the model.\nIf \\(\\sigma(x) = x\\), then the MLP is equivalent to a linear model since \\[\\begin{align*}\n\\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{h} & = \\beta_0 + \\boldsymbol{\\beta}^T \\sigma(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b}) = \\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{\\beta}^T \\boldsymbol{b}\\\\\n& = \\left(\\beta_0 + \\boldsymbol{\\beta}^T \\boldsymbol{b}\\right) + \\left(\\boldsymbol{\\beta}^T \\boldsymbol{W}\\right) \\boldsymbol{x} = \\tilde{\\beta}_0 + \\tilde{\\boldsymbol{\\beta}}^T \\boldsymbol{x}.\n\\end{align*}\\]\nFrom this perspective, \\(\\sigma\\) can be any nonlinear function.\nHowever, the choice of activation function has a crucial impact on training, especially when the neural network is deep, i.e., it has many hidden layers.\nCommon choices of activation functions include:\n\nRectified Linear Unit (ReLU): \\(\\sigma(x) = \\max(0, x)\\)\nLogistic: \\(\\sigma(x) = \\frac{1}{1 + \\exp(-x)}\\)\nHyperbolic Tangent (tanh): \\(\\sigma(x) = \\tanh(x)\\)"
  },
  {
    "objectID": "slides/04-mlp.html#activation-functions",
    "href": "slides/04-mlp.html#activation-functions",
    "title": "Multilayer Perceptron",
    "section": "Activation Functions",
    "text": "Activation Functions"
  },
  {
    "objectID": "slides/04-mlp.html#more-hidden-layers",
    "href": "slides/04-mlp.html#more-hidden-layers",
    "title": "Multilayer Perceptron",
    "section": "More Hidden Layers",
    "text": "More Hidden Layers\n\nIncreasing the number of hidden layers is straightforward and it allows the model to learn more complex functions.\nThe MLP is also called a fully connected neural network (FCNN) because we have connections between all the units in adjacent layers."
  },
  {
    "objectID": "slides/04-mlp.html#the-general-form-of-an-fcnn",
    "href": "slides/04-mlp.html#the-general-form-of-an-fcnn",
    "title": "Multilayer Perceptron",
    "section": "The general form of an FCNN",
    "text": "The general form of an FCNN\n\nAn \\(L\\)-layer FCNN (the \\(L\\)th layer is the output layer) can be written recursively as \\[\nf^{(L)}(\\boldsymbol{x}) = \\boldsymbol{W}^{(L)}\\boldsymbol{h}^{(L-1)} +  \\boldsymbol{b}^{(L)} \\in \\R^k,\n\\] where \\[\n\\boldsymbol{h}^{(l)} = \\sigma^{(l)}\\left(\\boldsymbol{W}^{(l)} \\boldsymbol{h}^{(l-1)} + \\boldsymbol{b}^{(l)}\\right), \\quad l = 1, \\ldots, L-1,\n\\] and \\(\\boldsymbol{h}^{(0)} = \\boldsymbol{x} \\in \\R^p\\).\nThe function \\(\\sigma^{(l)}\\) is the activation function for the \\(l\\)-th layer. Typically, we use the same activation function for all layers.\nThe parameter of the model is \\(\\theta = \\{\\boldsymbol{W}^{(1)}, \\ldots, \\boldsymbol{W}^{(L)}, \\boldsymbol{b}^{(1)}, \\ldots, \\boldsymbol{b}^{(L)}\\}\\).\nDenote the number of nodes in the \\(l\\)th layer by \\(m_l\\) (\\(m_0 = p\\) and \\(m_{L} = k\\)), i.e., \\(\\boldsymbol{h}^{(l)} \\in \\R^{m_l}\\).\nThen \\(\\boldsymbol{W}^{(l)} \\in \\R^{m_l \\times m_{l-1}}\\) and \\(\\boldsymbol{b}^{(l)} \\in \\R^{m_l}\\) and the total number of parameters is \\[\n    \\sum_{i=1}^{L} m_l\\cdot m_{l-1} + m_l = \\sum_{i=1}^{L} m_l(m_{l-1} + 1).\n\\]"
  },
  {
    "objectID": "slides/04-mlp.html#training-a-neural-network",
    "href": "slides/04-mlp.html#training-a-neural-network",
    "title": "Multilayer Perceptron",
    "section": "Training a Neural Network",
    "text": "Training a Neural Network\n\nSuppose now we have a dataset \\(\\mathcal{D} = \\{(\\boldsymbol{x}_1, y_1), \\ldots, (\\boldsymbol{x}_n, y_n)\\}\\) and we have chosen a hypothesis class \\(\\mathcal{H}^{(L)}\\) consisting of \\(L\\)-layer FCNNs.\nBased on the dataset, we can also specify an appropriate loss function \\(\\ell\\).\nFollowing the ERM principle, we want to find the model \\(\\hat{f} \\in \\mathcal{H}^{(L)}\\) that minimizes the empirical risk function \\[\n\\hat{f} = \\argmin_{f \\in \\mathcal{H}^{(L)}} \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f(\\boldsymbol{x}_i)).\n\\]\nSince \\(\\mathcal{H}^{(L)}\\) is parametrized by the weights and biases, we need to find the optimal weights and biases that minimize the empirical risk function.\nLet \\(J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f_{\\theta}(\\boldsymbol{x}_i))\\). The goal is to find \\(\\theta\\) that minimizes \\(J(\\theta)\\)."
  },
  {
    "objectID": "slides/04-mlp.html#gradient-descent",
    "href": "slides/04-mlp.html#gradient-descent",
    "title": "Multilayer Perceptron",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nThe gardient descent algorithm is a simple and widely used optimization algorithm for finding the minimum of a function.\nLet \\(g: \\R^d \\to \\R\\) be a differentiable function. The gradient of \\(g\\) at \\(\\boldsymbol{x} \\in \\R^d\\) is the vector of partial derivatives of \\(g\\) at \\(\\boldsymbol{x}\\): \\[\n\\nabla g(\\boldsymbol{x}) = \\left[\\frac{\\partial}{\\partial x_1} g(\\boldsymbol{x}), \\ldots, \\frac{\\partial}{\\partial x_d} g(\\boldsymbol{x})\\right]^T.\n\\]\nConsider the first-order Taylor expansion of \\(g\\) at \\(\\boldsymbol{x}\\): \\[\ng(\\boldsymbol{x} + \\boldsymbol{\\epsilon}) = g(\\boldsymbol{x}) + \\boldsymbol{\\epsilon} ^T\\nabla g(\\boldsymbol{x})  + O(\\|\\boldsymbol{\\epsilon}\\|^2).\n\\]\nWhen \\(\\|\\boldsymbol{\\epsilon}\\|\\) is small, we can approximate \\(g(\\boldsymbol{x} + \\boldsymbol{\\epsilon})\\) by \\(g(\\boldsymbol{x}) + \\boldsymbol{\\epsilon} ^T\\nabla g(\\boldsymbol{x})\\).\nTaking \\(\\boldsymbol{\\epsilon} = -\\eta \\nabla g(\\boldsymbol{x})\\) for some small \\(\\eta &gt; 0\\), we have \\[\ng(\\boldsymbol{x} - \\eta \\nabla g(\\boldsymbol{x})) \\approx g(\\boldsymbol{x}) - \\eta \\|\\nabla g(\\boldsymbol{x})\\|^2 \\leq g(\\boldsymbol{x}).\n\\]\nThat is, if we move in the opposite direction of the gradient by an appropriate distance, we can decrease the value of \\(g\\). This is called the gradient descent algorithm."
  },
  {
    "objectID": "slides/04-mlp.html#example-gradient-descent",
    "href": "slides/04-mlp.html#example-gradient-descent",
    "title": "Multilayer Perceptron",
    "section": "Example: Gradient Descent",
    "text": "Example: Gradient Descent"
  },
  {
    "objectID": "slides/04-mlp.html#gradient-descent-1",
    "href": "slides/04-mlp.html#gradient-descent-1",
    "title": "Multilayer Perceptron",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nThe gradient descent/ascent algorithm is the most simple optimization algorithm for finding the minimum/maximum of a function.\nIt is a first-order optimization algorithm that uses only the first-order derivative information.\nThe extra parameter \\(\\eta\\) is called the learning rate, which controls the step size of the algorithm.\nLearning rate is a crucial hyperparameter in the gradient descent algorithm as it determines the convergence rate and the stability of the algorithm.\nIf the learning rate is too small, the algorithm may converge very slowly. If it is too large, the algorithm may diverge.\nThere are also second-order optimization algorithms that use the second-order derivative information (the Hessian matrix), e.g., Newton’s method, IRWLS.\nModern deep learning frameworks use variants of gradient descent algorithms, e.g., Adam, RMSprop, Adagrad, etc."
  },
  {
    "objectID": "slides/04-mlp.html#train-a-neural-network-using-gd",
    "href": "slides/04-mlp.html#train-a-neural-network-using-gd",
    "title": "Multilayer Perceptron",
    "section": "Train a Neural Network using GD",
    "text": "Train a Neural Network using GD\n\nRecall that the goal is to find the optimal weights and biases that minimize the empirical risk function \\(J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f_{\\theta}(\\boldsymbol{x}_i))\\).\nTo apply the gradient descent algorithm, we need to compute the gradient of \\(J(\\theta)\\) with respect to the weights and biases.\nFor simplicity, we assume that \\(L = 2\\) (one hidden layer, \\(m_0 = p\\), \\(m_1 = m\\), \\(m_2 = 1\\)), i.e., \\[\nf_{\\theta}(\\boldsymbol{x}) = \\boldsymbol{W}^{(2)}\\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right) + \\boldsymbol{b}^{(2)}.\n\\]\nThe parameters are \\(\\theta = \\{\\boldsymbol{W}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{b}^{(1)}, \\boldsymbol{b}^{(2)}\\}\\) where \\(\\boldsymbol{W}^{(1)} \\in \\R^{m \\times p}\\), \\(\\boldsymbol{W}^{(2)} \\in \\R^{1 \\times m}\\), \\(\\boldsymbol{b}^{(1)} \\in \\R^m\\), and \\(\\boldsymbol{b}^{(2)} \\in \\R\\)."
  },
  {
    "objectID": "slides/04-mlp.html#chain-rule",
    "href": "slides/04-mlp.html#chain-rule",
    "title": "Multilayer Perceptron",
    "section": "Chain Rule",
    "text": "Chain Rule\nWrite \\[\\begin{align*}\nf_{\\theta}(\\boldsymbol{x}) & = \\boldsymbol{W}^{(2)}\\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right) + \\boldsymbol{b}^{(2)} = \\boldsymbol{W}^{(2)}\\boldsymbol{h}^{(1)} + \\boldsymbol{b}^{(2)}\\\\\n\\boldsymbol{h}^{(1)} & = \\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right),\n\\end{align*}\\] and we have \\[\\begin{align*}\n\\nabla_{\\boldsymbol{b}^{(2)}} f_{\\theta}(\\boldsymbol{x}) & = 1\\\\\n\\nabla_{\\boldsymbol{W}^{(2)}} f_{\\theta}(\\boldsymbol{x}) & = \\boldsymbol{h}^{(1)} = \\sigma\\left(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right) \\in \\R^m\\\\\n\\nabla_{\\boldsymbol{b}^{(1)}} f_{\\theta}(\\boldsymbol{x}) & = \\left[\\nabla_{\\boldsymbol{b}^{(1)}} \\boldsymbol{h}^{(1)}\\right] \\left(\\boldsymbol{W}^{(2)}\\right)^T \\in \\R^m\\\\\n\\nabla_{\\boldsymbol{W}^{(1)}} f_{\\theta}(\\boldsymbol{x}) & = \\boldsymbol{W}^{(2)}\\nabla_{\\boldsymbol{W}^{(1)}} \\boldsymbol{h}^{(1)} = \\sum_{k=1}^m W_k^{(2)} \\nabla_{\\boldsymbol{W}^{(1)}} h_k^{(1)}\\in \\R^{m \\times p}\\\\\n\\nabla_{\\boldsymbol{b}^{(1)}} \\boldsymbol{h}^{(1)} & = \\text{diag}\\left(\\sigma^{\\prime}(\\boldsymbol{W}^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)})\\right) \\in \\R^{m \\times m}\\\\\n\\nabla_{\\boldsymbol{W}^{(1)}} h_k^{(1)} & \\in \\R^{m \\times p}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-mlp.html#computational-graph",
    "href": "slides/04-mlp.html#computational-graph",
    "title": "Multilayer Perceptron",
    "section": "Computational Graph",
    "text": "Computational Graph\nA graphical representation of computation:\n\nNodes indicate variables (scalar, vector, matrix, etc.)\nEdges indicate operations (addition, multiplication, function evaluation, etc.)"
  },
  {
    "objectID": "slides/04-mlp.html#computational-graph-for-a-neural-network",
    "href": "slides/04-mlp.html#computational-graph-for-a-neural-network",
    "title": "Multilayer Perceptron",
    "section": "Computational Graph for a Neural Network",
    "text": "Computational Graph for a Neural Network\n\nThe computational graph for a neural network is the following\n\n\n\n\n\n\n\nThe square nodes represent unknown parameters.\nThe process of transforming the input \\(\\boldsymbol{x}\\) to the output \\(y\\) following the arrows is called forward propagation."
  },
  {
    "objectID": "slides/04-mlp.html#back-propagation-1",
    "href": "slides/04-mlp.html#back-propagation-1",
    "title": "Multilayer Perceptron",
    "section": "Back-propagation",
    "text": "Back-propagation\n\nThe process of computing the gradient of the loss function with respect to the parameters is called back-propagation.\nThe back-propogation algorithm is simply an application of the chain rule to compute the gradient of the loss function with respect to the parameters.\nTo each edge in the computational graph, we associate a gradient that represents the derivative of the starting node with respect to the variable at the end node."
  },
  {
    "objectID": "slides/04-mlp.html#example",
    "href": "slides/04-mlp.html#example",
    "title": "Multilayer Perceptron",
    "section": "Example",
    "text": "Example\nConsider the same dataset as the previous example. We want to build a regression model using neural networks."
  },
  {
    "objectID": "slides/04-mlp.html#some-questions",
    "href": "slides/04-mlp.html#some-questions",
    "title": "Multilayer Perceptron",
    "section": "Some questions",
    "text": "Some questions\n\nReLU v.s. Sigmoid? Which is better?\nHow to choose the number of hidden units? The more the better?\nHow to choose the number of hidden layers? The deeper the better?\nIt seems that kernel regresion gives a slightly better result than MLP does.\n\nIn the previous example, kernel regression takes ~0.01 second to train, while MLP takes ~9.5 seconds."
  },
  {
    "objectID": "slides/04-mlp.html#proof-of-in-p.6",
    "href": "slides/04-mlp.html#proof-of-in-p.6",
    "title": "Multilayer Perceptron",
    "section": "Proof of (*) in P.6",
    "text": "Proof of (*) in P.6\nShow that for any \\(\\boldsymbol{Z} \\in \\R^{n \\times d}\\), \\(\\boldsymbol{y} \\in \\R^n\\), and \\(\\alpha &gt; 0\\), we have \\[\n(\\boldsymbol{Z}^T \\boldsymbol{Z} + \\alpha I_d)^{-1} \\boldsymbol{Z}^T \\boldsymbol{y} = \\boldsymbol{Z}^T (\\boldsymbol{Z} \\boldsymbol{Z}^T + \\alpha I_n)^{-1} \\boldsymbol{y}.\n\\]\nProof:\n\nThe left hand side is the solution to the equation \\(\\boldsymbol{Z}^T \\boldsymbol{Z} \\boldsymbol{\\beta}+\\alpha \\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{y}\\).\nRearranging the terms gives \\(\\boldsymbol{\\beta}=\\boldsymbol{Z}^T\\left[\\frac{1}{\\alpha}(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{\\beta})\\right]\\).\nDefine \\(\\boldsymbol{b}=\\frac{1}{\\alpha}(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{\\beta})\\), and then \\(\\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{b}\\).\nSubstituting \\(\\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{b}\\) into \\(\\boldsymbol{b}=\\frac{1}{\\alpha}(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{\\beta})\\), we have \\[\n\\boldsymbol{b}=\\frac{1}{\\alpha}\\left(\\boldsymbol{y}-\\boldsymbol{Z} \\boldsymbol{Z}^T \\boldsymbol{b}\\right).\n\\]\nRearranging the terms gives \\(\\left(\\boldsymbol{Z}\\boldsymbol{Z}^{T}+\\alpha I_n\\right) \\boldsymbol{b}=\\boldsymbol{y}\\), which yields \\(\\boldsymbol{b}=\\left(\\boldsymbol{Z} \\boldsymbol{Z}^T+\\alpha I_n\\right)^{-1} \\boldsymbol{y}\\)\nSubstituting into \\(\\boldsymbol{\\beta}=\\boldsymbol{Z}^T \\boldsymbol{b}\\) gives \\(\\boldsymbol{\\beta}=\\boldsymbol{Z}^T\\left(\\boldsymbol{Z} \\boldsymbol{Z}^T+\\alpha I_n\\right)^{-1} \\boldsymbol{y}\\)."
  },
  {
    "objectID": "slides/04-mlp.html#feature-map-of-rbf-kernel",
    "href": "slides/04-mlp.html#feature-map-of-rbf-kernel",
    "title": "Multilayer Perceptron",
    "section": "Feature map of RBF kernel",
    "text": "Feature map of RBF kernel\nConsider the univariate RBF kernel \\(k(x, y) = \\exp\\left(-(x - y)^2\\right)\\). We have \\[\\begin{align*}\nk(x, y) & = \\exp\\left(-(x - y)^2\\right) = \\exp\\left(-x^2 + 2xy - y^2\\right) = \\exp(-x^2)\\exp(2xy)\\exp(-y^2)\\\\\n& = \\exp(-x^2)\\left(\\sum_{m=0}^{\\infty} \\frac{2^mx^my^m}{m!}\\right)\\exp(-y^2)\\\\\n& = \\exp(-x^2) \\left(1, \\sqrt{\\frac{2^1}{1!}}x, \\sqrt{\\frac{2^2}{2!}}x^2, \\sqrt{\\frac{2^3}{3!}}x^3, \\ldots \\right)^T\\\\\n& \\qquad \\left(1, \\sqrt{\\frac{2^1}{1!}}y, \\sqrt{\\frac{2^2}{2!}}y^2, \\sqrt{\\frac{2^3}{3!}}y^3, \\ldots \\right)\\exp(-y^2)\n\\end{align*}\\]\nHence the feature map corresponding to the RBF kernel is \\[\n\\phi(x) = \\exp(-x^2)\\left(1, \\sqrt{\\frac{2^1}{1!}}x, \\sqrt{\\frac{2^2}{2!}}x^2, \\sqrt{\\frac{2^3}{3!}}x^3, \\ldots \\right)^T.\n\\]\n\n\n\nHome"
  },
  {
    "objectID": "slides/02-lm.html#outline",
    "href": "slides/02-lm.html#outline",
    "title": "Generalized Linear Models",
    "section": "Outline",
    "text": "Outline\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nClassical Linear Model\n\nOrdinary Least Squares (OLS) Estimation\nMaximum Likelihood (ML) Estimation\nPenalty and Regularization\n\nGeneralized Linear Models\n\nLogistic Regression\nMultinomial Regression\n\nNon-linear Models\n\nGeneralized Additive Models (GAM)\nProjection Pursuit Regression (PPR)"
  },
  {
    "objectID": "slides/02-lm.html#classical-linear-model",
    "href": "slides/02-lm.html#classical-linear-model",
    "title": "Generalized Linear Models",
    "section": "Classical Linear Model",
    "text": "Classical Linear Model\n\nGiven \\(p\\) covariates \\(x_1, \\ldots, x_p\\) and a response variable \\(y\\), the classical linear model assumes that the relationship between the \\(x_i\\)’s and \\(y\\) is linear: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p.\n\\]\nDenote \\(\\boldsymbol{x} = (1, x_{1}, \\ldots, x_{p})^T\\) and \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^T\\), the model can be written as \\[\ny = \\boldsymbol{x}^T\\boldsymbol{\\beta}.\n\\]\nSuppose now we have \\(n\\) samples \\((\\boldsymbol{x}_1, y_1), \\ldots (\\boldsymbol{x}_n, y_n)\\) and we believe that the linear model above is a reasonable approximation of the relationship between the \\(\\boldsymbol{x}_i\\)’s and \\(y_i\\).\nThe goal is to estimate the model parameter \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/02-lm.html#ordinary-least-squares-ols-estimation",
    "href": "slides/02-lm.html#ordinary-least-squares-ols-estimation",
    "title": "Generalized Linear Models",
    "section": "Ordinary Least Squares (OLS) Estimation",
    "text": "Ordinary Least Squares (OLS) Estimation\n\nThe most common method to estimate \\(\\boldsymbol{\\beta}\\) is the ordinary least squares (OLS) estimation, that is, we find \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals: \\[\n\\hat{\\boldsymbol{\\beta}} = \\argmin_{\\boldsymbol{\\beta}} \\sum_{i=1}^n (y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2.\n\\]\nDenoting \\[\n\\boldsymbol{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad\n\\boldsymbol{X} = \\begin{bmatrix} \\boldsymbol{x}_1^T \\\\ \\vdots \\\\ \\boldsymbol{x}_n^T \\end{bmatrix},\n\\] the minimization problem can be written as \\[\n\\hat{\\boldsymbol{\\beta}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#ordinary-least-squares-ols-estimation-1",
    "href": "slides/02-lm.html#ordinary-least-squares-ols-estimation-1",
    "title": "Generalized Linear Models",
    "section": "Ordinary Least Squares (OLS) Estimation",
    "text": "Ordinary Least Squares (OLS) Estimation\n\nLet \\(L(\\boldsymbol{\\beta}) = \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2\\). This is often called the loss function.\nTaking the gradient of \\(L(\\boldsymbol{\\beta})\\) with respect to \\(\\boldsymbol{\\beta}\\) and setting it to zero, we have \\[\n\\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = -2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) = 0.\n\\]\nThe OLS estimation has a closed-form solution: \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n\\]\nTo ensure the existence of the inverse, we need to assume that \\(\\boldsymbol{X}^T\\boldsymbol{X}\\) is invertible, that is, the columns of \\(\\boldsymbol{X}\\) are linearly independent.\nTo verify that \\(\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}}\\) is indeed the minimizer, we need to show that the Hessian of \\(L(\\boldsymbol{\\beta})\\) is positive definite: \\[\n\\nabla^2_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = 2\\boldsymbol{X}^T\\boldsymbol{X} \\succ 0.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#maximum-likelihood-ml-estimation",
    "href": "slides/02-lm.html#maximum-likelihood-ml-estimation",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood (ML) Estimation",
    "text": "Maximum Likelihood (ML) Estimation\n\nAnother way to estimate \\(\\boldsymbol{\\beta}\\) is the maximum likelihood (ML) estimation.\nSuppose that the response variable \\(y\\) is normally distributed with mean \\(\\mu(\\boldsymbol{x}) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\\) and variance \\(\\sigma^2\\): \\[\ny \\mid \\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{x}^T\\boldsymbol{\\beta}, \\sigma^2).\n\\]\nAssuming the samples are i.i.d, the likelihood function is \\[\nL(\\boldsymbol{\\beta}, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right).\n\\]\nThe log-likelihood function is \\[\n\\ell(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2 = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2.\n\\]\nThe ML estimation is \\(\\hat{\\boldsymbol{\\beta}}, \\hat{\\sigma}^2 = \\argmax_{\\boldsymbol{\\beta}, \\sigma^2} \\ell(\\boldsymbol{\\beta}, \\sigma^2)\\)."
  },
  {
    "objectID": "slides/02-lm.html#maximum-likelihood-ml-estimation-1",
    "href": "slides/02-lm.html#maximum-likelihood-ml-estimation-1",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood (ML) Estimation",
    "text": "Maximum Likelihood (ML) Estimation\n\nTaking the gradient of \\(\\ell(\\boldsymbol{\\beta}, \\sigma^2)\\) with respect to \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) and setting them to zero, we have \\[\n\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sigma^2} \\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) = 0,\n\\] and \\[\n\\frac{\\partial}{\\partial \\sigma^2} \\ell(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 = 0.\n\\]\nThe ML estimation has a closed-form solution: \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}, \\quad\n\\hat{\\sigma}^2 = \\frac{1}{n}\\|\\boldsymbol{y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}\\|_2^2.\n\\]\nThe MLE of \\(\\boldsymbol{\\beta}\\) is the same as the OLS estimation."
  },
  {
    "objectID": "slides/02-lm.html#ols-v.s.-ml-estimation",
    "href": "slides/02-lm.html#ols-v.s.-ml-estimation",
    "title": "Generalized Linear Models",
    "section": "OLS v.s. ML Estimation",
    "text": "OLS v.s. ML Estimation\n\nCompared to the OLS estimation, the ML estimation requires an additional assumption on the distribution of \\(y\\).\nIn ths case of linear regression, the normality assumption is the most common one.\nAn equivalent way to express the linear regression under the normality assumption is \\[\ny  = \\boldsymbol{x}^T\\boldsymbol{\\beta} + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2).\n\\]\nOne of the advantages of the ML estimation is that it provides a way to estimate the variance of the estimated parameter \\(\\hat{\\boldsymbol{\\beta}}\\): \\[\\begin{align*}\n\\var(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}) & = \\var((\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}) \\\\\n& = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\var(\\boldsymbol{y})\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\\\\n& = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T(\\sigma^2 I)\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\\\\n& = \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-lm.html#useful-properties-of-mle",
    "href": "slides/02-lm.html#useful-properties-of-mle",
    "title": "Generalized Linear Models",
    "section": "Useful Properties of MLE",
    "text": "Useful Properties of MLE\n\nUnder the normality assumption, the MLE has the following properties:\n\nUnbiasedness: \\(\\E(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}) = \\boldsymbol{\\beta}\\).\nNormality: \\(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1})\\).\nPrediction Intervals: for a given \\(\\boldsymbol{x}^{\\star}\\), the predicted value is \\(y^{\\star} = \\boldsymbol{x}^{\\star T}\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}\\) and the prediction interval is \\[\ny^{\\star} \\pm t_{n-p-1, 1-\\alpha/2} \\hat{\\sigma} \\sqrt{1 + \\boldsymbol{x}^{\\star T}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{x}^{\\star}}.\n\\]\nWe can also derive the confidence intervals and hypothesis tests for \\(c^T\\boldsymbol{\\beta}\\) for any \\(c\\)."
  },
  {
    "objectID": "slides/02-lm.html#what-if-the-samples-are-not-i.i.d",
    "href": "slides/02-lm.html#what-if-the-samples-are-not-i.i.d",
    "title": "Generalized Linear Models",
    "section": "What if the samples are not i.i.d?",
    "text": "What if the samples are not i.i.d?\n\nIf the samples are not i.i.d, we can model the joint distribution of the samples: \\[\n\\boldsymbol{y} \\mid \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{X}\\boldsymbol{\\beta}, \\sigma^2 W^{-1})\n\\] where \\(W\\) is an \\(n\\times n\\) covariance matrix describing the dependence between the samples.\nConsider the transformation \\(\\widetilde{\\boldsymbol{y}} = W^{1/2}\\boldsymbol{y}\\) and \\(\\widetilde{\\boldsymbol{X}} = W^{1/2}\\boldsymbol{X}\\). The model becomes \\[\n\\widetilde{\\boldsymbol{y}} \\mid \\widetilde{\\boldsymbol{X}} \\sim \\mathcal{N}(\\widetilde{\\boldsymbol{X}}\\boldsymbol{\\beta}, \\sigma^2 I).\n\\]\nTherefore the MLE for \\(\\boldsymbol{\\beta}\\) is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = (\\widetilde{\\boldsymbol{X}}^T\\widetilde{\\boldsymbol{X}})^{-1}\\widetilde{\\boldsymbol{X}}^T\\widetilde{\\boldsymbol{y}} = (\\boldsymbol{X}^TW\\boldsymbol{X})^{-1}\\boldsymbol{X}^TW\\boldsymbol{y},\n\\] which is called the weighted least squares estimation."
  },
  {
    "objectID": "slides/02-lm.html#penalized-likelihood-estimation",
    "href": "slides/02-lm.html#penalized-likelihood-estimation",
    "title": "Generalized Linear Models",
    "section": "Penalized Likelihood Estimation",
    "text": "Penalized Likelihood Estimation\n\nHowever, in practice, the MLE might not be the best choice.\nFor example, when \\(X\\) contains columns that are close to collinear or if the number of covariates \\(p\\) is large, computing \\((X^TX)^{-1}\\) will become numerically unstable.\nOne of the most common ways to address this issue is to add penalization or regularization.\nThe idea is to add a penalty term to the negative log-likelihood function, i.e., \\[\n-\\ell(\\boldsymbol{\\beta}, \\sigma^2) + \\lambda \\cdot \\text{pen}(\\boldsymbol{\\beta}).\n\\]\nThat is, we are looking for the \\(\\boldsymbol{\\beta}\\) that minimizes the negaive log-likelihood and the penalty.\nThe extra term \\(\\lambda\\) is a hyperparameter that controls the trade-off between the likelihood and the penalty."
  },
  {
    "objectID": "slides/02-lm.html#ridge-regression",
    "href": "slides/02-lm.html#ridge-regression",
    "title": "Generalized Linear Models",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nOne of the most common penalization methods is the Ridge regression.\nThe penalty term is the \\(L_2\\) norm of \\(\\boldsymbol{\\beta}\\): \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\|\\boldsymbol{\\beta}\\|_2^2 = \\sum_{j=0}^p \\beta_j^2.\n\\]\nThe Ridge estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2.\n\\]\nLet \\(L_{\\lambda}(\\boldsymbol{\\beta}) = \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2\\) and set the gradient to zero \\[\n\\nabla_{\\boldsymbol{\\beta}} L_{\\lambda}(\\boldsymbol{\\beta}) = -2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + 2\\lambda \\boldsymbol{\\beta} = 0.\n\\]\nHence the Ridge estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\lambda I)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#ridge-regression-1",
    "href": "slides/02-lm.html#ridge-regression-1",
    "title": "Generalized Linear Models",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nTypically, penalization of the intercept is not desired in Ridge regression so that \\(\\beta_0\\) should be excluded from the penalty term.\nA simple way to achieve this is to center all covariates and the responses so that \\(\\bar{y} = 0\\) and \\(\\bar{\\boldsymbol{x}} = 0\\) which automatically results in \\(\\hat{\\beta}_0 = 0\\).\nA second approach is to use the following penalty term: \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\sum_{j=1}^p \\beta_j^2 = \\boldsymbol{\\beta}^TK\\boldsymbol{\\beta},\n\\] where \\(K = \\text{diag}(0, 1, \\ldots, 1)\\).\nThe Ridge estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\lambda K)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#ridge-v.s.-ols",
    "href": "slides/02-lm.html#ridge-v.s.-ols",
    "title": "Generalized Linear Models",
    "section": "Ridge v.s. OLS",
    "text": "Ridge v.s. OLS\n\nThe Ridge estimator is biased and the OLS is unbiased.\nHowever, one can show that the Ridge estimator has a smaller variance than the OLS estimator.\nWhen choosing an appropriate hyperparameter \\(\\lambda\\), the Ridge estimator can have a smaller mean squared error (MSE) than the OLS estimator.\nThe Ridge estimator is shrinkage estimator that shrinks the coefficients towards zero (large value of \\(\\lambda\\) yields a stronger shrinkage).\nThe Ridge estimator is particularly useful when the covariates are collinear or when the number of covariates is large.\nThe choice of \\(\\lambda\\) is often done using cross-validation."
  },
  {
    "objectID": "slides/02-lm.html#least-absolute-shrinkage-and-selection-operator-lasso",
    "href": "slides/02-lm.html#least-absolute-shrinkage-and-selection-operator-lasso",
    "title": "Generalized Linear Models",
    "section": "Least Absolute Shrinkage and Selection Operator (LASSO)",
    "text": "Least Absolute Shrinkage and Selection Operator (LASSO)\n\nAnother common penalization method is the least absolute shrinkage and selection operator (LASSO).\nThe penalty term is the \\(L_1\\) norm of \\(\\boldsymbol{\\beta}\\): \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\sum_{j=1}^p |\\beta_j|.\n\\]\nThe LASSO estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{LASSO}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#lasso-estimation",
    "href": "slides/02-lm.html#lasso-estimation",
    "title": "Generalized Linear Models",
    "section": "LASSO Estimation",
    "text": "LASSO Estimation\n\nNote that the objective function of the LASSO estimator is not differentiable, due to the absolute value term.\nNo closed-form solution for the LASSO estimator is available. However, it can be solved using the Least Angle Regression1 or the coordinate descent2 algorithm.\nOne of the key properties of the LASSO estimator is that it produces sparse solutions, i.e., some of the estimated coefficients are exactly zero.\nThis is particularly useful for variable selection, i.e., to identify the important covariates.\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. The Annals of Statistics, 32(2), pages 407–499.Friedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), pages 1–22."
  },
  {
    "objectID": "slides/02-lm.html#ridge-v.s.-lasso-l_2-penalty-v.s.-l_1-penalty",
    "href": "slides/02-lm.html#ridge-v.s.-lasso-l_2-penalty-v.s.-l_1-penalty",
    "title": "Generalized Linear Models",
    "section": "Ridge v.s. LASSO (\\(L_2\\) penalty v.s. \\(L_1\\) penalty)",
    "text": "Ridge v.s. LASSO (\\(L_2\\) penalty v.s. \\(L_1\\) penalty)\n\n\n\n\n\n\n\nFigure 3.11 of ESL"
  },
  {
    "objectID": "slides/02-lm.html#elastic-net",
    "href": "slides/02-lm.html#elastic-net",
    "title": "Generalized Linear Models",
    "section": "Elastic-Net",
    "text": "Elastic-Net\n\nThe Elastic-Net1 is a combination of the Ridge and the LASSO.\nThe penalty term is a combination of the \\(L_1\\) and \\(L_2\\) norm of \\(\\boldsymbol{\\beta}\\): \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2.\n\\]\nThe Elastic-Net estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{EN}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2.\n\\]\nThrough the choice of \\(\\lambda_1\\) and \\(\\lambda_2\\), the Elastic-Net can be used to achieve the benefits of both the ridge and the LASSO.\n\nZou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2), pages 301-320."
  },
  {
    "objectID": "slides/02-lm.html#example---sparse-features",
    "href": "slides/02-lm.html#example---sparse-features",
    "title": "Generalized Linear Models",
    "section": "Example - Sparse features",
    "text": "Example - Sparse features\n\nWe generate a synthetic dataset with 50 samples and 10 features.\nOnly 5 out of the 10 features are informative.\nWe fit the linear regression, Ridge, LASSO, and Elastic-Net models to the data.\n\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.datasets import make_regression\n\nX, y, true_coef = make_regression(n_samples = 50, n_features = 10, \n                                  n_informative = 5, noise = 5,\n                                  coef = True, random_state = 42)\n\nlm = LinearRegression().fit(X, y)\nridge = Ridge(alpha=1.0).fit(X, y)\nlasso = Lasso(alpha=1.0).fit(X, y)\nenet = ElasticNet(alpha=1.0, l1_ratio=0.5).fit(X, y)"
  },
  {
    "objectID": "slides/02-lm.html#example---sparse-features-1",
    "href": "slides/02-lm.html#example---sparse-features-1",
    "title": "Generalized Linear Models",
    "section": "Example - Sparse features",
    "text": "Example - Sparse features\n\n\n\n\n\nTrue Coef\nLinear\nRidge\nLASSO\nElastic-Net\n\n\n\n\n57.078\n56.306\n55.626\n55.459\n40.091\n\n\n0.000\n0.173\n0.471\n0.000\n1.685\n\n\n0.000\n-0.185\n0.073\n-0.000\n1.556\n\n\n35.610\n33.877\n33.447\n33.189\n25.617\n\n\n0.000\n0.702\n1.655\n0.000\n8.187\n\n\n60.577\n60.568\n59.158\n59.634\n38.185\n\n\n0.000\n1.586\n1.838\n0.650\n4.251\n\n\n64.592\n64.964\n63.242\n63.704\n37.886\n\n\n0.000\n-0.440\n-0.428\n-0.000\n-1.682\n\n\n98.652\n99.563\n96.871\n98.682\n61.042"
  },
  {
    "objectID": "slides/02-lm.html#beyond-normality",
    "href": "slides/02-lm.html#beyond-normality",
    "title": "Generalized Linear Models",
    "section": "Beyond Normality",
    "text": "Beyond Normality\n\nWhen the response variable is not real-valued, the classical linear model is not appropriate.\nFor example:\n\nBinary responses: \\(y \\in \\{0, 1\\}\\).\nCount data: \\(y \\in \\{0, 1, 2, \\ldots\\}\\).\nMultinomial responses: \\(y \\in \\{1, 2, \\ldots, K\\}\\).\n\nIn these cases, neither the OLS estimation nor the normality assumption is appropriate.\nGeneralized Linear Model (GLM) is a generalization of the classical linear model that allows for non-normal responses.\nThe key is to find a reasonable distribution to model \\(y\\)."
  },
  {
    "objectID": "slides/02-lm.html#binary-responces-logistic-regression",
    "href": "slides/02-lm.html#binary-responces-logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Binary Responces: Logistic regression",
    "text": "Binary Responces: Logistic regression\n\nWhen \\(y\\) is binary, we can use the Bernoulli distribution \\[\nY \\mid \\boldsymbol{x} \\sim \\text{Ber}(p(\\boldsymbol{x})),\n\\]\nThat is, \\(\\P(Y = 1 \\mid \\boldsymbol{x}) = p(\\boldsymbol{x})\\) and \\(\\P(Y = 0 \\mid \\boldsymbol{x}) = 1 - p(\\boldsymbol{x})\\) and the expectation is \\(\\E(Y \\mid \\boldsymbol{x}) = p(\\boldsymbol{x})\\).\nThe logistic regression model assumes \\[\np(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T\\boldsymbol{\\beta})}.\n\\]\nEquivalently, we can write \\[\n\\log\\left(\\frac{p(\\boldsymbol{x})}{1 - p(\\boldsymbol{x})}\\right) = \\boldsymbol{x}^T\\boldsymbol{\\beta}.\n\\]\nThat is, the log-odds of the event \\(\\{Y = 1\\}\\) is linear in \\(\\boldsymbol{x}\\)."
  },
  {
    "objectID": "slides/02-lm.html#ml-estimation-for-logistic-regression",
    "href": "slides/02-lm.html#ml-estimation-for-logistic-regression",
    "title": "Generalized Linear Models",
    "section": "ML Estimation for Logistic Regression",
    "text": "ML Estimation for Logistic Regression\n\nGiven \\(n\\) samples \\((\\boldsymbol{x}_1, y_1), \\ldots (\\boldsymbol{x}_n, y_n)\\), the likelihood function is \\[\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n p(\\boldsymbol{x}_i)^{y_i} (1 - p(\\boldsymbol{x}_i))^{1 - y_i}.\n\\]\nThe log-likelihood function is \\[\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n y_i \\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)).\n\\]\nHence the MLE of \\(\\boldsymbol{\\beta}\\) is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = \\argmax_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}).\n\\]\nThe negative log-likelihood is also called the cross-entropy loss, i.e., maximizing the likelihood is equivalent to minimizing the cross-entropy loss."
  },
  {
    "objectID": "slides/02-lm.html#cross-entropy-loss",
    "href": "slides/02-lm.html#cross-entropy-loss",
    "title": "Generalized Linear Models",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\n\nIn Information Theory, the cross-entropy is defined as \\[\nH(p, q) = -\\sum_{x} p(x) \\log(q(x)) = -\\E_p[\\log(q(X))],\n\\] where \\(p(x)\\) and \\(q(x)\\) are two discrete probability distributions.\nLarge value of cross-entropy indicates that the two distributions are different.\nIn the case of logistic regression, we want to measure the discrepancy between the data \\([y_i, 1-y_i]\\) and the model \\([p(\\boldsymbol{x}_i), 1 - p(\\boldsymbol{x}_i)]\\).\nHence the cross-entropy is \\[\n-\\sum_{i=1}^n y_i \\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)).\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#exponential-family",
    "href": "slides/02-lm.html#exponential-family",
    "title": "Generalized Linear Models",
    "section": "Exponential family",
    "text": "Exponential family\n\nRecall that an exponential family is a family of distributions \\[\nf(x \\mid \\theta) = h(x)\\exp(\\theta^T T(x) - \\psi(\\theta))\n\\] where \\(\\theta \\in \\R^k\\) and \\(T(x) = [T_1(x), \\ldots, T_k(x)]^T\\).\nThe parameter \\(\\theta\\) is called the natural parameter or the canonical parameter and \\(T(x)\\) is the sufficient statistic.\nTwo useful properties (from Bartlett’s identities):\n\n\\(\\E(T(X)) = \\nabla_{\\theta}\\psi(\\theta)\\)\n\\(\\var(T(X)) = \\text{Hess}(\\psi(\\theta)) = \\nabla^2_{\\theta} \\psi(\\theta)\\).\n\nThat is, the relationship between the parameter \\(\\theta\\) and the expectation \\(\\E(T(X))\\) determined by \\(\\nabla \\psi\\)."
  },
  {
    "objectID": "slides/02-lm.html#examples",
    "href": "slides/02-lm.html#examples",
    "title": "Generalized Linear Models",
    "section": "Examples",
    "text": "Examples\n\nNormal disribution: \\[\nf(x \\mid \\mu, \\sigma^2) = \\exp\\left(-\\frac{1}{2\\sigma^2} x^2 + \\frac{\\mu}{\\sigma^2}x - \\frac{\\mu^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\\right), \\quad x \\in \\R\n\\]\n\n\\(\\theta = \\left(-\\frac{1}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}\\right)\\), \\(T(x) = (-x^2, x)\\), \\(\\psi(\\theta) = -\\frac{\\theta_2^2}{4\\theta_1} - \\frac{1}{2}\\log\\left(-\\frac{\\theta_1}{\\pi}\\right)= \\frac{\\mu^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\\)\n\nBernoulli distribution: \\[\nf(x \\mid p) = p^x(1-p)^{1-x} = \\exp\\left(x\\log\\frac{p}{1-p} + \\log(1-p)\\right), \\quad x \\in \\{0, 1\\}\n\\]\n\n\\(\\theta = \\log\\frac{p}{1-p}\\), \\(T(x) = x\\), \\(\\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta})\\)\n\nPoisson distribution: \\[\nf(x \\mid \\lambda) = \\frac{\\lambda^x e^{\\lambda}}{x!}= \\frac{1}{x!}\\exp(x\\log\\lambda - \\lambda), \\quad x = 0, 1, 2, \\ldots\n\\]\n\n\\(\\theta = \\log\\lambda\\), \\(T(x) = x\\), \\(\\psi(\\theta) = \\exp(\\theta) = \\lambda\\)"
  },
  {
    "objectID": "slides/02-lm.html#generalized-linear-model-glm",
    "href": "slides/02-lm.html#generalized-linear-model-glm",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Model (GLM)",
    "text": "Generalized Linear Model (GLM)\n\nLet \\(Y\\) be univariate, \\(\\boldsymbol{x} \\in \\R^p\\), and \\(\\boldsymbol{\\beta} \\in \\R^p\\).\nA GLM is assuming \\(Y \\mid \\boldsymbol{x} \\sim F_{\\theta}\\), where \\(\\theta = \\boldsymbol{x}^T\\boldsymbol{\\beta}\\) and \\(F_\\theta\\) has the density function \\[\nf(y \\mid \\theta) = h(y)\\exp(\\theta\\cdot y - \\psi(\\theta)).\n\\]\nTherefore \\[\\begin{align*}\n\\E(Y \\mid \\boldsymbol{x}) & = \\frac{d}{d\\theta}\\psi(\\theta) = \\psi^{\\prime}(\\boldsymbol{x}^T\\boldsymbol{\\beta}).\n\\end{align*}\\]\nEquivalently, \\[\ng(\\E(Y \\mid \\boldsymbol{x})) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n\\] where \\(g\\) is the inverse of \\(\\psi^{\\prime}\\).\nThe function \\(g\\) is called the link function."
  },
  {
    "objectID": "slides/02-lm.html#logistic-regression",
    "href": "slides/02-lm.html#logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor Bernoulli distributions, we have \\[\n\\theta = \\log\\frac{p}{1-p}, \\quad \\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta}).\n\\]\nThus, \\(\\psi^{\\prime}(\\theta) = \\frac{e^{\\theta}}{1 + e^{\\theta}}\\) and \\(g(p) = (\\psi^{\\prime})^{-1}(p) = \\log\\frac{p}{1-p}\\). \\(\\psi^{\\prime}\\) is called the logistic function and \\(g\\) is called the logit function.\nPutting altogether, we have \\[\ng(\\E(Y \\mid \\boldsymbol{x})) = \\log\\left(\\frac{p(\\boldsymbol{x})}{1 - p(\\boldsymbol{x})}\\right) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n\\] or equivalently \\[\n\\P(Y = 1 \\mid \\boldsymbol{x}) = \\psi^{\\prime}(\\boldsymbol{x}^T\\boldsymbol{\\beta}) = \\frac{\\exp(\\boldsymbol{x}^T\\boldsymbol{\\beta})}{1 + \\exp(\\boldsymbol{x}^T\\boldsymbol{\\beta})}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#remarks",
    "href": "slides/02-lm.html#remarks",
    "title": "Generalized Linear Models",
    "section": "Remarks",
    "text": "Remarks\n\nThe link function \\(g = (\\psi^{\\prime})^{-1}\\) is sometimes called the canonical link function, since it is derived from the canonical representation of an exponential family.\nAll we need for a link function is that it is invertible and matches the range of \\(\\E(Y \\mid \\boldsymbol{x})\\) and \\(\\boldsymbol{x}^T\\boldsymbol{\\beta}\\).\nFor example, in the Bernoulli linear model, we could have used the probit link function \\[\ng(u) = \\Phi^{-1}(u): [0, 1] \\to \\R\n\\] where \\(\\Phi\\) is the CDF of the standard normal distribution.\nThis is called the probit regression."
  },
  {
    "objectID": "slides/02-lm.html#multinomial-regression",
    "href": "slides/02-lm.html#multinomial-regression",
    "title": "Generalized Linear Models",
    "section": "Multinomial Regression",
    "text": "Multinomial Regression\n\nMultinomial regression is a generalization of Logistic regression to categorical variables with more than two categories.\nSuppose \\(Y\\) is a categorical variable with \\(K\\) categories, \\(Y \\in \\{1, 2, \\ldots, K\\}\\).\nA more useful representation is to use the one-hot encoding: \\[\nY = [0, 0, \\ldots, 1, \\ldots, 0]^T\n\\] where the \\(k\\)-th element is 1 and the rest are 0.\nThe multinomial regression model assumes \\[\nY \\mid \\boldsymbol{x} \\sim \\text{Multi}(1, [p_1(\\boldsymbol{x}), p_2(\\boldsymbol{x}), \\ldots, p_K(\\boldsymbol{x})]^T)\n\\] where \\(p_k(\\boldsymbol{x}) = \\P(Y = 1_k \\mid \\boldsymbol{x})\\) and \\(1_k\\) is the one-hot encoding of the \\(k\\)-th category."
  },
  {
    "objectID": "slides/02-lm.html#multinomial-distribution",
    "href": "slides/02-lm.html#multinomial-distribution",
    "title": "Generalized Linear Models",
    "section": "Multinomial Distribution",
    "text": "Multinomial Distribution\n\nThe probability mass function of the \\(\\text{Multi}(m,p)\\) is \\[\nf(x \\mid p) = \\frac{m!}{x_1!\\cdots x_K!}\\prod_{k=1}^K p_k^{x_k} = \\frac{m!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^K x_k\\log p_k\\right)\n\\] where \\(x = [x_1, x_2, \\ldots, x_K]^T\\), \\(\\sum_{k=1}^K x_k = m\\), and \\(\\sum_{k=1}^K p_k = 1\\).\nNote that \\(p_K = 1 - p_1 - \\ldots - p_{K-1}\\) and therefore \\[\\begin{align*}\nf(x \\mid p) & = \\frac{m!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^{K-1} x_k\\log p_k + \\left(m - \\sum_{k=1}^{K-1} x_k\\right)\\log(1 - p_1 - \\ldots - p_{K-1})\\right)\\\\\n& = \\frac{m!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^{K-1} x_k\\log\\frac{p_k}{p_K} + m\\log(1 - p_1 - \\ldots - p_{K-1})\\right).\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-lm.html#softmax-function",
    "href": "slides/02-lm.html#softmax-function",
    "title": "Generalized Linear Models",
    "section": "Softmax Function",
    "text": "Softmax Function\n\nThe canonical parameter is \\(\\theta = [\\log\\frac{p_1}{p_K}, \\ldots, \\log\\frac{p_{K-1}}{p_K}]^T\\) and therefore \\(p_i = p_K\\exp(\\theta_i)\\).\nUsing the relationship \\(p_K = 1 - \\sum_{k=1}^{K-1} p_k\\), we have \\[\np_K = 1 - p_K\\sum_{k=1}^{K-1} \\exp(\\theta_k) \\quad \\Rightarrow \\quad p_K = \\frac{1}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}.\n\\]\nHence (assume \\(m = 1\\) for simplicity) \\[\\begin{align*}\n\\psi(\\theta) & = - \\log(1 - p_1 - \\ldots - p_{K-1})\n= - \\log(1 - p_Ke^{\\theta_1} - \\ldots - p_Ke^{\\theta_{K-1}})\\\\\n& = - \\log\\left(1 - \\frac{\\sum_{k=1}^{K-1} \\exp(\\theta_k)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}\\right)\n= \\log\\left(1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)\\right).\n\\end{align*}\\]\nTaking the derivative, we have the softmax function: \\[\n\\nabla_{\\theta}\\psi(\\theta) = \\left[\\frac{\\exp(\\theta_1)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}, \\ldots, \\frac{\\exp(\\theta_{K-1})}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}\\right].\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#multinomial-regression-1",
    "href": "slides/02-lm.html#multinomial-regression-1",
    "title": "Generalized Linear Models",
    "section": "Multinomial Regression",
    "text": "Multinomial Regression\n\nThe multinomial regression model is given by \\[\\begin{align*}\n   \\theta_i & = \\boldsymbol{x}^T\\boldsymbol{\\beta}_i, \\\\\n   p_i(\\boldsymbol{x}) & = \\frac{\\exp(\\theta_i)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_i)}, \\quad i = 1, 2, \\ldots, K-1,\n\\end{align*}\\] where \\(\\boldsymbol{\\beta}_i \\in \\R^p\\).\nIn fact, a more common representation is \\[\\begin{align*}\n   \\tilde{\\theta}_i & = \\boldsymbol{x}^T\\tilde{\\boldsymbol{\\beta}}_i, \\\\\n   p_i(\\boldsymbol{x}) & = \\frac{\\exp(\\tilde{\\theta}_i)}{\\sum_{k=1}^{K} \\exp(\\tilde{\\theta}_i)}, \\quad i = 1, 2, \\ldots, K.\n\\end{align*}\\]\nThe equivalence is due to the transformation \\(\\theta_i = \\tilde{\\theta}_i - \\tilde{\\theta}_K\\) and \\(\\boldsymbol{\\beta}_i = \\tilde{\\boldsymbol{\\beta}}_i - \\tilde{\\boldsymbol{\\beta}}_K\\). We can also write \\[\n[p_1(\\boldsymbol{x}), \\ldots, p_K(\\boldsymbol{x})] = \\texttt{softmax}(\\boldsymbol{x}^T\\boldsymbol{\\beta}_1, \\ldots, \\boldsymbol{x}^T\\boldsymbol{\\beta}_K).\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#quick-summary",
    "href": "slides/02-lm.html#quick-summary",
    "title": "Generalized Linear Models",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nA GLM is \\[\n   g(\\E(Y \\mid X = x)) = x^T\\beta \\Leftrightarrow \\E(Y \\mid X = x) = g^{-1}(x^T\\beta).\n\\]\nThe link function \\(g\\) connects the conditional expectation and the linear predictor and is chosen based on the distribution of \\(Y\\).\nExamples:\n\nLogistic regression: \\(g(p) = \\log\\left(\\frac{p}{1-p}\\right)\\), \\(g^{-1}(x) = \\frac{1}{1+e^{-x}}\\).\nLinear regression: \\(g(\\mu) = \\mu\\).\nMultinomial regression: softmax function.\nThere are other choices and the above are called the canonical link functions."
  },
  {
    "objectID": "slides/02-lm.html#beyond-linearity",
    "href": "slides/02-lm.html#beyond-linearity",
    "title": "Generalized Linear Models",
    "section": "Beyond Linearity",
    "text": "Beyond Linearity\n\nUp to now, we have assumed that a linear relationship between the features and the (transformed) conditional expectation: \\[\n   g(\\E(Y \\mid \\boldsymbol{x})) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n\\]\nHowever, this is a strong assumption and may not be appropriate in many cases.\nTo remove this assumption, we can consider \\[\n   g(\\E(Y \\mid \\boldsymbol{x})) = f(\\boldsymbol{x})\n\\] where \\(f: \\R^p \\to \\R\\) is an unknown function.\nThe problem is now to estimate the function \\(f\\).\nDepending on the restrictions on \\(f\\), we can use different methods to estimate \\(f\\)."
  },
  {
    "objectID": "slides/02-lm.html#generalized-additive-models-gam",
    "href": "slides/02-lm.html#generalized-additive-models-gam",
    "title": "Generalized Linear Models",
    "section": "Generalized Additive Models (GAM)",
    "text": "Generalized Additive Models (GAM)\n\nAn additive model assumes that the unknown function \\(f\\) is a sum of univariate functions: \\[\nf(\\boldsymbol{x}) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p).\n\\]\nTherefore, the model is \\[\ng(\\E(Y \\mid \\boldsymbol{x})) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p).\n\\]\nThe functions \\(f_j: \\R \\to \\R\\) are unknown and need to be estimated.\nNonparametric methods can be used to estimate the functions \\(f_j\\), for example, kernel smoothing, splines, etc.\nThe GAM is a generalization of the linear model that allows for non-linear relationships between the features and the conditional expectation."
  },
  {
    "objectID": "slides/02-lm.html#projection-pursuit-regression-ppr",
    "href": "slides/02-lm.html#projection-pursuit-regression-ppr",
    "title": "Generalized Linear Models",
    "section": "Projection Pursuit Regression (PPR)",
    "text": "Projection Pursuit Regression (PPR)\n\nThe projection pursuit regression (PPR)1 model assumes: \\[\nf(\\boldsymbol{x}) = \\sum_{m=1}^M f_m(\\boldsymbol{x}^T\\boldsymbol{\\omega}_m),\n\\] where \\(\\boldsymbol{\\omega}_m \\in \\R^p\\) are unknown unit vectors and \\(f_m: \\R \\to \\R\\) are unknown functions.\nThe scalar variable \\(V_m = \\boldsymbol{x}^T\\boldsymbol{\\omega}_m\\) is the projection of \\(\\boldsymbol{x}\\) onto the unit vector \\(\\boldsymbol{\\omega}_m\\), and we seek \\(\\boldsymbol{\\omega}_m\\) so that the model fits well, hence the name “projection pursuit.”\nIf \\(M\\) is taken arbitrarily large, for appropriate choice of \\(f_m\\) the PPR model can approximate any continuous function in \\(\\R^p\\) arbitrarily well, i.e., the PPR is a universal approximator.\nHowever, this model is not widely used due to the difficulty in estimating the functions \\(f_m\\).\n\nFriedman, J. H., & Tukey, J. W. (1974). A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on computers, 100(9), 881-890."
  },
  {
    "objectID": "slides/02-lm.html#the-log-likelihood-function",
    "href": "slides/02-lm.html#the-log-likelihood-function",
    "title": "Generalized Linear Models",
    "section": "The log-likelihood function",
    "text": "The log-likelihood function\nIn order to find the MLE, we need to simplify the log-likelihood function: \\[\\begin{align*}\n\\ell(\\boldsymbol{\\beta}) & = \\log \\left(\\prod_{i=1}^n p(\\boldsymbol{x}_i)^{y_i}(1-p(\\boldsymbol{x}_i))^{1-y_i}\\right) \\\\\n& = \\sum_{i=1}^n y_i \\log p(\\boldsymbol{x}_i) + (1-y_i)\\log(1-p(\\boldsymbol{x}_i))\\\\\n& = \\sum_{i=1}^n \\left[y_i \\log \\left(\\frac{p(\\boldsymbol{x}_i)}{1-p(\\boldsymbol{x}_i)}\\right) + \\log(1-p(\\boldsymbol{x}_i))\\right]\\\\\n& = \\sum_{i=1}^n \\left[y_i\\boldsymbol{x}_i^T\\boldsymbol{\\beta} - \\log(1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}))\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-lm.html#gradient-and-hessian",
    "href": "slides/02-lm.html#gradient-and-hessian",
    "title": "Generalized Linear Models",
    "section": "Gradient and Hessian",
    "text": "Gradient and Hessian\nNow we compute the gradient and the Hessian of the log-likelihood function:\n\\[\\begin{align*}\n    \\nabla \\ell(\\boldsymbol{\\beta}) & = \\sum_{i=1}^n \\left[y_i \\boldsymbol{x}_i - \\frac{\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})}{1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})}\\boldsymbol{x}_i\\right] = \\sum_{i=1}^n (y_i - p(\\boldsymbol{x}_i))\\boldsymbol{x}_i\\\\\n    & = X^T(\\boldsymbol{y}-\\mathbf{p})\\\\\n    \\nabla^2 \\ell(\\boldsymbol{\\beta}) & = -\\sum_{i=1}^n p(\\boldsymbol{x}_i)(1-p(\\boldsymbol{x}_i))\\boldsymbol{x}_i\\boldsymbol{x}_i^T = -X^TWX\n\\end{align*}\\] where \\[\n\\mathbf{p} = [p(\\boldsymbol{x}_1), \\ldots, p(\\boldsymbol{x}_n)]^T, \\quad W= \\diag(\\mathbf{p})\\diag(1-\\mathbf{p}), \\quad X = \\begin{bmatrix} \\boldsymbol{x}_1^T \\\\ \\vdots \\\\ \\boldsymbol{x}_n^T \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#iteratively-re-weighted-least-squares-irwls",
    "href": "slides/02-lm.html#iteratively-re-weighted-least-squares-irwls",
    "title": "Generalized Linear Models",
    "section": "Iteratively Re-Weighted Least Squares (IRWLS)",
    "text": "Iteratively Re-Weighted Least Squares (IRWLS)\nThere is no analytic solution for the MLE of the logistic regression. However, the Newton-Raphson method can be used to find the MLE. The Newton-Raphson method is an iterative method that updates the parameter \\(\\boldsymbol{\\beta}\\) as follows:\n\\[\\begin{align*}\n\\boldsymbol{\\beta}^{(t+1)} & = \\boldsymbol{\\beta}^{(t)} - \\left[\\nabla^2 \\ell(\\boldsymbol{\\beta}^{(t)})\\right]^{-1} \\nabla \\ell\\left(\\boldsymbol{\\beta}^{(t)}\\right) \\\\\n& = \\boldsymbol{\\beta}^{(t)}+\\left(X^T W^{(t)}X\\right)^{-1} X^T\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right) \\\\\n& = \\left(X^T W^{(t)} X\\right)^{-1} X^T W^{(t)}\\left[X \\boldsymbol{\\beta}^{(t)}+\\left(W^{(t)}\\right)^{-1}\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right)\\right] \\\\\n& = \\left(X^T W^{(t)} X\\right)^{-1} X^T W^{(t)} \\mathbf{z}^{(t)}\n\\end{align*}\\] where \\[\\begin{align*}\n\\mathbf{z}^{(t)} & =X \\boldsymbol{\\beta}^{(t)}+\\left(W^{(t)}\\right)^{-1}\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right), \\quad \\mathbf{p}^{(t)} = [p^{(t)}(\\boldsymbol{x}_1), \\ldots, p^{(t)}(\\boldsymbol{x}_n)]^T\\\\\np^{(t)}(\\boldsymbol{x}_i) & = \\frac{\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}^{(t)})}{1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}^{(t)})}, \\quad\nW^{(t)} = \\diag(\\mathbf{p}^{(t)})\\diag(1-\\mathbf{p}^{(t)}).\n\\end{align*}\\]\n\n\n\nHome"
  },
  {
    "objectID": "slides/11-generative_model_2.html#outline",
    "href": "slides/11-generative_model_2.html#outline",
    "title": "Generative Models",
    "section": "Outline",
    "text": "Outline\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nGenerative Adversarial Networks (GAN)\n\nLoss function of GAN\nJensen–Shannon Divergence\nWasserstein GAN (WGAN)\n\nFlow-based Generative Models\n\nChange of Variable\nNormalizing Flow (NF)\nCoupling Layers\n\nDiffusion Model\n\nDenoising Diffusion Probabilistic Model (DDPM)"
  },
  {
    "objectID": "slides/11-generative_model_2.html#generative-adversarial-networks-gan",
    "href": "slides/11-generative_model_2.html#generative-adversarial-networks-gan",
    "title": "Generative Models",
    "section": "Generative Adversarial Networks (GAN)",
    "text": "Generative Adversarial Networks (GAN)\n\nA discriminator estimates the probability of a given sample coming from the real dataset.\nA generator generates synthetic samples given a noise variable input.\n\n\n\n\n\n\n\n\nImage source: https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html"
  },
  {
    "objectID": "slides/11-generative_model_2.html#training-objective-of-gan",
    "href": "slides/11-generative_model_2.html#training-objective-of-gan",
    "title": "Generative Models",
    "section": "Training Objective of GAN",
    "text": "Training Objective of GAN\n\nThe discriminator \\(D\\) tries to maximize the probability of correctly classifying the generated samples.\nThe generator \\(G\\) tries to minimize the probability of the discriminator correctly classifying the generated samples.\nThese two models try to compete with each other, leading to a minimax game.\nNotations:\n\n\\(p_{\\text{data}}(\\boldsymbol{x})\\): the distribution of real data \\(\\boldsymbol{x} \\in \\mathbb{R}^d\\)\n\\(p_{g}(\\boldsymbol{x})\\): the distribution of generated data.\n\\(p_{\\boldsymbol{z}}(\\boldsymbol{z})\\): the distribution of noise variable \\(\\boldsymbol{z} \\in \\mathbb{R}^k\\).\n\\(D(\\boldsymbol{x})\\): the discriminator, i.e., the probability of \\(\\boldsymbol{x}\\) being real (\\(D: \\mathbb{R}^d \\to [0,1]\\))\n\\(G(\\boldsymbol{z})\\): the generator (\\(G: \\mathbb{R}^k \\to \\mathbb{R}^d\\))."
  },
  {
    "objectID": "slides/11-generative_model_2.html#training-objective-of-gan-1",
    "href": "slides/11-generative_model_2.html#training-objective-of-gan-1",
    "title": "Generative Models",
    "section": "Training Objective of GAN",
    "text": "Training Objective of GAN\n\nWe train \\(D\\) to maximize the probability of assigning the correct label to both training examples and samples from \\(G\\).\nWe simultaneously train \\(G\\) to minimize \\(\\log(1−D(G(\\boldsymbol{z})))\\).\nIn other words, \\(D\\) and \\(G\\) play the following two-player minimax game with value function \\(V(G,D)\\): \\[\nV(G, D)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))].\n\\]\nThe goal is to maximize \\(V(G, D)\\) with respect to \\(D\\) and then minimize it with respect to \\(G\\), i.e., \\[\n\\min_{G} \\max_{D} V(G, D).\n\\]\nThe discriminator and the generator are both parametrized by deep neural networks: \\(D(\\boldsymbol{x}) = D(\\boldsymbol{x}, \\theta_D)\\) and \\(G(\\boldsymbol{z}) = G(\\boldsymbol{z}, \\theta_G)\\).\nHence the minimization and maximization are with respect to \\(\\theta_G\\) and \\(\\theta_D\\) respectively."
  },
  {
    "objectID": "slides/11-generative_model_2.html#global-optimality-of-d",
    "href": "slides/11-generative_model_2.html#global-optimality-of-d",
    "title": "Generative Models",
    "section": "Global Optimality of \\(D\\)",
    "text": "Global Optimality of \\(D\\)\nProposition (Goodfellow et al., 2014) For \\(G\\) fixed, the optimal discriminator \\(D\\) is \\[\\begin{align*}\nD_G^*(\\boldsymbol{x})=\\frac{p_{\\text {data }}(\\boldsymbol{x})}{p_{\\text {data }}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\n\\end{align*}\\]\nProof: The training criterion for the discriminator \\(D\\), given any generator \\(G\\), is to maximize the quantity \\(V(G, D)\\)\n\\[\\begin{align*}\n\\begin{aligned}\nV(G, D) & =\\int_{\\boldsymbol{x}} p_{\\text {data }}(\\boldsymbol{x}) \\log (D(\\boldsymbol{x})) d x+\\int_{\\boldsymbol{z}} p_{\\boldsymbol{z}}(\\boldsymbol{z}) \\log (1-D(g(\\boldsymbol{z}))) d z \\\\\n& =\\int_{\\boldsymbol{x}} p_{\\text {data }}(\\boldsymbol{x}) \\log (D(\\boldsymbol{x}))+p_g(\\boldsymbol{x}) \\log (1-D(\\boldsymbol{x})) d x\n\\end{aligned}\n\\end{align*}\\]\nFor any \\((a, b) \\in \\mathbb{R}^2 \\backslash\\{0,0\\}\\), the function \\(y \\rightarrow a \\log (y)+b \\log (1-y)\\) achieves its maximum in \\([0,1]\\) at \\(\\frac{a}{a+b}\\)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#remarks",
    "href": "slides/11-generative_model_2.html#remarks",
    "title": "Generative Models",
    "section": "Remarks",
    "text": "Remarks\n\nNote that the training objective for \\(D\\) can be interpreted as maximizing the log-likelihood for estimating the conditional probability \\(\\mathbb{P}(Y= y \\mid \\boldsymbol{x})\\), where \\(Y\\) indicates whether x comes from \\(p_{\\text{data}}\\) (with \\(y= 1\\)) or from \\(p_g\\) (with \\(y= 0\\)).\nWhen the generator is trained to optimal, \\(p_g\\) gets closer to \\(p_{\\text{data}}\\), and thus \\(D^*_G(\\boldsymbol{x}) \\approx \\frac{1}{2}\\).\nThe cost of \\(G\\) in the minimax game can be formulated as \\[\n\\begin{aligned}\nC(G) & =\\max _D V(G, D) \\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}}\\left[\\log D_G^*(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}}\\left[\\log \\left(1-D_G^*(G(\\boldsymbol{z}))\\right)\\right] \\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}}\\left[\\log D_G^*(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_g}\\left[\\log \\left(1-D_G^*(\\boldsymbol{x})\\right)\\right] \\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}}\\left[\\log \\frac{p_{\\text {data }}(\\boldsymbol{x})}{P_{\\text {data }}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_g}\\left[\\log \\frac{p_g(\\boldsymbol{x})}{p_{\\text {data }}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]\n\\end{aligned}\n\\]\nThe goal is to minimize the cost function \\(C(G)\\), which is equivalent to minimizing the Jensen-Shannon divergence between \\(p_{\\text{data}}\\) and \\(p_g\\)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#jensenshannon-divergence",
    "href": "slides/11-generative_model_2.html#jensenshannon-divergence",
    "title": "Generative Models",
    "section": "Jensen–Shannon Divergence",
    "text": "Jensen–Shannon Divergence\n\nRecall that the KL divergence is \\[\nD_{\\text{KL}}(p \\| q) = \\int_{\\boldsymbol{x}} p(\\boldsymbol{x}) \\log \\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})} d \\boldsymbol{x} = \\mathbb{E}_{\\boldsymbol{x} \\sim p} \\left[ \\log \\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})} \\right].\n\\]\nOne of the main issues with KL divergence is that it is asymmetric, i.e., \\(D_{\\text{KL}}(p \\| q) \\neq D_{\\text{KL}}(q \\| p)\\).\nThe Jensen–Shannon divergence is a symmetrized version of KL divergence, which is defined as \\[\nD_{\\text{JS}}(p \\| q) = \\frac{1}{2} D_{\\text{KL}}\\left(p \\bigg\\| \\frac{p+q}{2}\\right) + \\frac{1}{2} D_{\\text{KL}}\\left(q \\bigg\\| \\frac{p+q}{2}\\right).\n\\]"
  },
  {
    "objectID": "slides/11-generative_model_2.html#the-jsd-loss-for-g",
    "href": "slides/11-generative_model_2.html#the-jsd-loss-for-g",
    "title": "Generative Models",
    "section": "The JSD loss for \\(G\\)",
    "text": "The JSD loss for \\(G\\)\n\nThe JS divergence between \\(p_{\\text{data}}\\) and \\(p_g\\) is \\[\n\\begin{aligned}\nD_{\\text{JS}}(p_{\\text{data}} \\| p_g) &= \\frac{1}{2} D_{\\text{KL}}\\left(p_{\\text{data}} \\bigg\\| \\frac{p_{\\text{data}}+p_g}{2}\\right) + \\frac{1}{2} D_{\\text{KL}}\\left(p_g \\bigg\\| \\frac{p_{\\text{data}}+p_g}{2}\\right) \\\\\n& = \\frac{1}{2}\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text{data}}} \\left[ \\log \\frac{2p_{\\text{data}}(\\boldsymbol{x})}{p_{\\text{data}}(\\boldsymbol{x})+p_g(\\boldsymbol{x})} \\right] + \\frac{1}{2}\\mathbb{E}_{\\boldsymbol{x} \\sim p_g} \\left[ \\log \\frac{2p_g(\\boldsymbol{x})}{p_{\\text{data}}(\\boldsymbol{x})+p_g(\\boldsymbol{x})} \\right] \\\\\n& = \\log 2 + \\frac{1}{2} C(G).\n\\end{aligned}\n\\]\nTherefore \\(C(G) = 2D_{\\text{JS}}(p_{\\text{data}} \\| p_g) - 2\\log 2\\) and \\(C(G)\\) attains its minimum of \\(-2\\log 2\\) when \\(p_{\\text{data}} = p_g\\)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#algorithm",
    "href": "slides/11-generative_model_2.html#algorithm",
    "title": "Generative Models",
    "section": "Algorithm",
    "text": "Algorithm\n\n\n\n\n\n\n\nSource: Generative Adversarial Nets by Goodfellow et al. (2014)"
  },
  {
    "objectID": "slides/11-generative_model_2.html#problems-in-gans",
    "href": "slides/11-generative_model_2.html#problems-in-gans",
    "title": "Generative Models",
    "section": "Problems in GANs",
    "text": "Problems in GANs\n\nTheoretically, the training objective of GANs has a global optimum (also called the Nash equilibrium): \\(G(\\boldsymbol{z})\\) is such that \\(p_g = p_{\\text{data}}\\) and \\(D(\\boldsymbol{x}) = 1/2\\).\nThat is, the synthetic data generated by \\(G\\) is indistinguishable from the real data.\nIn practice, training GANs using gradient-based optimization is challenging due to the following reasons:\n\nLow dimensional support: The support of the real data distribution is usually low-dimensional, making the JS divergence numerically unstable.\nMode collapse: The generator always produces the same outputs.\nVanishing gradients: When the discriminator is too good, the gradients of the generator vanish.\n\nThere are many ways to improve the training of GANs, e.g., Salimans et al. (2016) and Bottou and Arjovsky (2017).\nWe discuss one of the most popular improvements, the Wasserstein GAN (WGAN).\n\n\n\n\nSalimans et al. (2016). Improved Techniques for Training GANs. In NeurIPS.\nBottou and Arjovsky (2017). Towards Principled Methods for Training Generative Adversarial Networks."
  },
  {
    "objectID": "slides/11-generative_model_2.html#wasserstein-distance",
    "href": "slides/11-generative_model_2.html#wasserstein-distance",
    "title": "Generative Models",
    "section": "Wasserstein Distance",
    "text": "Wasserstein Distance\n\nThe Wasserstein distance between two distributions \\(p\\) and \\(q\\) is defined as \\[\nW_r(p, q) = \\inf_{\\gamma \\in \\Pi(p, q)} \\Big(\\mathbb{E}_{(\\boldsymbol{x}, \\boldsymbol{y}) \\sim \\gamma} \\left[ \\| \\boldsymbol{x} - \\boldsymbol{y} \\|^r \\right]\\Big)^{1/r},\n\\] where \\(\\Pi(p, q)\\) is the set of all joint distributions \\(\\gamma(\\boldsymbol{x}, \\boldsymbol{y})\\) whose marginals are \\(p\\) and \\(q\\).\nFor \\(r = 1\\), the Wasserstein distance is called the Earth Mover’s distance.\nFor \\(r = 1\\), we also have the Kantorovich-Rubinstein duality: The Wasserstein distance can be expressed as \\[\nW_1(p, q) = \\sup_{\\| f \\|_L \\leq 1} \\mathbb{E}_{\\boldsymbol{x} \\sim p} [f(\\boldsymbol{x})] - \\mathbb{E}_{\\boldsymbol{y} \\sim q} [f(\\boldsymbol{y})],\n\\] where the supremum is taken over all 1-Lipschitz functions \\(f\\).\nNote: a function \\(f\\) is \\(k\\)-Lipschitz if \\(|f(\\boldsymbol{x}) - f(\\boldsymbol{y})| \\leq k \\| \\boldsymbol{x} - \\boldsymbol{y} \\|\\) for all \\(\\boldsymbol{x}, \\boldsymbol{y}\\)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#wasserstein-distance-vs.-jskl-divergence",
    "href": "slides/11-generative_model_2.html#wasserstein-distance-vs.-jskl-divergence",
    "title": "Generative Models",
    "section": "Wasserstein distance vs. JS/KL divergence",
    "text": "Wasserstein distance vs. JS/KL divergence\n\nThe KL divergence is motivated from Information theory while the Wasserstein distance is motivated from the optimal transport problem.\nThe main disadvantage of KL divergence is that it is not symmetric and is not defined when the support of \\(p\\) is not contained in the support of \\(q\\).\nThe asymmetry of KL divergence can be solved by the JS divergence.\nHowever the support problem still exists in JS divergence and is the reason for numerical instability in GAN.\nThe Wasserstein distance is symmetric and is defined even when the supports of \\(p\\) and \\(q\\) are disjoint.\nThe disjoint support issue is common in high-dimensional data, as in those cases, the samples are usually concentrated on a low-dimensional manifold."
  },
  {
    "objectID": "slides/11-generative_model_2.html#wasserstein-gan-wgan",
    "href": "slides/11-generative_model_2.html#wasserstein-gan-wgan",
    "title": "Generative Models",
    "section": "Wasserstein GAN (WGAN)",
    "text": "Wasserstein GAN (WGAN)\n\nIn the standard GAN, the generator \\(G\\) is trained to minimize the cost \\[\n\\begin{aligned}\nC(G) & =\\max _D V(G, D) \\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text{data}}}\\left[\\log \\frac{p_{\\text {data }}(\\boldsymbol{x})}{P_{\\text{data}}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_g}\\left[\\log \\frac{p_g(\\boldsymbol{x})}{p_{\\text{data}}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]\\\\\n& = 2D_{\\text{JS}}(p_{\\text{data}} \\| p_g) - 2\\log 2.\n\\end{aligned}\n\\]\nIn WGAN, the generator is trained to minimize the Wasserstein distance between \\(p_{\\text{data}}\\) and \\(p_g\\), i.e., \\[\n\\begin{aligned}\nC_W(G) & = \\max _{w \\in \\mathcal{W}} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text{data}}}\\left[f_w(x)\\right]-\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}}\\left[f_w\\left(G(\\boldsymbol{z})\\right)\\right] \\\\\n& = \\max _{w \\in \\mathcal{W}} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text{data}}}\\left[f_w(x)\\right]-\\mathbb{E}_{\\boldsymbol{x} \\sim p_g}\\left[f_w(\\boldsymbol{x})\\right]\\\\\n& \\approx W_1(p_{\\text{data}}, p_g),\n\\end{aligned}\n\\] where \\(\\{f_w\\}_{w \\in \\mathcal{W}}\\) is a family of 1-Lipschitz functions.\nIn WGAN, the function \\(f_w\\) serves as the critic (analogous to the discriminator in GAN)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#synthetic-example",
    "href": "slides/11-generative_model_2.html#synthetic-example",
    "title": "Generative Models",
    "section": "Synthetic Example",
    "text": "Synthetic Example\nUsing GAN/WGAN to learn a mixture of 8 Gaussians spread in a circle\n\n\n\n\n\n\n\nThe Unrolled GAN is proposed by Metz et al. (2017) in Unrolled Generative Adversarial Networks."
  },
  {
    "objectID": "slides/11-generative_model_2.html#flow-based-generative-models-1",
    "href": "slides/11-generative_model_2.html#flow-based-generative-models-1",
    "title": "Generative Models",
    "section": "Flow-based Generative Models",
    "text": "Flow-based Generative Models\n\nA flow-based generative model is constructed by a sequence of invertible transformations.\nGAN: \\(p(\\boldsymbol{x})\\) is learned implicitly by a minimax game between the generator and the discriminator.\nVAE: \\(p(\\boldsymbol{x})\\) is learned implicitly by maximizing the ELBO in the encoder-decoder model.\nUnlike other two, the model explicitly learns the data distribution and therefore the loss function is simply the negative log-likelihood."
  },
  {
    "objectID": "slides/11-generative_model_2.html#change-of-variable",
    "href": "slides/11-generative_model_2.html#change-of-variable",
    "title": "Generative Models",
    "section": "Change of Variable",
    "text": "Change of Variable\n\nLet \\(\\boldsymbol{z} \\sim p(\\boldsymbol{z})\\) be a random vector and \\(\\boldsymbol{x} = f(\\boldsymbol{z})\\), where \\(f: \\mathbb{R}^d \\to \\mathbb{R}^d\\) is an invertible function.\nThen \\(\\boldsymbol{z} = f^{-1}(\\boldsymbol{x})\\) and the probability density of \\(\\boldsymbol{x}\\) is given by \\[\n\\begin{aligned}\np_x(\\boldsymbol{x}) = p(\\boldsymbol{z})\\left|\\det \\frac{d \\boldsymbol{z}}{d\\boldsymbol{x}}\\right|=p\\left(f^{-1}(\\boldsymbol{x})\\right)\\left|\\det \\frac{d f^{-1}}{d \\boldsymbol{x}}\\right|.\n\\end{aligned}\n\\]\nThe matrix \\(\\frac{d f}{d \\boldsymbol{z}}\\) is called the Jacobian matrix of \\(f\\), denoted by \\(J_f\\) \\[\nJ_f(\\boldsymbol{z}) = \\frac{d f}{d \\boldsymbol{z}} =\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial z_1} & \\cdots & \\frac{\\partial f_1}{\\partial z_d} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_d}{\\partial z_1} & \\cdots & \\frac{\\partial f_d}{\\partial z_d}\n\\end{bmatrix}.\n\\]\nHence, \\[\n\\begin{aligned}\np_x(\\boldsymbol{x}) & = p\\left(f^{-1}(\\boldsymbol{x})\\right)\\left|\\det J_{f^{-1}}(\\boldsymbol{x})\\right| = p(f^{-1}(\\boldsymbol{x})) \\left| \\det J_f^{-1}(f^{-1}(\\boldsymbol{x}))\\right|\\\\\n& = p(f^{-1}(\\boldsymbol{x})) \\left| \\det J_f(f^{-1}(\\boldsymbol{x}))\\right|^{-1}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/11-generative_model_2.html#normalizing-flows",
    "href": "slides/11-generative_model_2.html#normalizing-flows",
    "title": "Generative Models",
    "section": "Normalizing Flows",
    "text": "Normalizing Flows\n\nA normalizing flow transforms a simple distribution into a complex one by applying a sequence of invertible transformations.\nThe change of variable formula is used to compute the probability density of the transformed distribution.\nLet \\(\\boldsymbol{z}_0\\) be a random variable with distribution \\(p_0(\\boldsymbol{z}_0)\\), usually a simple distribution like Gaussian.\nLet \\(\\boldsymbol{z}_i = f_i(\\boldsymbol{z}_{i-1})\\) be the transformed variable after applying the \\(i\\)-th transformation and \\(p_i(\\boldsymbol{z}_i)\\) be the distribution of \\(\\boldsymbol{z}_i\\).\nLet \\(\\boldsymbol{x} = \\boldsymbol{z}_k = f_k \\circ \\cdots \\circ f_1(\\boldsymbol{z}_0)\\). The log-likelihood of \\(x\\) is given by \\[\n\\begin{aligned}\n\\log p(\\boldsymbol{x}) &= \\log p_k(\\boldsymbol{z}_k)\\\\\n& = \\log p_{k-1}(f_k^{-1}(\\boldsymbol{z}_k)) - \\log \\left| \\det J_{f_k}(f_k^{-1}(\\boldsymbol{z}_k))\\right|\\\\\n& = \\log p_{k-1}(\\boldsymbol{z}_{k-1}) - \\log \\left| \\det J_{f_k}(\\boldsymbol{z}_{k-1}) \\right| \\\\\n& = \\log p_0(\\boldsymbol{z}_0) - \\sum_{i=1}^k \\log \\left| \\det J_{f_i}(\\boldsymbol{z}_{i-1}) \\right|.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/11-generative_model_2.html#normalizing-flows-1",
    "href": "slides/11-generative_model_2.html#normalizing-flows-1",
    "title": "Generative Models",
    "section": "Normalizing Flows",
    "text": "Normalizing Flows\n\nTo maximize the log-likelihood \\(\\log p(\\boldsymbol{x})\\), we need to maximize \\(- \\sum_{i=1}^k \\log \\left| \\det J_{f_i}(\\boldsymbol{z}_{i-1}) \\right|\\), or equivalently, \\[\n\\min \\sum_{i=1}^k \\log \\left| \\det J_{f_i}(\\boldsymbol{z}_{i-1}) \\right|.\n\\]\nRequiring by the computation in the change of variable formula, the transformations \\(f_i\\) should satisfy the two properties:\n\nInvertibility: \\(f_i\\) is easily invertible.\nEfficient Jacobian computation: The determinant of the Jacobian matrix of \\(f_i\\) can be computed efficiently."
  },
  {
    "objectID": "slides/11-generative_model_2.html#general-coupling-layer",
    "href": "slides/11-generative_model_2.html#general-coupling-layer",
    "title": "Generative Models",
    "section": "General Coupling Layer",
    "text": "General Coupling Layer\n\nA coupling layer is a type of invertible transformation that is commonly used in normalizing flows.\nLet \\(\\boldsymbol{x} = (\\boldsymbol{x}_1, \\boldsymbol{x}_2) \\in \\mathbb{R}^d\\) be a partition of the input variable \\(\\boldsymbol{x}\\), where \\(\\boldsymbol{x}_1 \\in \\mathbb{R}^{d_1}\\) and \\(\\boldsymbol{x}_2 \\in \\mathbb{R}^{d_2}\\) and \\(d_1 + d_2 = d\\).\nDefine the transformation \\((\\boldsymbol{x}_1, \\boldsymbol{x}_2) \\mapsto \\boldsymbol{y} = (\\boldsymbol{y_1}, \\boldsymbol{y}_2) \\in \\mathbb{R}^d\\) as \\[\n\\begin{aligned}\n\\boldsymbol{y}_1 &= \\boldsymbol{x}_1,\\\\\n\\boldsymbol{y}_2 &= g(\\boldsymbol{x}_2; m(\\boldsymbol{x}_1)),\n\\end{aligned}\n\\] where\n\n\\(m\\) is a function on \\(\\mathbb{R}^{d_1}\\),\n\\(g\\) is a function on \\(\\mathbb{R}^{d_2} \\times m(\\mathbb{R}^{d_1})\\).\n\nThe function \\(g\\) is called a coupling law and it needs to be an invertible map with respect to its first argument given the second."
  },
  {
    "objectID": "slides/11-generative_model_2.html#jacobian-of-coupling-layer",
    "href": "slides/11-generative_model_2.html#jacobian-of-coupling-layer",
    "title": "Generative Models",
    "section": "Jacobian of Coupling Layer",
    "text": "Jacobian of Coupling Layer\n\nTha main advantage of such mapping is that its Jacobian and inverse mapping are easy to compute. The Jacobian matrix of the coupling layer is given by \\[\nJ_{g,m}(\\boldsymbol{x}_1, \\boldsymbol{x}_2) = \\frac{d \\boldsymbol{y}}{d \\boldsymbol{x}} =\n\\begin{bmatrix}\n\\boldsymbol{I}_{d_1} & 0 \\\\\n\\frac{d \\boldsymbol{y}_2}{d \\boldsymbol{x}_1} & \\frac{d \\boldsymbol{y}_2}{d \\boldsymbol{x}_2}\n\\end{bmatrix}.\n\\]\nThe determinant of the Jacobian matrix is given by \\[\n\\begin{aligned}\n\\det J_{g,m}(\\boldsymbol{x}_1, \\boldsymbol{x}_2)  = \\det \\frac{d \\boldsymbol{y}_2}{d \\boldsymbol{x}_2}\n= \\det \\frac{d g(\\boldsymbol{x}_2; m(\\boldsymbol{x}_1))}{d \\boldsymbol{x}_2}.\n\\end{aligned}\n\\]\nAlso we can invert the mapping using \\[\n\\begin{aligned}\n\\boldsymbol{x}_1 &= \\boldsymbol{y}_1,\\\\\n\\boldsymbol{x}_2 &= g^{-1}(\\boldsymbol{y}_2; m(\\boldsymbol{y}_1)).\n\\end{aligned}\n\\]\nWe call such a transformation a coupling layer with coupling function \\(m\\). Note that we don’t need any assumption on the coupling function \\(m\\)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#additive-coupling-layer",
    "href": "slides/11-generative_model_2.html#additive-coupling-layer",
    "title": "Generative Models",
    "section": "Additive Coupling Layer",
    "text": "Additive Coupling Layer\n\nThe simplest choice for \\(g\\) is \\(g(a, b) = a + b\\), and the corresponding coupling layer is called an additive coupling layer.\nTherefore the transformation is given by \\[\n\\boldsymbol{y}_1 = \\boldsymbol{x}_1, \\quad\n\\boldsymbol{y}_2 = \\boldsymbol{x}_2 + m(\\boldsymbol{x}_1).\n\\]\nThe Jacobian matrix of the additive coupling layer is given by \\[\nJ^{\\text{add}}_{m}(\\boldsymbol{x}_1, \\boldsymbol{x}_2) =\n\\begin{bmatrix}\n\\boldsymbol{I}_{d_1} & 0 \\\\\n\\frac{d \\boldsymbol{y}_2}{d \\boldsymbol{x}_1} & \\boldsymbol{I}_{d_2}\n\\end{bmatrix},\\; \\text{and}\\; \\det J^{\\text{add}}_{m}(\\boldsymbol{x}_1, \\boldsymbol{x}_2) = 1.\n\\]\nThe inverse mapping is given by \\[\n\\boldsymbol{x}_1 = \\boldsymbol{y}_1, \\quad\n\\boldsymbol{x}_2 = \\boldsymbol{y}_2 - m(\\boldsymbol{y}_1).\n\\]\nThe coupling function \\(m\\) can be a neural network with \\(d_1\\) input units and \\(d_2\\) output units.\nThis coupling layer is used in the Non-linear Independent Components Estimation (NICE) model1.\n\nDinh et al. (2015). NICE: Non-linear Independent Components Estimation. In ICLR workshop."
  },
  {
    "objectID": "slides/11-generative_model_2.html#affine-coupling-layer",
    "href": "slides/11-generative_model_2.html#affine-coupling-layer",
    "title": "Generative Models",
    "section": "Affine Coupling Layer",
    "text": "Affine Coupling Layer\n\nAnother choice for \\(g\\) is the affine mapping: \\[\n\\boldsymbol{y}_1 = \\boldsymbol{x}_1, \\quad\n\\boldsymbol{y}_2 = \\boldsymbol{x}_2 \\odot \\exp(s(\\boldsymbol{x}_1)) + t(\\boldsymbol{x}_1),\n\\] where \\(s\\) and \\(t\\) stand for scale and translation, and are functions from \\(\\mathbb{R}^{d_1} \\to \\mathbb{R}^{d_2}\\), and \\(\\odot\\) is the element-wise product.\nThe Jacobian matrix and its determinant are given by \\[\nJ^{\\text{aff}}_{s,t}(\\boldsymbol{x}_1, \\boldsymbol{x}_2) =\n\\begin{bmatrix}\n\\boldsymbol{I}_{d_1} & 0 \\\\\n\\frac{d \\boldsymbol{y}_2}{d \\boldsymbol{x}_1} & \\text{diag}(\\exp(s(\\boldsymbol{x}_1)))\n\\end{bmatrix}, \\quad\n\\det J^{\\text{aff}}_{s,t}(\\boldsymbol{x}_1, \\boldsymbol{x}_2) = \\exp\\left(\\sum_{i=1}^{d_2} s_i(\\boldsymbol{x}_1)\\right).\n\\]\nThe inverse mapping is also easy to compute: \\[\n\\boldsymbol{x}_1 = \\boldsymbol{y}_1, \\quad\n\\boldsymbol{x}_2 = (\\boldsymbol{y}_2 - t(\\boldsymbol{y}_1)) \\odot \\exp(-s(\\boldsymbol{y}_1)).\n\\]\nThe functions \\(s\\) and \\(t\\) can be implemented by neural networks.\nThe affine coupling layer is used in the Real-valued Non-Volume Preserving (Real NVP) model1.\n\nDinh et al. (2017). Density Estimation using Real NVP. In ICLR."
  },
  {
    "objectID": "slides/11-generative_model_2.html#combining-coupling-layers",
    "href": "slides/11-generative_model_2.html#combining-coupling-layers",
    "title": "Generative Models",
    "section": "Combining Coupling Layers",
    "text": "Combining Coupling Layers\n\nThe coupling layers are used as the transformation \\(\\boldsymbol{z}_{i-1} \\mapsto \\boldsymbol{z}_i\\) in a normalizing flow model.\nWe can compose several coupling layers to obtain a more complex layered transformation.\nSince a coupling layer leaves part of its input unchanged, we need to exchange the role of the two subsets in the partition in alternating layers, so that the composition of two coupling layers modifies every dimension.\n\nExamining the Jacobian, we observe that at least three coupling layers are necessary to allow all dimensions to influence one another. The original paper uses four.\nNote that the coupling layers introduced here are still quite general. If the goal is to generate images or sequential data, we can modify the coupling law using, for example, the convolution operation.\nThe main idea is to design the coupling law such that the Jacobian matrix is easy to compute and invert."
  },
  {
    "objectID": "slides/11-generative_model_2.html#example",
    "href": "slides/11-generative_model_2.html#example",
    "title": "Generative Models",
    "section": "Example",
    "text": "Example\nReal NVP learns an invertible, stable, mapping between a data distribution \\(\\hat{p}_X\\) and a latent distribution \\(p_Z\\) (typically a Gaussian).\n\n\n\n\n\n\n\nImage source: Dinh et al. (2017). Density Estimation using Real NVP. In ICLR."
  },
  {
    "objectID": "slides/11-generative_model_2.html#diffusion-model-1",
    "href": "slides/11-generative_model_2.html#diffusion-model-1",
    "title": "Generative Models",
    "section": "Diffusion Model",
    "text": "Diffusion Model\n\nA diffusion model is a generative model that learns the data distribution by iteratively applying a diffusion process to a simple distribution.\n\n\n\n\n\n\n\n\nImage source: What are Diffusion Models? by Lilian Weng"
  },
  {
    "objectID": "slides/11-generative_model_2.html#forward-diffusion-process",
    "href": "slides/11-generative_model_2.html#forward-diffusion-process",
    "title": "Generative Models",
    "section": "Forward Diffusion Process",
    "text": "Forward Diffusion Process\n\nLet \\(\\boldsymbol{x}_0\\) be a sample from the data distribution \\(p_{\\text{data}}(\\boldsymbol{x})\\).\nThe forward diffusion process adds Gaussian noise to \\(\\boldsymbol{x}_0\\) at each step \\(t\\): \\[\n\\boldsymbol{x}_t = \\sqrt{\\alpha_t}\\boldsymbol{x}_{t-1} + \\sqrt{1-\\alpha_t} \\boldsymbol{\\epsilon}_t,\n\\] where \\(\\boldsymbol{\\epsilon}_t \\sim \\mathcal{N}(0, \\boldsymbol{I})\\) is a standard Gaussian noise and \\(\\sigma_t\\) is positive.\nThat is, \\(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_{t-1} \\sim \\mathcal{N}(\\sqrt{\\alpha_t}\\boldsymbol{x}_{t-1}, (1-\\alpha_t)\\boldsymbol{I})\\).\nThe choice of the scaling factor \\(\\sqrt{\\alpha_t}\\) is to make sure that the variance magnitude is preserved so that it will not explode and vanish after many iterations1.\nAs \\(t\\) increases, \\(\\boldsymbol{x}_t\\) becomes more and more noisy, and the distribution of \\(\\boldsymbol{x}_t\\) becomes more and more similar to an isotropic Gaussian.\n\nThis choice is proposed by Ho et al. (2020) in Denosing Diffusion Probabilistic Models. It can be shown that this choice is necessary for \\(\\boldsymbol{x}_t\\) to converge to a standard Gaussian, see Theorem 2.1 in Stanley Chan (2024)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#example-1",
    "href": "slides/11-generative_model_2.html#example-1",
    "title": "Generative Models",
    "section": "Example",
    "text": "Example\n\nConsider the Gaussian mixture model: \\[\n\\boldsymbol{x}_0 \\sim \\pi_1 N(\\mu_1, \\sigma_1^2) + \\pi_2 N(\\mu_2, \\sigma_2^2)\n\\] where \\(\\pi_1 = 0.3\\), \\(\\pi_2 = 0.7\\), \\(\\mu_1 = -2\\), \\(\\mu_2 = 2\\), \\(\\sigma_1 = 0.2\\), and \\(\\sigma_2 = 1\\).\nChoose \\(\\alpha_t = 0.97\\) for all \\(t\\). The probability distribution of \\(\\boldsymbol{x}_t\\) for different \\(t\\) is shown below.\n\n\n\n\n\n\n\n\nExample 2.1 in Stanley Chan (2024). Tutorial on Diffusion Models for Imaging and Vision"
  },
  {
    "objectID": "slides/11-generative_model_2.html#conditional-distribution-of-boldsymbolx_t-mid-boldsymbolx_0",
    "href": "slides/11-generative_model_2.html#conditional-distribution-of-boldsymbolx_t-mid-boldsymbolx_0",
    "title": "Generative Models",
    "section": "Conditional distribution of \\(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0\\)",
    "text": "Conditional distribution of \\(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0\\)\n\nThe recursion gives us \\[\n\\begin{aligned}\n\\boldsymbol{x}_t & =\\sqrt{\\alpha_t} \\boldsymbol{x}_{t-1}+\\sqrt{1-\\alpha_t} \\epsilon_{t-1} \\\\\n& =\\sqrt{\\alpha_t}\\left(\\sqrt{\\alpha_{t-1}} \\boldsymbol{x}_{t-2}+\\sqrt{1-\\alpha_{t-1}} \\boldsymbol{\\epsilon}_{t-2}\\right)+\\sqrt{1-\\alpha_t} \\epsilon_{t-1} \\\\\n& =\\sqrt{\\alpha_t \\alpha_{t-1}} \\boldsymbol{x}_{t-2}+\\sqrt{\\alpha_t} \\sqrt{1-\\alpha_{t-1}} \\epsilon_{t-2}+\\sqrt{1-\\alpha_t} \\epsilon_{t-1}\\\\\n& = \\sqrt{\\alpha_t \\alpha_{t-1}} \\boldsymbol{x}_{t-2} + \\sqrt{1-\\alpha_t \\alpha_{t-1}} \\bar{\\epsilon}_{t-2}\n\\end{aligned}\n\\] where \\(\\bar{\\epsilon}_{t-2} \\sim N(0, \\boldsymbol{I})\\) since the sum of two Gaussians is still a Gaussian.\nContinuing this procedure, we have \\(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0 \\sim N(\\sqrt{\\bar{\\alpha}_t}\\boldsymbol{x}_0, (1- \\bar{\\alpha}_t)\\boldsymbol{I})\\), where \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_t\\).\nThis distribution \\(q_\\phi\\left(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0\\right)\\) gives a one-shot forward diffusion step compared to the chain \\(\\boldsymbol{x}_0 \\rightarrow \\boldsymbol{x}_1 \\rightarrow \\ldots \\rightarrow \\boldsymbol{x}_{T-1} \\rightarrow \\boldsymbol{x}_T\\).\nThat is, given \\(\\boldsymbol{x}_0\\), we can obtain the distribution of \\(\\boldsymbol{x}_t\\) directly for any \\(t\\).\nUsually, we can afford a larger update step when the sample gets noisier, so \\(1 &gt; \\alpha_1 &gt; \\alpha_2 &gt; \\cdots &gt; \\alpha_t\\) and therefore \\(\\bar{\\alpha}_1 &gt; \\cdots &gt; \\bar{\\alpha}_t\\)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#reverse-diffusion-process",
    "href": "slides/11-generative_model_2.html#reverse-diffusion-process",
    "title": "Generative Models",
    "section": "Reverse Diffusion Process",
    "text": "Reverse Diffusion Process\n\nIf we can reverse the diffusion process, we can obtain \\(\\boldsymbol{x}_0\\) from \\(\\boldsymbol{x}_t\\).\nIn other words, if we know \\(q(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t)\\), we can get \\(\\boldsymbol{x}_0 \\sim p_{\\text{data}}(\\boldsymbol{x})\\) from \\(\\boldsymbol{x}_t \\mathrel{\\dot\\sim} N(0, \\boldsymbol{I})\\).\nBy Bayes theorem, \\[\nq(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t) = \\frac{q(\\boldsymbol{x}_{t} \\mid \\boldsymbol{x}_{t-1}) p(\\boldsymbol{x}_{t-1})}{p(\\boldsymbol{x}_{t})},\n\\] which can not be easily estimated since we don’t know the marginal \\(p(\\boldsymbol{x}_{t-1})\\).\nSolution: use \\(p_{\\theta}(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t) = N(\\mu_{\\theta}(\\boldsymbol{x}_t), \\Sigma_{\\theta}(\\boldsymbol{x}_t))\\) to estimate \\(q(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t)\\) by minimizing KL divergence (maximizing the ELBO). See Section 2.2~2.4 of Stanley Chan (2024) for the derivation.\nThis model is called the denoising diffusion probabilistic model (DDPM), proposed by Ho et al. (2020)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#training-the-ddpm",
    "href": "slides/11-generative_model_2.html#training-the-ddpm",
    "title": "Generative Models",
    "section": "Training the DDPM",
    "text": "Training the DDPM\n\n\n\n\n\n\n\nImage source: Figure 16 in Stanley Chan (2024)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#inference-of-ddpm",
    "href": "slides/11-generative_model_2.html#inference-of-ddpm",
    "title": "Generative Models",
    "section": "Inference of DDPM",
    "text": "Inference of DDPM\n\n\n\n\n\n\n\nImage source: Figure 17 in Stanley Chan (2024)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#images-generated-by-ddpm",
    "href": "slides/11-generative_model_2.html#images-generated-by-ddpm",
    "title": "Generative Models",
    "section": "Images generated by DDPM",
    "text": "Images generated by DDPM\nThe left panel is generated from the CelebA-HQ dataset, and the right panel is generated from the CIFAR-10 dataset.\n\n\n\n\n\n\n\nImage source: Figure 1 in Ho et al. (2020)."
  },
  {
    "objectID": "slides/11-generative_model_2.html#variants-of-diffusion-models",
    "href": "slides/11-generative_model_2.html#variants-of-diffusion-models",
    "title": "Generative Models",
    "section": "Variants of Diffusion Models",
    "text": "Variants of Diffusion Models\n\nA diffusion model is mainly characterized by\n\nthe forward diffusion process \\(q(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_{t-1})\\) (also called the transition distribution), and\nthe divergence used for approximating the reverse diffusion process.\n\nHence we can construct different diffusion models by choosing different transition distribution and divergence.\nFor example,\n\nSong and Ermon (2019) use score-based divergence to construct generative models,\nSong et al. (2023) use a different transition distribution to construct the denoising diffusion implicit models (DDIM).\n\n\n\n\n\nSong and Ermon (2019). Generative modeling by estimating gradients of the data distribution. In NeurIPS.\nSong et al. (2023). Denoising diffusion implicit models. In ICLR."
  },
  {
    "objectID": "slides/11-generative_model_2.html#summary",
    "href": "slides/11-generative_model_2.html#summary",
    "title": "Generative Models",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nImage source: What are Diffusion Models? by Lilian Weng"
  },
  {
    "objectID": "slides/11-generative_model_2.html#references",
    "href": "slides/11-generative_model_2.html#references",
    "title": "Generative Models",
    "section": "References",
    "text": "References\nGAN\n\nGenerative Adversarial Nets by Goodfellow et al. (2014)\nNIPS 2016 Tutorial on GAN by Goodfellow.\nGenerative adversarial networks: Introduction and outlook by Wang et al. (2017).\nGenerative adversarial networks: An overview by Creswell et al. (2018).\nFrom GAN to WGAN by Lilian Weng (2017)\n\nNormalizing Flows\n\nNormalizing Flows: An Introduction and Review of Current Methods by Kobyzev et al. (2021)\nNormalizing Flows for Probabilistic Modeling and Inference by Papamakarios et al. (2021)\nFlow-based Deep Generative Models by Lilian Weng (2018)\n\nDiffusion Model\n\nTutorial on Diffusion Models for Imaging and Vision by Stanley Chan (2024)\nWhat are Diffusion Models? by Lilian Weng (2021)\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/09-rnn.html#outline",
    "href": "slides/09-rnn.html#outline",
    "title": "Recurrent Neural Networks",
    "section": "Outline",
    "text": "Outline\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nSequential Data and Modeling\n\nAutoregressive Moving Average (ARMA) Models\nState Space Models\n\nDeep Learning for Sequential Data\n\nRecurrent Neural Networks\nLong-Short Term Memory (LSTM)\nGated Recurrent Unit (GRU)\n\nApplications\n\nTime Series Forecasting\nNatural Language Processing"
  },
  {
    "objectID": "slides/09-rnn.html#dependent-samples",
    "href": "slides/09-rnn.html#dependent-samples",
    "title": "Recurrent Neural Networks",
    "section": "Dependent samples",
    "text": "Dependent samples\n\nFor a typical regression problem, we have a set of pairs of input and output data, \\(\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}\\), and we usually assumes that the pairs are independent and identically distributed (i.i.d.).\nTwo of the most common types of dependent data are\n\nsequential data (time series, text, speech, etc.): \\(x^{(1)}, x^{(2)}, \\ldots, x^{(t)}, \\ldots\\), where the order of the data points matters.\nspatial data (data that includes spatial information): samples that are close to each other in space are more likely to be similar than samples that are far apart.\n\nTo apply (deep) neural networks to model these types of data, we need to use models that can capture the dependencies between the data points:\n\nRecurrent Neural Networks (RNNs): for sequential data.\nGraph Neural Networks (GNNs): for spatial data."
  },
  {
    "objectID": "slides/09-rnn.html#sequential-data",
    "href": "slides/09-rnn.html#sequential-data",
    "title": "Recurrent Neural Networks",
    "section": "Sequential Data",
    "text": "Sequential Data\n\nThe simplest case of sequential data is time series data, where the data points are collected at regular time intervals.\nExamples include:\n\nfinancial data (stock prices, exchange rates, coorporate earnings, etc.)\nenvironmental data (temperature, humidity, earthquake, etc.)\nphysiological data (heart rate, blood pressure, etc.)\nspeech data and text data\n\nDenote the time series data as \\(\\{x^{(1)}, x^{(2)}, \\ldots, x^{(t)}, \\ldots\\}\\), where \\(x^{(t)}\\) is the data point at time \\(t\\).\nThe sample at time \\(t\\) can be a scalar, a vector, or a tensor, depending on the application."
  },
  {
    "objectID": "slides/09-rnn.html#statistical-models-for-sequential-data",
    "href": "slides/09-rnn.html#statistical-models-for-sequential-data",
    "title": "Recurrent Neural Networks",
    "section": "Statistical Models for Sequential Data",
    "text": "Statistical Models for Sequential Data\n\nFor time series data, there are several questions we may want to answer:\n\nDescriptive: How are the data points related to each other?\nModeling: What is the underlying process that generates the data? How can we model the data?\nForcasting: Given \\(x^{(1)}, x^{(2)}, \\ldots, x^{(t)}\\), how can we predict \\(x^{(t+1)}\\)?\nClassification: Given \\(\\{x_i^{(1)}, x_i^{(2)}, \\ldots, x_i^{(t)}, y_i\\}_{i=1}^n\\), \\(y_i \\in \\{0, 1\\}\\), how can we predict \\(y_i\\) based on \\(x_i^{(1)}, x_i^{(2)}, \\ldots, x_i^{(t)}\\)?\n\nCommonly used statistical models for time series data include:\n\nAutoregressive Moving Average (ARMA) models\nState Space models"
  },
  {
    "objectID": "slides/09-rnn.html#autoregressive-models",
    "href": "slides/09-rnn.html#autoregressive-models",
    "title": "Recurrent Neural Networks",
    "section": "Autoregressive Models",
    "text": "Autoregressive Models\n\nFor univariate time series data, the simplest model for time series data is the autoregressive (AR) model.\nThe AR model of order \\(p\\) is defined as \\[\nx^{(t)} = \\phi_1 x^{(t-1)} + \\phi_2 x^{(t-2)} + \\ldots + \\phi_p x^{(t-p)} + \\epsilon^{(t)}\n\\] where \\(\\phi_1, \\phi_2, \\ldots, \\phi_p\\) are the parameters of the model, and \\(\\epsilon^{(t)} \\sim N(0, \\sigma^2)\\) is the noise term.\nFor vector-valued time series data, we can use the vector autoregressive (VAR) model, which is defined as \\[\n\\boldsymbol{x}^{(t)} = \\Phi_1 \\boldsymbol{x}^{(t-1)} + \\Phi_2 \\boldsymbol{x}^{(t-2)} + \\ldots + \\Phi_p \\boldsymbol{x}^{(t-p)} + \\boldsymbol{\\epsilon}^{(t)}\n\\] where \\(\\Phi_1, \\Phi_2, \\ldots, \\Phi_p\\) are matrices, and \\(\\boldsymbol{\\epsilon}^{(t)} \\sim N_d(\\boldsymbol{0}, \\Sigma)\\) is the noise term.\nThe model parameters can be estimated by MLE or other methods, e.g., the Yule-Walker equations. See Ch. 3 of Shumway and Stoffer (2017)1 for more details.\n\nShumway, R. H., & Stoffer, D. S. (2017). Time series analysis and its applications: With R examples. Springer."
  },
  {
    "objectID": "slides/09-rnn.html#remarks",
    "href": "slides/09-rnn.html#remarks",
    "title": "Recurrent Neural Networks",
    "section": "Remarks",
    "text": "Remarks\n\nThe AR/VAR models assume linear relationships between the data at different time points.\nThe time lag parameter \\(p\\) can be determined by\n\nprior data analysis (e.g., autocorrelation function),\ninformation criteria (AIC, BIC), or\ncross-validation.\n\nLarge \\(p\\) leads to a more complicated model, which is harder to interpret and may lead to overfitting.\nThe linear assumption may not be sufficent for complex time series data."
  },
  {
    "objectID": "slides/09-rnn.html#state-space-model",
    "href": "slides/09-rnn.html#state-space-model",
    "title": "Recurrent Neural Networks",
    "section": "State Space Model",
    "text": "State Space Model\n\nState space models are a more general class of probabilistic models for time series data.\nThe state space model can be written as \\[\\begin{align*}\n\\textbf{State}: & \\quad  h^{(t)} = \\Phi h^{(t-1)} + \\epsilon^{(t)} \\\\\n\\textbf{Observation}: & \\quad y^{(t)} = \\Psi h^{(t)} + \\eta^{(t)}\n\\end{align*}\\] where\n\n\\(h^{(t)} \\in \\mathbb{R}^p\\) is the latent state at time \\(t\\) (unknown),\n\\(y^{(t)} \\in \\mathbb{R}^q\\) is the observed data at time \\(t\\) (observed),\n\\(\\Phi \\in \\mathbb{R}^{p\\times p}\\) is the state transition matrix (unknown),\n\\(\\Psi \\in \\mathbb{R}^{q \\times p}\\) is the observation matrix (unknown),\n\\(\\epsilon^{(t)}\\) and \\(\\eta^{(t)}\\) are the state noise and observation noise, respectively.\n\nThe main difference between this model and the AR model is that the state space model includes the measurement noice \\(\\eta^{(t)}\\).\nIf the observation noise \\(\\eta^{(t)}\\) is zero, the state space model reduces to an autoregressive model."
  },
  {
    "objectID": "slides/09-rnn.html#exogenous-variables",
    "href": "slides/09-rnn.html#exogenous-variables",
    "title": "Recurrent Neural Networks",
    "section": "Exogenous Variables",
    "text": "Exogenous Variables\n\nIn some cases, we may have exogenous variables entering the states and the observations.\nThat is, given the (observed) exogenous variables \\(x^{(t)} \\in \\mathbb{R}^r\\), the state space model can be written as \\[\\begin{align*}\n\\textbf{State}: & \\quad  h^{(t)} = \\Phi h^{(t-1)} + \\Gamma x^{(t)} + \\epsilon^{(t)} \\\\\n\\textbf{Observation}: & \\quad y^{(t)} = \\Psi h^{(t)} + \\Xi x^{(t)} + \\eta^{(t)}\n\\end{align*}\\] where \\(\\Gamma \\in \\mathbb{R}^{p \\times r}\\) and \\(\\Xi \\in \\mathbb{R}^{q \\times r}\\) are matrices."
  },
  {
    "objectID": "slides/09-rnn.html#motivation",
    "href": "slides/09-rnn.html#motivation",
    "title": "Recurrent Neural Networks",
    "section": "Motivation",
    "text": "Motivation\n\nThe state space models were first proposed for tracking problems in engineering by Rudolf E. Kálmán in the 1960s.\nFor example,\n\nthe latent state \\(h^{(t)}\\) is the actual position and velocity of an object,\nthe observed data \\(y^{(t)}\\) can be the noisy measurements (e.g., by the GPS) of the object’s position and velocity.\n\nUsually we assume Gaussian noises \\(\\epsilon^{(t)} \\sim N_p(0, \\Sigma)\\) and \\(\\eta^{(t)} \\sim N_q(0, \\Lambda)\\).\nIn this case, we might be interested in\n\nestimating the actual position \\(h^{(t)}\\) of the object at each time step given \\(y^{(t)}\\),\nestimating the observation noise \\(\\Lambda\\) to know how far the observed position can be from the actual position.\n\nWe will now briefly introduce one special case of state space models: the Kalman filter."
  },
  {
    "objectID": "slides/09-rnn.html#kalman-filter",
    "href": "slides/09-rnn.html#kalman-filter",
    "title": "Recurrent Neural Networks",
    "section": "Kalman Filter",
    "text": "Kalman Filter\n\nConsider the model: \\[\\begin{align*}\nh^{(t)} & = \\Phi h^{(t-1)} + \\Gamma x^{(t)} + \\epsilon^{(t)}, \\qquad \\epsilon^{(t)} \\sim N_p(0, \\Sigma_h) \\\\\ny^{(t)} & = \\Psi h^{(t)} + \\Xi x^{(t)} + \\eta^{(t)}, \\qquad \\eta^{(t)} \\sim N_q(0, \\Sigma_{\\text{obs}})\n\\end{align*}\\]\nThe goal is to estimate the hidden state \\(h^{(t)}\\) given the observed data \\(y^{(1:s)} = \\{y^{(1)}, y^{(2)}, \\ldots, y^{(s)}\\}\\) and the exogenous variables \\(x^{(1:s)} = \\{x^{(1)}, x^{(2)}, \\ldots, x^{(s)}\\}\\), where \\(s &lt; t\\).\nDenote \\[\\begin{align*}\n\\textbf{Estimated State}: & \\quad \\hat{h}_s^{(t)} = \\mathbb{E}\\left(h^{(t)} \\mid y^{(1:s)}, x^{(1:s)}\\right)\\\\\n\\textbf{Esitmated Covariance}: & \\quad P_s^{(t_1,t_2)} = \\mathbb{E}\\left[(h^{(t_1)} - \\hat{h}_s^{(t_1)})(h^{(t_2)} - \\hat{h}_s^{(t_2)})^T \\mid y^{(1:s)}, x^{(1:s)}\\right]\n\\end{align*}\\]\nFor simplicity, we write \\(P_s^{(t,t)} = P_s^{(t)}\\).\nUnder the Gaussian assumptions for \\(\\epsilon^{(t)}\\) and \\(\\eta^{(t)}\\), the conditional distribution of \\(h^{(t)}\\) given \\(y^{(1:t-1)}\\) and \\(x^{(1:t-1)}\\) is also Gaussian.\nWe can derive the Kalman filter algorithm to compute \\(\\hat{h}_{t-1}^{(t)}\\) and \\(P_{t-1}^{(t)}\\)."
  },
  {
    "objectID": "slides/09-rnn.html#remarks-1",
    "href": "slides/09-rnn.html#remarks-1",
    "title": "Recurrent Neural Networks",
    "section": "Remarks",
    "text": "Remarks\n\nThe Kalman filter is a linear filter since all the prediction and updates are linear.\nThe estimation of the model parameters \\(\\Phi, \\Gamma, \\Psi, \\Xi, \\Sigma_h, \\Sigma_{\\text{obs}}\\) can be done by maximum likelihood estimation.\nThe likelihood function is quite complicated and one can use the Expectation-Maximization (EM) algorithm to estimate the parameters.\nThe Kalman filter can also serve as a smoothing algorithm: the hidden states are smooth versions of the observed data since the observation noise is removed.\nWe can also use time-varying parameters \\(\\Phi_t, \\Gamma_t, \\Psi_t, \\Xi_t\\) to model the dynamics of the system."
  },
  {
    "objectID": "slides/09-rnn.html#recurrent-neural-networks",
    "href": "slides/09-rnn.html#recurrent-neural-networks",
    "title": "Recurrent Neural Networks",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\n\nRecurrent neural networks, or RNNs, are a family of neural networks for processing sequential data.\nThe main difference between RNNs and feedforward neural networks is that RNNs have connections between the neurons in the same layer, forming a directed cycle.\nThese connection allow the model to capture the dependencies between the data points in the sequence."
  },
  {
    "objectID": "slides/09-rnn.html#a-simple-rnn",
    "href": "slides/09-rnn.html#a-simple-rnn",
    "title": "Recurrent Neural Networks",
    "section": "A simple RNN",
    "text": "A simple RNN\n\n\n\nThe simplest RNN can be constructed as follows: \\[\\begin{align*}\n\\boldsymbol{h}^{(t)} & = \\sigma(\\boldsymbol{W} \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U} \\boldsymbol{x}^{(t)} + \\boldsymbol{b})\\\\\n\\boldsymbol{y}^{(t)} & = \\boldsymbol{V} \\boldsymbol{h}^{(t)} + \\boldsymbol{c}\n\\end{align*}\\] where \\(\\boldsymbol{h}^{(t)}\\) is the hidden state at time \\(t\\), \\(\\boldsymbol{x}^{(t)}\\) is the input, and \\(\\boldsymbol{y}^{(t)}\\) is the output.\nThe parameters are \\(\\boldsymbol{W}, \\boldsymbol{U}, \\boldsymbol{V}, \\boldsymbol{b}, \\boldsymbol{c}\\), and \\(\\sigma\\) is the activation function.\nIn other words, an RNN is a nonlinear state space model."
  },
  {
    "objectID": "slides/09-rnn.html#unfolding-the-graph",
    "href": "slides/09-rnn.html#unfolding-the-graph",
    "title": "Recurrent Neural Networks",
    "section": "Unfolding the graph",
    "text": "Unfolding the graph"
  },
  {
    "objectID": "slides/09-rnn.html#computing-the-gradient",
    "href": "slides/09-rnn.html#computing-the-gradient",
    "title": "Recurrent Neural Networks",
    "section": "Computing the Gradient",
    "text": "Computing the Gradient\n\nComputing the gradient for the RNN is through backpropogation similar to the feedforward neural network.\nHowever, for RNNs, we need to also back-propagate through time (BPTT) to compute the gradient of the loss function with respect to the parameter \\(\\boldsymbol{W}\\).\nHence if we have a long sequence even with only one hidden layer, we still face the vanishing/exlpoding gradient problem.\nThis problem prevents the RNN from capturing long-term dependencies in the data.\nIn practice, gradient-based algorithms (e.g., SGD) are not able to train RNNs with sequences longer than 10-20.\nStrategies:\n\nGradient clipping\nAdding skip connections through time\nUsing gates to control the information flow in the network."
  },
  {
    "objectID": "slides/09-rnn.html#gated-rnns",
    "href": "slides/09-rnn.html#gated-rnns",
    "title": "Recurrent Neural Networks",
    "section": "Gated RNNs",
    "text": "Gated RNNs\n\nThe main idea of gated RNNs is to use gates to control the flow of information in the network, i.e., to decide whether we need to keep the information or forget it.\nA gate is a number between 0 and 1. We multiply the gate by the input to get the output: \\[\n\\text{output} = \\text{gate} \\times \\text{input}\n\\]\nIf the gate is 0, we completely forget the input; if the gate is 1, we keep the input as it is.\nThe gate is also parametrized as a neural network, and the parameters are learned during training.\nThe most famous gated RNNs are Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU)."
  },
  {
    "objectID": "slides/09-rnn.html#long-short-term-memory-lstm",
    "href": "slides/09-rnn.html#long-short-term-memory-lstm",
    "title": "Recurrent Neural Networks",
    "section": "Long-Short Term Memory (LSTM)",
    "text": "Long-Short Term Memory (LSTM)\n\nStates:\n\nCell state: the memory of the network.\nHidden state: the output of the network.\n\nThree gates:\n\nForget gate: decide what information to forget from the cell state.\nInput gate: decide what information to store in the cell state.\nOutput gate: decide what information to output from the cell state.\n\nAt time \\(t\\), we have\n\nthe cell state \\(\\boldsymbol{c}^{(t-1)}\\),\nthe hidden state \\(\\boldsymbol{h}^{(t-1)}\\), and\nthe input \\(\\boldsymbol{x}^{(t)}\\)."
  },
  {
    "objectID": "slides/09-rnn.html#gates",
    "href": "slides/09-rnn.html#gates",
    "title": "Recurrent Neural Networks",
    "section": "Gates",
    "text": "Gates\n\nThe forget gate at time \\(t\\) is defined as \\[\n\\boldsymbol{F}^{(t)} = \\sigma(\\boldsymbol{W}_F \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U}_F \\boldsymbol{x}^{(t)} + \\boldsymbol{b}_F).\n\\]\nThe input gate at time \\(t\\) is defined as \\[\n\\boldsymbol{I}^{(t)} = \\sigma(\\boldsymbol{W}_I \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U}_I \\boldsymbol{x}^{(t)} + \\boldsymbol{b}_I).\n\\]\nThe output gate at time \\(t\\) is defined as \\[\n\\boldsymbol{O}^{(t)} = \\sigma(\\boldsymbol{W}_O \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U}_O \\boldsymbol{x}^{(t)} + \\boldsymbol{b}_O).\n\\]\nThe gate parameters \\(\\boldsymbol{W}_F, \\boldsymbol{U}_F, \\boldsymbol{b}_F, \\boldsymbol{W}_I, \\boldsymbol{U}_I, \\boldsymbol{b}_I, \\boldsymbol{W}_O, \\boldsymbol{U}_O, \\boldsymbol{b}_O\\) are learned during training.\nThe activation function \\(\\sigma\\) is usually the sigmoid function to keep the gate values in \\([0,1]\\)."
  },
  {
    "objectID": "slides/09-rnn.html#updating-the-states",
    "href": "slides/09-rnn.html#updating-the-states",
    "title": "Recurrent Neural Networks",
    "section": "Updating the States",
    "text": "Updating the States\nWith the gates, we can update the cell state and the hidden state as follows.\n\nCompute the candidate cell state: \\[\n\\tilde{\\boldsymbol{c}}^{(t)} = \\tanh(\\boldsymbol{W}_C \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U}_C \\boldsymbol{x}^{(t)} + \\boldsymbol{b}_C).\n\\] This encodes the new information that we want to store in the cell state. The parameters \\(\\boldsymbol{W}_C, \\boldsymbol{U}_C, \\boldsymbol{b}_C\\) are also learned during training.\nUpdate the cell state: \\[\n\\boldsymbol{c}^{(t)} = \\boldsymbol{F}^{(t)} \\odot \\boldsymbol{c}^{(t-1)} + \\boldsymbol{I}^{(t)} \\odot \\tilde{\\boldsymbol{c}}^{(t)}.\n\\] This step forgets some information from the previous cell state and adds new information to the current cell state.\nUpdate the hidden state: \\[\n\\boldsymbol{h}^{(t)} = \\boldsymbol{O}^{(t)} \\odot \\tanh(\\boldsymbol{c}^{(t)}).\n\\] The hidden state at time \\(t\\) will be further used to compute the output and hence we use the output gate to control the information flow."
  },
  {
    "objectID": "slides/09-rnn.html#lstm-module",
    "href": "slides/09-rnn.html#lstm-module",
    "title": "Recurrent Neural Networks",
    "section": "LSTM Module",
    "text": "LSTM Module"
  },
  {
    "objectID": "slides/09-rnn.html#parameters-of-the-lstm-module",
    "href": "slides/09-rnn.html#parameters-of-the-lstm-module",
    "title": "Recurrent Neural Networks",
    "section": "Parameters of the LSTM module",
    "text": "Parameters of the LSTM module\n\nThe LSTM module has the following parameters:\n\n\\(\\boldsymbol{W}_F, \\boldsymbol{U}_F, \\boldsymbol{b}_F\\): forget gate parameters.\n\\(\\boldsymbol{W}_I, \\boldsymbol{U}_I, \\boldsymbol{b}_I\\): input gate parameters.\n\\(\\boldsymbol{W}_O, \\boldsymbol{U}_O, \\boldsymbol{b}_O\\): output gate parameters.\n\\(\\boldsymbol{W}_C, \\boldsymbol{U}_C, \\boldsymbol{b}_C\\): candidate cell state parameters.\n\nAssume the dimension of the input \\(\\boldsymbol{x}^{(t)}\\) is \\(p\\) and the dimension of the hidden state \\(\\boldsymbol{h}^{(t)}\\) is \\(q\\). Then the dimensions of the parameters are\n\n\\(\\boldsymbol{W}_F, \\boldsymbol{W}_I, \\boldsymbol{W}_O, \\boldsymbol{W}_C \\in \\mathbb{R}^{q \\times q}\\),\n\\(\\boldsymbol{U}_F, \\boldsymbol{U}_I, \\boldsymbol{U}_O, \\boldsymbol{U}_C \\in \\mathbb{R}^{q \\times p}\\),\n\\(\\boldsymbol{b}_F, \\boldsymbol{b}_I, \\boldsymbol{b}_O, \\boldsymbol{b}_C \\in \\mathbb{R}^q\\).\n\nHence the total number of parameters in the LSTM module is \\(4q^2 + 4pq + 4q = 4q(p+q+1)\\)."
  },
  {
    "objectID": "slides/09-rnn.html#issues-with-lstm",
    "href": "slides/09-rnn.html#issues-with-lstm",
    "title": "Recurrent Neural Networks",
    "section": "Issues with LSTM",
    "text": "Issues with LSTM\nThe inclusion of the three gates and the memory cell has some pros and cons:\n\nPros:\n\nHandling long sequences: LSTMs are specifically designed to remember information for long periods.\nMitigating vanishing gradient problem: LSTMs address the vanishing gradient problem commonly encountered in traditional RNNs.\n\nCons:\n\nComplexity and computational cost: LSTMs are more complex than standard RNNs due to large number of parameters.\nOverfitting: LSTMs can be prone to overfitting.\n\nAre all the gates necessary? Can we simplify the LSTM module?"
  },
  {
    "objectID": "slides/09-rnn.html#gated-recurrent-unit-gru",
    "href": "slides/09-rnn.html#gated-recurrent-unit-gru",
    "title": "Recurrent Neural Networks",
    "section": "Gated Recurrent Unit (GRU)",
    "text": "Gated Recurrent Unit (GRU)\n\nIn contrast to LSTM, the GRU has only two gates: the update gate and the reset gate, which are defined as follows: \\[\\begin{align*}\n\\boldsymbol{Z}^{(t)} & = \\sigma(\\boldsymbol{W}_Z \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U}_Z \\boldsymbol{x}^{(t)} + \\boldsymbol{b}_Z)\\\\\n\\boldsymbol{R}^{(t)} & = \\sigma(\\boldsymbol{W}_R \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U}_R \\boldsymbol{x}^{(t)} + \\boldsymbol{b}_R).\n\\end{align*}\\]\nThe hidden state at time \\(t\\) is updated as follows:\n\nCompute the candidate hidden state: \\[\n\\tilde{\\boldsymbol{h}}^{(t)} = \\tanh(\\boldsymbol{W}_H \\left(\\boldsymbol{R}^{(t)} \\odot \\boldsymbol{h}^{(t-1)}\\right) + \\boldsymbol{U}_H \\boldsymbol{x}^{(t)} + \\boldsymbol{b}_H).\n\\] This step uses the reset gate to control the information flow from the previous hidden state.\nUpdate the hidden state: \\[\n\\boldsymbol{h}^{(t)} = \\boldsymbol{Z}^{(t)} \\odot \\boldsymbol{h}^{(t-1)} + (1-\\boldsymbol{Z}^{(t)}) \\odot \\tilde{\\boldsymbol{h}}^{(t)}.\n\\] This step uses the update gate to control the information flow from the previous hidden state and the candidate hidden state."
  },
  {
    "objectID": "slides/09-rnn.html#quick-summary",
    "href": "slides/09-rnn.html#quick-summary",
    "title": "Recurrent Neural Networks",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nThe main difference between the Kalman filter, RNNs, and LSTM/GRU is the way they model the hidden states:\n\nKalman filter: \\(\\boldsymbol{h}^{(t)} = \\boldsymbol{W} \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U} \\boldsymbol{x}^{(t)} + \\boldsymbol{b}\\).\nRNN: \\(\\boldsymbol{h}^{(t)} = \\sigma(\\boldsymbol{W} \\boldsymbol{h}^{(t-1)} + \\boldsymbol{U} \\boldsymbol{x}^{(t)} + \\boldsymbol{b})\\).\nLSTM: \\(\\boldsymbol{h}^{(t)} = \\text{LSTM}( \\boldsymbol{h}^{(t-1)}, \\boldsymbol{x}^{(t)}, \\text{parameters})\\).\nGRU: \\(\\boldsymbol{h}^{(t)} = \\text{GRU}( \\boldsymbol{h}^{(t-1)}, \\boldsymbol{x}^{(t)}, \\text{parameters})\\).\n\nThe output depends on the prediction target:\n\nReal-valued output: \\(\\boldsymbol{y}^{(t)} = \\boldsymbol{V} \\boldsymbol{h}^{(t)} + \\boldsymbol{c}\\).\nBinary output: \\(\\boldsymbol{y}^{(t)} = \\text{sigmoid}(\\boldsymbol{V} \\boldsymbol{h}^{(t)} + \\boldsymbol{c})\\).\nMulticlass output: \\(\\boldsymbol{y}^{(t)} = \\text{softmax}(\\boldsymbol{V} \\boldsymbol{h}^{(t)} + \\boldsymbol{c})\\)."
  },
  {
    "objectID": "slides/09-rnn.html#deep-rnn",
    "href": "slides/09-rnn.html#deep-rnn",
    "title": "Recurrent Neural Networks",
    "section": "Deep RNN",
    "text": "Deep RNN\nWe can also increase the depth of the RNN by stacking multiple hidden layers on top of each other:"
  },
  {
    "objectID": "slides/09-rnn.html#applications-of-rnns",
    "href": "slides/09-rnn.html#applications-of-rnns",
    "title": "Recurrent Neural Networks",
    "section": "Applications of RNNs",
    "text": "Applications of RNNs\n\nRNNs are designed for sequential data:\n\nTime series forecasting: predict the future values of a time series.\nBiological signal processing: fMRI, EEG, ECG, etc.\n\nIn particular, they are widely used in Natural Language Processing (NLP):\n\nSentiment analysis: classify the sentiment of a text as positive, negative, or neutral.\nText generation: generate text based on a given prompt.\nMachine translation: translate text from one language to another.\nSpeech recognition: convert spoken language into text."
  },
  {
    "objectID": "slides/09-rnn.html#word-embedding",
    "href": "slides/09-rnn.html#word-embedding",
    "title": "Recurrent Neural Networks",
    "section": "Word Embedding",
    "text": "Word Embedding\n\nTo process text data, we need to convert the text into numerical data. This step is called word embedding.\nThe simplest way to do word embedding is to use the one-hot encoding:\n\nEach word is represented as a vector of zeros with a single 1 at the index of the word in the vocabulary.\nThe dimension of the one-hot vector is the size of the vocabulary.\n\nHowever, one-hot encoding has some drawbacks:\n\nThe vectors are sparse and high-dimensional.\nThe vectors do not capture the semantic relationships between the words.\n\nA more practical approach is to ues word embedding models, e.g.,\n\nWord2Vec\nGloVe (Global Vectors for Word Representation)\nBERT (Bidirectional Encoder Representations from Transformers)\n\nThese models learn dense, low-dimensional vectors that capture the semantic relationships between the words."
  },
  {
    "objectID": "slides/09-rnn.html#variants-of-rnns",
    "href": "slides/09-rnn.html#variants-of-rnns",
    "title": "Recurrent Neural Networks",
    "section": "Variants of RNNs",
    "text": "Variants of RNNs\nDepending on the application/task, there are several variants of RNNs:\n\nMany-to-one: for sentiment analysis, text classification, etc.\nOne-to-many: for music generation, image captioning, etc.\nMany-to-many (sequence-to-sequence): for machine translation, speech recognition, etc."
  },
  {
    "objectID": "slides/09-rnn.html#image-captioning",
    "href": "slides/09-rnn.html#image-captioning",
    "title": "Recurrent Neural Networks",
    "section": "Image Captioning",
    "text": "Image Captioning\n\n\n\n\n\n\n\nImage source: Figure 8.9 of Aggarwal (2023)."
  },
  {
    "objectID": "slides/09-rnn.html#machine-translation",
    "href": "slides/09-rnn.html#machine-translation",
    "title": "Recurrent Neural Networks",
    "section": "Machine Translation",
    "text": "Machine Translation\n\n\n\n\n\n\n\nImage source: Figure 8.10 of Aggarwal (2023)."
  },
  {
    "objectID": "slides/09-rnn.html#sentence-classification",
    "href": "slides/09-rnn.html#sentence-classification",
    "title": "Recurrent Neural Networks",
    "section": "Sentence Classification",
    "text": "Sentence Classification\n\n\n\n\n\n\n\nImage source: Figure 8.11 of Aggarwal (2023)."
  },
  {
    "objectID": "slides/09-rnn.html#token-level-classification",
    "href": "slides/09-rnn.html#token-level-classification",
    "title": "Recurrent Neural Networks",
    "section": "Token-level Classification",
    "text": "Token-level Classification\n\n\n\n\n\n\n\nImage source: Figure 8.12 of Aggarwal (2023)."
  },
  {
    "objectID": "slides/09-rnn.html#temporal-recommender-systems",
    "href": "slides/09-rnn.html#temporal-recommender-systems",
    "title": "Recurrent Neural Networks",
    "section": "Temporal Recommender Systems",
    "text": "Temporal Recommender Systems\n\n\n\n\n\n\n\nImage source: Figure 8.13 of Aggarwal (2023)."
  },
  {
    "objectID": "slides/09-rnn.html#more-examples",
    "href": "slides/09-rnn.html#more-examples",
    "title": "Recurrent Neural Networks",
    "section": "More Examples",
    "text": "More Examples\nThe official PyTorch documentation and d2l.ai provide several examples of RNNs and LSTMs:\n\nLSTM: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\nNLP from scratch: https://pytorch.org/tutorials/intermediate/nlp_from_scratch_index.html\nBidirectional Recurrent Neural Networks: https://d2l.ai/chapter_recurrent-modern/bi-rnn.html\nMachine translation example: https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/05-optimization.html#recap-of-the-last-lecture",
    "href": "slides/05-optimization.html#recap-of-the-last-lecture",
    "title": "Optimization in DL",
    "section": "Recap of the Last Lecture",
    "text": "Recap of the Last Lecture\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nAn \\(L\\)-layer FCNN (the \\(L\\)th layer is the output layer) can be written recursively as \\[\nf^{(L)}(\\boldsymbol{x}) = \\boldsymbol{W}^{(L)}\\boldsymbol{h}^{(L-1)} +  \\boldsymbol{b}^{(L)} \\in \\mathbb{R}^k,\n\\] where \\[\n\\boldsymbol{h}^{(l)} = \\sigma\\left(\\boldsymbol{W}^{(l)} \\boldsymbol{h}^{(l-1)} + \\boldsymbol{b}^{(l)}\\right), \\quad l = 1, \\ldots, L-1,\n\\] and \\(\\boldsymbol{h}^{(0)} = \\boldsymbol{x} \\in \\mathbb{R}^p\\).\nWe use a link function to connect the predictor \\(f^{(L)}(\\boldsymbol{x})\\) to the conditional expectation \\(\\mathbb{E}(Y \\mid \\boldsymbol{x})\\).\n\nThe parameters of the model can be learning by minimizing the empirical risk using the gradient descent algorithm.\nThe gradient of the loss function with respect to the parameters can be computed using the back-propagation.\nIn practice, all the gradients computations are done automatically, for example, using torch.autograd."
  },
  {
    "objectID": "slides/05-optimization.html#deep-neural-networks",
    "href": "slides/05-optimization.html#deep-neural-networks",
    "title": "Optimization in DL",
    "section": "Deep Neural Networks",
    "text": "Deep Neural Networks\n\nNeural network is a very flexible and powerful class of models; there are many ways to design them:\n\nDepth (number of hidden layers)\nWidth (number of hidden units)\nActivation functions\nOptimization (learning rate and learning schedule)\nRegularization\n\nIt is impossible for us to know the best architecture for a given problem in advance. Typically, we monitor the learning process and adjust the architectures accordingly.\nNumerous experiments have shown that deep but narrow networks are more efficient than shallow but wide networks.\nHowever, training deep networks is challenging due to various reasons and we will discuss some strategies to address these challenges."
  },
  {
    "objectID": "slides/05-optimization.html#outline",
    "href": "slides/05-optimization.html#outline",
    "title": "Optimization in DL",
    "section": "Outline",
    "text": "Outline\n\nDeep Neural Networks\n\nDepth v.s. Width\nChallenges of training deep networks\nDepth-Friendly Architectures\n\n\nFirst-order Optimization Methods\n\nStochastic Gradient Descent\nLearning Rate Schedules\nMomentum\nAdaptive Learning Rates"
  },
  {
    "objectID": "slides/05-optimization.html#deep-vs-shallow-networks",
    "href": "slides/05-optimization.html#deep-vs-shallow-networks",
    "title": "Optimization in DL",
    "section": "Deep vs Shallow Networks",
    "text": "Deep vs Shallow Networks\n\nUsing the same number of nodes (parameters), deep networks can represent more complex functions than shallow networks:\n\nShallow network (one hidden layer): linear combination of simple nonlinear functions\nDeep networks (multiple hidden layers): functional composition of simple nonlinear functions\n\nEarlier layers capture primitive features, and later layers capture more complex features."
  },
  {
    "objectID": "slides/05-optimization.html#challenges-of-training-deep-networks",
    "href": "slides/05-optimization.html#challenges-of-training-deep-networks",
    "title": "Optimization in DL",
    "section": "Challenges of Training Deep Networks",
    "text": "Challenges of Training Deep Networks\n\nDeeper networks are often harder to train, and are also unstable to parameter initialization or hyperparameter choices.\nThe main reason is that the loss function is high dimensional and highly non-convex, due to the recursive composition of nonlinear functions.\nThe loss function loos like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage source: Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). Visualizing the loss landscape of neural nets. NeurIPS."
  },
  {
    "objectID": "slides/05-optimization.html#challenges-of-training-deep-networks-1",
    "href": "slides/05-optimization.html#challenges-of-training-deep-networks-1",
    "title": "Optimization in DL",
    "section": "Challenges of Training Deep Networks",
    "text": "Challenges of Training Deep Networks\n\nThe challenges of training deep networks include:\n\nComplicated loss landscape (local minima, saddle points, flat regions, cliffs, etc)\nVanishing and exploding gradients\nConvergence (the convergence rate slows down exponentially with depth)\n\nNext, we will discuss some special architectures and optimization strategies to address these challenges."
  },
  {
    "objectID": "slides/05-optimization.html#local-minima",
    "href": "slides/05-optimization.html#local-minima",
    "title": "Optimization in DL",
    "section": "Local Minima",
    "text": "Local Minima\n\nIn convex optimization problem, it can be reduced to the problem of finding a local minimum, which is guaranteed to be the global minimum.\nWith nonconvex functions, such as neural nets, it is possible to have many local minima, which is due to the model identifiability problem.\nFor example,\n\nweight space symmetry (permutation symmetry): we can reorder the neurons in any hidden layer, along with the corresponding permutation of the weights and biases associated with them\nscaling symmetry: in any ReLU network, we can scale all the incoming weights and biases of a unit by \\(a\\) and all its outgoing weights by \\(1/a\\).\n\nThe parameters in neural networks are not interpretable and so is any unidentifiable models.\nThus it is not important to find a true global minimum rather than to find a point in parameter space that has low but not minimal cost, i.e., good local minima."
  },
  {
    "objectID": "slides/05-optimization.html#saddle-points-and-flat-regions",
    "href": "slides/05-optimization.html#saddle-points-and-flat-regions",
    "title": "Optimization in DL",
    "section": "Saddle Points and Flat Regions",
    "text": "Saddle Points and Flat Regions\n\nThese regions are where the gradient is close to zero, but not local minima:\n\nSaddle points: points where the gradient is zero but the Hessian has both positive and negative eigenvalues.\nFlat regions: regions where the gradient is close to zero but the Hessian has all eigenvalues near zero.\n\nGradient-based optimization algorithms can get stuck in these regions.\n\n\n\n\n\n\n\n\nImage source: Figure 4.13 of Aggarwal (2023) Neural Networks and Deep Learning."
  },
  {
    "objectID": "slides/05-optimization.html#vanishing-gradients",
    "href": "slides/05-optimization.html#vanishing-gradients",
    "title": "Optimization in DL",
    "section": "Vanishing Gradients",
    "text": "Vanishing Gradients\n\nThe vanishing gradient problem is a common issue in training deep networks: the gradient in the earlier layers is close to zero.\nThis problem would lead to slow convergence or even the model cannot be trained.\nConsider a deep network with \\(L\\) layers, each layer with 1 node and no bias term.\nDenote the weights of the \\(l\\)-th layer as \\(w_l\\) and \\(h_l = \\sigma(w_l h_{l-1})\\).\nThe gradient of the loss \\(\\ell\\) with respect to \\(w_l\\) is \\[\n\\frac{\\partial \\ell}{\\partial w_l} = \\frac{\\partial \\ell}{\\partial h_L} \\cdot \\frac{\\partial h_L}{\\partial h_{L-1}} \\cdot \\frac{\\partial h_{L-1}}{\\partial h_{L-2}} \\cdots \\frac{\\partial h_{l+1}}{\\partial h_{l}} \\cdot \\frac{\\partial h_l}{\\partial w_l}.\n\\]\nNote that \\(\\frac{\\partial h_l}{\\partial h_{l-1}} = \\sigma^{\\prime}(w_l h_{l-1})w_{l}\\) and hence \\[\\begin{align*}\n\\frac{\\partial \\ell}{\\partial w_l} & = \\frac{\\partial \\ell}{\\partial h_L} \\cdot \\frac{\\partial h_L}{\\partial h_{L-1}}  \\cdots \\frac{\\partial h_{l+1}}{\\partial h_{l}} \\cdot \\frac{\\partial h_l}{\\partial w_l}\n= \\frac{\\partial \\ell}{\\partial h_L} \\left(\\prod_{i=l+1}^L w_i \\right)\\left(\\prod_{i=l+1}^L \\sigma^{\\prime}(w_ih_{i-1})\\right)\\frac{\\partial h_l}{\\partial w_l}.\n\\end{align*}\\]\nWhen \\(L\\) is large and \\(\\sigma^{\\prime}(x) &lt; 1\\), the gradient \\(\\frac{\\partial \\ell}{\\partial w_l}\\) is close to zero."
  },
  {
    "objectID": "slides/05-optimization.html#cliffs-and-exploding-gradients",
    "href": "slides/05-optimization.html#cliffs-and-exploding-gradients",
    "title": "Optimization in DL",
    "section": "Cliffs and Exploding Gradients",
    "text": "Cliffs and Exploding Gradients\n\nCliffs are regions where the gradient is very steep, which can cause the optimization algorithm to diverge.\nThis issue can be avoided by the gradient clipping method:\n\nValue Clipping: Each component of the gradient vector is individually clipped to lie within a predefined range, such as [-threshold, threshold].\nNorm Clipping: The entire gradient vector is scaled down if its norm (such as the L2 norm) exceeds the threshold, preserving its direction but reducing its magnitude.\n\n\n\n\n\n\n\n\n\nImage source: Figure 8.3 of DL."
  },
  {
    "objectID": "slides/05-optimization.html#activation-function-choice",
    "href": "slides/05-optimization.html#activation-function-choice",
    "title": "Optimization in DL",
    "section": "Activation Function Choice",
    "text": "Activation Function Choice\nThe specific choice of activation function often has a considerable eﬀect on thes everity of the vanishing gradient problem. The following are the derivatives of some common activation functions:\n\n\n\n\n\n\n\nImage source: Figure 4.8 of Aggarwal (2023) Neural Networks and Deep Learning."
  },
  {
    "objectID": "slides/05-optimization.html#dead-neurons",
    "href": "slides/05-optimization.html#dead-neurons",
    "title": "Optimization in DL",
    "section": "Dead Neurons",
    "text": "Dead Neurons\n\nIn recent years, the use of the sigmoid and the tanh activation functions has been increasingly replaced with the ReLU and the hard tanh functions.\nThe ReLU is faster to train because its gradient computation amounts to checking nonnegativity.\nHowever the ReLU activation introduces a new problem of dead neurons: when a neuron has negative activation, it is dead.\nThe negative activation would happen for a couple of reasons:\n\nThe weights are initialized to be negative\nThe learning rate is too high\n\nOnce a neuron is dead, the weights of this neuron will never be updated further during training.\nSome solutions are:\n\nChoose a modest learning rate\nUse some variants of ReLU"
  },
  {
    "objectID": "slides/05-optimization.html#variants-of-relu",
    "href": "slides/05-optimization.html#variants-of-relu",
    "title": "Optimization in DL",
    "section": "Variants of ReLU",
    "text": "Variants of ReLU\nThere are several variants of ReLU that are designed to address the dying ReLU problem, for example,\n\nLeaky ReLU: \\(f(x) = \\max(\\alpha x, x)\\)\nExponential Linear Unit (ELU): \\(f(x) = \\begin{cases} x & \\text{if } x &gt; 0, \\\\ \\alpha(\\exp(x) - 1) & \\text{if } x \\leq 0. \\end{cases}\\)\n\n\n\n\n\n\n\n\nImage source: Clevert, D. A., Unterthiner, T., & Hochreiter, S. (2016). Fast and accurate deep network learning by exponential linear units (elus). ICLR."
  },
  {
    "objectID": "slides/05-optimization.html#maxout-networks",
    "href": "slides/05-optimization.html#maxout-networks",
    "title": "Optimization in DL",
    "section": "Maxout Networks",
    "text": "Maxout Networks\n\nThe maxout network is proposed by Goodfellow et. al. (2013) to address the dying ReLU problem.\nThe maxout unit outputs \\(\\max(W_1x + b_1, W_2x + b_2)\\).\nIt can be viewed as a generalization of the ReLU and the leaky ReLU:\n\nIf \\(W_2 = 0\\) and \\(b_2 = 0\\), then it is the ReLU.\nIf \\(W_2 = \\alpha W_1\\) and \\(b_2 = \\alpha b_1\\), then it is the leaky ReLU.\n\nHowever, it does not saturate at all, and is linear almost everywhere. In spite of its linearity, it has been shown that maxout networks are universal function approximators.\nMaxout has advantages over the ReLU, and it enhances the performance of ensemble methods like Dropout.\nHowever, one drawback with maxout is that it doubles the number of parameters."
  },
  {
    "objectID": "slides/05-optimization.html#skip-connections",
    "href": "slides/05-optimization.html#skip-connections",
    "title": "Optimization in DL",
    "section": "Skip connections",
    "text": "Skip connections\n\nThe skip connection is proposed by He et. al. (2016)1.\nThe idea is to reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.\nThis is often called residual learning.\n\n\n\n\n\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (pp. 770-778)."
  },
  {
    "objectID": "slides/05-optimization.html#loss-landscape-with-skip-connections",
    "href": "slides/05-optimization.html#loss-landscape-with-skip-connections",
    "title": "Optimization in DL",
    "section": "Loss Landscape with Skip Connections",
    "text": "Loss Landscape with Skip Connections\n\nThe ResNet-56 is a network proposed by He et. al. (2016), which has 56 layers and 0.85M parameters.\nWith the skip connections, the loss function becomes much smoother and easier to optimize.\n\n\n\n\n\n\n\n\nImage source: Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). Visualizing the loss landscape of neural nets. NeurIPS."
  },
  {
    "objectID": "slides/05-optimization.html#batch-normalization",
    "href": "slides/05-optimization.html#batch-normalization",
    "title": "Optimization in DL",
    "section": "Batch Normalization",
    "text": "Batch Normalization\n\nBatch normalization (BN) is a method to address the vanishing and exploding gradient problems.\nThe idea is simple: we normalize the output of a unit \\(i\\) over a batch of training samples (substract the mean and divide by the standard deviation) and then scale and shift the result.\nMore specifically, let \\(x_{ij}\\) be the output of \\(j\\)th sample in the \\(i\\)th unit. Then the BN layer computes \\[\\begin{align*}\n\\mu_i & = \\frac{1}{m}\\sum_{j=1}^m x_{ij}, \\quad \\sigma_i^2 = \\frac{1}{m}\\sum_{j=1}^m (x_{ij} - \\mu_i)^2, \\\\\n\\tilde{x}_{ij} & = \\frac{x_{ij} - \\mu_i}{\\sigma_i}, \\quad y_{ij} = \\gamma_i \\tilde{x}_{ij} + \\beta_i\n\\end{align*}\\] where \\(\\gamma_i\\) and \\(\\beta_i\\) are learnable parameters.\nThere are two types of batch normalization:\n\npost-activation BN: normalize the output of the activation function\npre-activation BN: normalize the input to the activation function\n\nIt is argued that the pre-activation BN is better than the post-activation BN."
  },
  {
    "objectID": "slides/05-optimization.html#benefits-of-batch-normalization",
    "href": "slides/05-optimization.html#benefits-of-batch-normalization",
    "title": "Optimization in DL",
    "section": "Benefits of Batch Normalization",
    "text": "Benefits of Batch Normalization\n\nBatch normalization has several benefits:\n\nIt reduces the internal covariate shift, which is the change in the distribution of the inputs to a layer.\nIt acts as a regularizer, which reduces the need for dropout.\nIt allows for higher learning rates, which accelerates the convergence.\nIt makes the optimization more stable and less sensitive to the initialization.\n\nA variant of batch normalization, known as layer normalization, is known to work well with recurrent networks."
  },
  {
    "objectID": "slides/05-optimization.html#recap-of-gradient-descent",
    "href": "slides/05-optimization.html#recap-of-gradient-descent",
    "title": "Optimization in DL",
    "section": "Recap of Gradient Descent",
    "text": "Recap of Gradient Descent\n\nGiven the loss function \\(J(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, f(x_i; \\theta))\\) of a neural network, the gradient descent algorithm updates the parameters as \\[\n\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla J(\\theta^{(t)}),\n\\] where \\(\\eta\\) is the learning rate.\nThe gradient \\(\\nabla J(\\theta)\\) can be computed using the back-propagation algorithm.\nThe learning rate \\(\\eta\\) is a hyperparameter that needs to be tuned:\n\nIf \\(\\eta\\) is too small, the convergence is slow.\nIf \\(\\eta\\) is too large, the algorithm may diverge.\n\nWe will discuss some strategies to make the vanilla GD algorithm more efficient and friendly to deep networks:\n\nminibatch updates (using only a portion of the data to compute the gradient)\nmomentum (accelerating the convergence)\nadaptive learning rate (adjusting the learning rate during training)"
  },
  {
    "objectID": "slides/05-optimization.html#stochastic-gradient-descent",
    "href": "slides/05-optimization.html#stochastic-gradient-descent",
    "title": "Optimization in DL",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nIn most cases, the loss function is of the form \\(J(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, f(x_i; \\theta))\\), where \\(n\\) is the number of samples and hence the gradient is \\[\n\\nabla J(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\nabla \\ell(y_i, f(x_i; \\theta)).\n\\]\nThe stochastic gradient descent (SGD) algorithm updates the parameters using only one sample, i.e., \\[\n\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla \\ell(y_i, f(x_i; \\theta^{(t)}).\n\\]\nIf the sample is drawn uniformly from the training samples, i.e., \\(I \\sim \\text{uniform}(\\{1,2,\\ldots, n\\})\\), the stochastic gradient is an unbiased estimate of the true gradient \\[\n\\mathbb{E}_I \\left[\\nabla \\ell(y_I, f(x_I; \\theta^{(t)})\\right] = \\sum_{i=1}^n \\mathbb{P}(I = i)\\nabla \\ell(y_i, f(x_i; \\theta)) = \\frac{1}{n}\\sum_{i=1}^n \\nabla \\ell(y_i, f(x_i; \\theta)) = \\nabla J(\\theta^{(t)})\n\\] where the expectation is taking with respect to the sample index \\(I\\)."
  },
  {
    "objectID": "slides/05-optimization.html#minibatch-stochastic-gradient-descent",
    "href": "slides/05-optimization.html#minibatch-stochastic-gradient-descent",
    "title": "Optimization in DL",
    "section": "Minibatch Stochastic Gradient Descent",
    "text": "Minibatch Stochastic Gradient Descent\n\nIn practice, we randomly split the samples into minibatches (or simply batches) \\(\\mathcal{B}_1, \\ldots, \\mathcal{B}_k\\) each with size \\(m\\).\nThe SGD upadtes the parameters when it sees a new batch and an epoch is completed when the algorithm has seen all the batches, i.e.,\n\nfor \\(t = 1, \\ldots, \\text{num. of epoch}\\):\n\nfor \\(b = 1, \\ldots, k\\):\n\n\\(\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\cdot \\frac{1}{m}\\sum_{i\\in \\mathcal{B}_b} \\nabla \\ell(y_i, f(x_i; \\boldsymbol{\\theta}))\\)."
  },
  {
    "objectID": "slides/05-optimization.html#benefits-of-using-sgd",
    "href": "slides/05-optimization.html#benefits-of-using-sgd",
    "title": "Optimization in DL",
    "section": "Benefits of Using SGD",
    "text": "Benefits of Using SGD\n\nThere are some benefits of using SGD:\n\ncomputational efficiency: the gradient is computed using a small batch of data (more frequent updates)\nThe algorithm can potentially escape from local minima more easily since the gradient is noisier.\n\nThe SGD also introduces implicit bias since it is not moving towards the optimal direction (true gradient direction).\n\nSmaller batch size leads to more implicit bias.\n\nThis implicit bias is also related to the generalization performance 1.\n\nPeleg, A. & Hein, M.. (2024). Bias of Stochastic Gradient Descent or the Architecture: Disentangling the Effects of Overparameterization of Neural Networks. ICML."
  },
  {
    "objectID": "slides/05-optimization.html#determining-the-batch-size",
    "href": "slides/05-optimization.html#determining-the-batch-size",
    "title": "Optimization in DL",
    "section": "Determining the Batch size",
    "text": "Determining the Batch size\n\nLarger batches provide a more accurate estimate of the gradient.\nComputational limitations:\n\nUse smaller batch sizes if the model is large.\n\nSmall batches can oﬀer a regularizing eﬀect, due to the noise they add to the learning process.\nHowever, training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient.\nHence using smaller batch sizes can be computationally expensive.\nTypically, the batch size is chosen to be a power of 2, e.g., 32, 64, 128, 256, etc."
  },
  {
    "objectID": "slides/05-optimization.html#variants-of-sgd",
    "href": "slides/05-optimization.html#variants-of-sgd",
    "title": "Optimization in DL",
    "section": "Variants of SGD",
    "text": "Variants of SGD\n\nDue to the high dimensionality and non-convexity of the loss function, the vanilla SGD algorithm may not be efficient.\nThere are many variants of SGD that are designed to accelerate the convergence.\nWe will introduce three commonly used strategies:\n\nLearning Rate Schedules (gradually decreasing the learning rate)\nMomentum (accelerating the convergence)\nAdaptive Learning Rates (choosing the learning rate adaptively for each parameter)\n\nThese strategies often introduce additional hyperparameters that need to be tuned and can be combined with each other.\nGood: we now have more optimization strategies to choose from.\nBad: there is no best optimization algorithm and we now have more hyperparameters to tune.\nTypically, there will be some recommended default values for these hyperparameters."
  },
  {
    "objectID": "slides/05-optimization.html#learning-rate-schedules",
    "href": "slides/05-optimization.html#learning-rate-schedules",
    "title": "Optimization in DL",
    "section": "Learning Rate Schedules",
    "text": "Learning Rate Schedules\n\nThe learning rate \\(\\eta\\) is a hyperparameter that needs to be tuned and it greatly affects the convergence of the algorithm.\nIn practice, we often use a learning rate schedule to adjust the learning rate during training.\nFor example, in SGD, we use \\(\\eta_t\\) for the \\(t\\)-th epoch"
  },
  {
    "objectID": "slides/05-optimization.html#learning-rate-schedules-1",
    "href": "slides/05-optimization.html#learning-rate-schedules-1",
    "title": "Optimization in DL",
    "section": "Learning Rate Schedules",
    "text": "Learning Rate Schedules\n\nA suﬃcient condition to guarantee convergence of SGD is that \\[\n\\sum_{t=1}^{\\infty} \\eta_t = \\infty, \\quad \\text{and} \\quad \\sum_{t=1}^{\\infty} \\eta_t^2 &lt; \\infty.\n\\]\nIn practice, it is common to decay the learning rate linearly until iteration \\(\\tau\\): \\[\n\\eta_t = (1 − \\alpha) \\eta_0 + \\eta_{\\tau}\n\\] with \\(\\alpha = \\frac{t}{\\tau}\\). After iteration \\(\\tau\\), it is common to leave constant.\nThe learning rate may be chosen by trial and error, but it is usually best to choose it by monitoring learning curves that plot the objective function as a function of time."
  },
  {
    "objectID": "slides/05-optimization.html#momentum-based-learning",
    "href": "slides/05-optimization.html#momentum-based-learning",
    "title": "Optimization in DL",
    "section": "Momentum-based Learning",
    "text": "Momentum-based Learning\n\nThe method of momentum is designed to accelerate learning, especially in the face of high curvature, small but consistent gradients, or noisy gradients.\n\n\n\n\n\n\n\n\nImage source: Figure 4.10 of Aggarwal (2023) Neural Networks and Deep Learning."
  },
  {
    "objectID": "slides/05-optimization.html#standard-momentum",
    "href": "slides/05-optimization.html#standard-momentum",
    "title": "Optimization in DL",
    "section": "Standard Momentum",
    "text": "Standard Momentum\n\nThe standard momentum algorithm introduces a variable \\(\\boldsymbol{v}\\) that plays the role of velocity — it is the direction and speed at which the parameters move through parameter space: \\[\\begin{align*}\n\\boldsymbol{v} & \\leftarrow \\alpha \\boldsymbol{v} - \\eta \\nabla_{\\boldsymbol{\\theta}}\\left(\\frac{1}{m} \\sum_{i \\in \\mathcal{B}} \\ell\\left(f\\left(x_i ; \\boldsymbol{\\theta}\\right), y_i\\right)\\right) \\\\\n\\boldsymbol{\\theta} &\\leftarrow \\boldsymbol{\\theta} + \\boldsymbol{v}\n\\end{align*}\\]\nNote that if the previous velocity is in the same direction as the current gradient, the update will be faster and vice versa.\nCommon values of \\(\\alpha\\) used in practice include 0.5, 0.9, and 0.99. Like the learning rate, \\(\\alpha\\) may also be adapted over time.\nTypically it begins with a small value and is later raised. Adapting \\(\\alpha\\) over time is less important than shrinking \\(\\eta\\) over time."
  },
  {
    "objectID": "slides/05-optimization.html#nesterov-momentum",
    "href": "slides/05-optimization.html#nesterov-momentum",
    "title": "Optimization in DL",
    "section": "Nesterov Momentum",
    "text": "Nesterov Momentum\n\nThe Nesterov momentum algorithm is a modification of the original momentum algorithm: \\[\\begin{align*}\n\\boldsymbol{v} & \\leftarrow \\alpha \\boldsymbol{v} - \\eta \\nabla_{\\boldsymbol{\\theta}}\\left(\\frac{1}{m} \\sum_{i \\in \\mathcal{B}} \\ell\\left(f\\left(x_i ; \\boldsymbol{\\theta} + \\alpha \\boldsymbol{v}\\right), y_i\\right)\\right) \\\\\n\\boldsymbol{\\theta} &\\leftarrow \\boldsymbol{\\theta} + \\boldsymbol{v}\n\\end{align*}\\]\nThe key difference is that the gradient is evaluated at the point \\(\\boldsymbol{\\theta} + \\alpha \\boldsymbol{v}\\) rather than at \\(\\boldsymbol{\\theta}\\).\nIt can be shown that for gradient descent case, Nesterov momentum converges faster than the original momentum algorithm."
  },
  {
    "objectID": "slides/05-optimization.html#adaptive-learning-rates",
    "href": "slides/05-optimization.html#adaptive-learning-rates",
    "title": "Optimization in DL",
    "section": "Adaptive Learning Rates",
    "text": "Adaptive Learning Rates\n\nThe learning rate is one of the most diﬃcult to set hyperparameters because it significantly aﬀects model performance.\nThe loss is often highly sensitive to some directions in parameter space and insensitive to others.\nHence we can cause a separate learning rate for each parameter and automatically adapt these learning rates throughout the course of learning.\nThe idea is simple: in the directions where the gradient is consistently small, we want to take larger steps and in the directions where the gradient is larger, we want to take smaller steps.\nWe will discuss some popular adaptive learning rate algorithms:\n\nAdaGrad\nRMSprop\nAdam"
  },
  {
    "objectID": "slides/05-optimization.html#adagrad",
    "href": "slides/05-optimization.html#adagrad",
    "title": "Optimization in DL",
    "section": "AdaGrad",
    "text": "AdaGrad\nThe AdaGrad algorithm individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all the historical squared values of the gradient."
  },
  {
    "objectID": "slides/05-optimization.html#rmsprop",
    "href": "slides/05-optimization.html#rmsprop",
    "title": "Optimization in DL",
    "section": "RMSprop",
    "text": "RMSprop\nThe RMSProp algorithm modifies AdaGrad to perform better in the nonconvex setting by changing the gradient accumulation into an exponentially weighted moving average."
  },
  {
    "objectID": "slides/05-optimization.html#adam",
    "href": "slides/05-optimization.html#adam",
    "title": "Optimization in DL",
    "section": "Adam",
    "text": "Adam\n\nAdam = RMSProp + Momentum + Bias Correction"
  },
  {
    "objectID": "slides/05-optimization.html#bias-correction",
    "href": "slides/05-optimization.html#bias-correction",
    "title": "Optimization in DL",
    "section": "Bias Correction",
    "text": "Bias Correction\n\nThe velocity \\(\\boldsymbol{v}\\) is actually an estimate of the first moment of the gradient: \\[\\begin{align*}\n\\boldsymbol{v}_t & = \\rho_1 \\boldsymbol{v}_{t-1} + (1-\\rho_1)\\boldsymbol{g}_{t} \\\\\n& = \\rho_1 \\left(\\rho_1 \\boldsymbol{v}_{t-2} + (1-\\rho_1)\\boldsymbol{g}_{t-1}\\right) + (1-\\rho_1)\\boldsymbol{g}_{t} \\\\\n& = \\rho_1^t \\boldsymbol{v}_0 + (1-\\rho_1)\\sum_{i=1}^{t} \\rho_1^{t-i}\\boldsymbol{g}_i.\n\\end{align*}\\]\nAssuming \\(\\boldsymbol{v}_0 = 0\\) and taking the expectation, we have \\[\\begin{align*}\n\\mathbb{E}[\\boldsymbol{v}_t] & = (1-\\rho_1)\\sum_{i=1}^t\\rho_1^{t-i}\\mathbb{E}[\\boldsymbol{g}_i] \\stackrel{\\textcolor{red}{(*)}}{=} (1-\\rho_1)\\cdot \\frac{1-\\rho_1^t}{1-\\rho_1}\\mathbb{E}[\\boldsymbol{g}_t] = (1-\\rho_1^t) \\mathbb{E}[\\boldsymbol{g}_t].\\\\\n\\end{align*}\\]\nThe expectation is taken with respect to the randomness in the gradient, i.e., we view \\(\\boldsymbol{g}_1, \\ldots, \\boldsymbol{g}_t \\sim F\\) as random vectors. The equality \\(\\textcolor{red}{(*)}\\) holds if the stochastic process \\(\\boldsymbol{g}_1, \\boldsymbol{g}_2, \\ldots\\) is stationary.\nHence the velocity is a biased estimate for \\(\\mathbb{E}[\\boldsymbol{g}_t]\\) and an unbiased estimate for \\(\\mathbb{E}[\\boldsymbol{g}_t]\\) is \\(\\frac{\\boldsymbol{v}_t}{1-\\rho_1^t}\\).\nThe same argument applies to the second moment \\(\\boldsymbol{r}_t\\)."
  },
  {
    "objectID": "slides/05-optimization.html#practical-recommendations",
    "href": "slides/05-optimization.html#practical-recommendations",
    "title": "Optimization in DL",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nTraining a deep neural network requires you to\n\nchoose a good architecture\nchoose a good optimization algorithm\n\nBoth choices have many hyperparameters that need to be tuned and there is no one-fit-all solution.\nFor optimization algorithms, it is recommended to start with Adam or RMSProp using the default hyperparameters (for the momentum or decay rate).\nIf the model is not converging, try to reduce the learning rate or use a learning rate schedule.\nAll the algorithms have been implemented in popular deep learning libraries, such as PyTorch and TensorFlow, and you can use them directly without worrying about the details.\nNext time, we will discuss some regularization techniques to improve the generalization performance of deep networks.\n\n\n\n\nHome"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Goodfellow et al. (2016) Deep Learning\nZhang et al. (2023) Dive into Deep Learning\nC. C. Aggarwal (2023). Neural Networks and Deep Learning\n\n\n\n\n\nV. Vapnik (2000). The Nature of Statistical Learning Theory\nC. M. Bishop (2006). Pattern Recognition and Machine Learning.\nT. Hastie, R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning.\nS. Shalev-Shwartz and S. Ben-David (2014). Understanding Machine Learning: From Theory to Algorithms.\n\n\n\n\n\nNalisnick, E., Smyth, P., & Tran, D. (2023). A brief tour of deep learning from a statistical perspective. Annual Review of Statistics and Its Application, 10(1), 219-246."
  },
  {
    "objectID": "resources.html#references",
    "href": "resources.html#references",
    "title": "Resources",
    "section": "",
    "text": "Goodfellow et al. (2016) Deep Learning\nZhang et al. (2023) Dive into Deep Learning\nC. C. Aggarwal (2023). Neural Networks and Deep Learning\n\n\n\n\n\nV. Vapnik (2000). The Nature of Statistical Learning Theory\nC. M. Bishop (2006). Pattern Recognition and Machine Learning.\nT. Hastie, R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning.\nS. Shalev-Shwartz and S. Ben-David (2014). Understanding Machine Learning: From Theory to Algorithms.\n\n\n\n\n\nNalisnick, E., Smyth, P., & Tran, D. (2023). A brief tour of deep learning from a statistical perspective. Annual Review of Statistics and Its Application, 10(1), 219-246."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Deep Learning (Fall 2024)",
    "section": "",
    "text": "Week\nDate\nTopics\nSlides\nReading\n\n\n\n\n1\n9/3\nCourse Introduction\nSlide\n\n\n\n2\n9/10\nReview of Linear Models\nSlide\n\n\n\n3\n9/17\nNo class (Mid-Autumn Festival)\n\n\n\n\n4\n9/24\nMachine Learning Basics\nSlide\nDL Ch. 5\n\n\n5\n10/1\nMultilayer Perceptron\nSlide\nD2L Ch. 5 & DL Ch. 6\n\n\n6\n10/8\nOptimization for DL Models\nSlide\nD2L Ch. 12 & DL Ch. 8\n\n\n7\n10/15\nRegularization for Deep Learning\nSlide\nDL Ch. 7\n\n\n8\n10/22\nProject Proposal\n\n\n\n\n9\n10/29\nImplementation of DL Models\nSlide Colab\nD2L Ch. 6\n\n\n10\n11/5\nConvolutional Networks\nSlide\nD2L Ch. 7, 8 & DL Ch. 9\n\n\n11\n11/12\nRecurrent Networks\nSlide\nD2L Ch. 9, 10 & DL Ch. 10\n\n\n12\n11/19\nGenerative Models: Autoencoder\nSlide\nDL Ch. 13, 14\n\n\n13\n11/26\nGenerative Models: GAN, Flow-based models, Diffusion models\nSlide\n\n\n\n14\n12/3\nAttention Mechanism and Graph Neural Networks\nSlide\n\n\n\n15-16\n12/10-17\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "Statistical Deep Learning (Fall 2024)",
    "section": "",
    "text": "Week\nDate\nTopics\nSlides\nReading\n\n\n\n\n1\n9/3\nCourse Introduction\nSlide\n\n\n\n2\n9/10\nReview of Linear Models\nSlide\n\n\n\n3\n9/17\nNo class (Mid-Autumn Festival)\n\n\n\n\n4\n9/24\nMachine Learning Basics\nSlide\nDL Ch. 5\n\n\n5\n10/1\nMultilayer Perceptron\nSlide\nD2L Ch. 5 & DL Ch. 6\n\n\n6\n10/8\nOptimization for DL Models\nSlide\nD2L Ch. 12 & DL Ch. 8\n\n\n7\n10/15\nRegularization for Deep Learning\nSlide\nDL Ch. 7\n\n\n8\n10/22\nProject Proposal\n\n\n\n\n9\n10/29\nImplementation of DL Models\nSlide Colab\nD2L Ch. 6\n\n\n10\n11/5\nConvolutional Networks\nSlide\nD2L Ch. 7, 8 & DL Ch. 9\n\n\n11\n11/12\nRecurrent Networks\nSlide\nD2L Ch. 9, 10 & DL Ch. 10\n\n\n12\n11/19\nGenerative Models: Autoencoder\nSlide\nDL Ch. 13, 14\n\n\n13\n11/26\nGenerative Models: GAN, Flow-based models, Diffusion models\nSlide\n\n\n\n14\n12/3\nAttention Mechanism and Graph Neural Networks\nSlide\n\n\n\n15-16\n12/10-17\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#important-dates",
    "href": "index.html#important-dates",
    "title": "Statistical Deep Learning (Fall 2024)",
    "section": "Important Dates:",
    "text": "Important Dates:\n\n9/17: No Class (Mid-Autumn Festival)\n10/22: Proposal Presentation\n12/10-17: Final Project Presentation"
  },
  {
    "objectID": "slides/01-intro.html#course-description",
    "href": "slides/01-intro.html#course-description",
    "title": "STAT 5011: Course Introduction",
    "section": "Course Description",
    "text": "Course Description\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nThis course provides an introduction to some commonly used models in deep learning:\n\nMultilayer Perceptron (MLP) or Fully-connected neural network (FCN)\nConvolutional Neural Network (CNN)\nRecurrent Neural Network (RNN)\nGenerative models\n\nThe course will cover the basic theory, practical implementation, and some applications of these models."
  },
  {
    "objectID": "slides/01-intro.html#prerequisites",
    "href": "slides/01-intro.html#prerequisites",
    "title": "STAT 5011: Course Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nKnowledge of linear algebra, calculus, probability, and statistics is required.\nExperiences in Python programming is also required (import libraries, write functions, etc.)\nKnowledge of object-oriented programming is a plus.\nKnowledge of machine learning would also be helpful (we will cover some basics in the course)."
  },
  {
    "objectID": "slides/01-intro.html#references",
    "href": "slides/01-intro.html#references",
    "title": "STAT 5011: Course Introduction",
    "section": "References",
    "text": "References\nDeep Learning: https://www.deeplearningbook.org"
  },
  {
    "objectID": "slides/01-intro.html#references-1",
    "href": "slides/01-intro.html#references-1",
    "title": "STAT 5011: Course Introduction",
    "section": "References",
    "text": "References\nDive into Deep Learning: https://d2l.ai"
  },
  {
    "objectID": "slides/01-intro.html#other-resources",
    "href": "slides/01-intro.html#other-resources",
    "title": "STAT 5011: Course Introduction",
    "section": "Other Resources",
    "text": "Other Resources\n\nT. Hastie, R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning.\nC. M. Bishop (2006). Pattern Recognition and Machine Learning.\nS. Shalev-Shwartz and S. Ben-David (2014). Understanding Machine Learning: From Theory to Algorithms.\nV. Vapnik (2000). The Nature of Statistical Learning Theory\nC. C. Aggarwal (2023). Neural Networks and Deep Learning"
  },
  {
    "objectID": "slides/01-intro.html#schedule",
    "href": "slides/01-intro.html#schedule",
    "title": "STAT 5011: Course Introduction",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReading\n\n\n\n\n1\n9/3\nCourse Introduction\n\n\n\n2\n9/10\nReview of Linear Models\n\n\n\n3\n9/17\nNo class (Mid-Autumn Festival)\n\n\n\n4\n9/24\nMachine Learning Basics\nDL Ch. 5\n\n\n5\n10/1\nMultilayer Perceptron\nD2L Ch. 5 & DL Ch. 6\n\n\n6\n10/8\nRegularization for Deep Learning\nDL Ch. 7\n\n\n7\n10/15\nOptimization for DL Models\nD2L Ch. 12 & DL Ch. 8\n\n\n8\n10/22\nProject Proposal\n\n\n\n9\n10/29\nImplementation of DL Models\nD2L Ch. 6\n\n\n10\n11/5\nConvolutional Networks\nD2L Ch. 7, 8 & DL Ch. 9\n\n\n11\n11/12\nRecurrent Networks\nD2L Ch. 9, 10 & DL Ch. 10\n\n\n12\n11/19\nHyperparameter Optimization and Tuning\nD2L Ch. 19 & DL Ch. 11\n\n\n13\n11/26\nGenerative Models: Autoencoders, GAN, Diffusion models\nD2L Ch. 20 & DL Ch. 14\n\n\n14\n12/3\nAdditional Topics: Attention Mechanisms and Gaussian Process\nD2L Ch. 11, 18\n\n\n15-16\n12/10-17\nFinal Project Presentation"
  },
  {
    "objectID": "slides/01-intro.html#grading",
    "href": "slides/01-intro.html#grading",
    "title": "STAT 5011: Course Introduction",
    "section": "Grading",
    "text": "Grading\n\nHomework: 30%\nProject proposal: 20%\n\nA 20-minute presentation\n\nFinal Project: 50%\n\nA 30-minute presentation (25%)\nA final report (25%)\n\nOffice hours: Tue. 15:00-17:00"
  },
  {
    "objectID": "slides/01-intro.html#homework",
    "href": "slides/01-intro.html#homework",
    "title": "STAT 5011: Course Introduction",
    "section": "Homework",
    "text": "Homework\n\nThere will be 3 homework assignments.\nHomework includes some math problems and programming exercises.\nProgramming assignments will be done using IPython notebooks and exported to PDF.\nMath problems will be submitted as a PDF file (using LaTeX preferably).\nDO NOT:\n\nPlagiarism: copy solution from others or from the internet.\nTake photos of your computer screen.\nTake photos of your handwritten solutions."
  },
  {
    "objectID": "slides/01-intro.html#project-proposal",
    "href": "slides/01-intro.html#project-proposal",
    "title": "STAT 5011: Course Introduction",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nA group of 2-3 students\nPick a topic that you plan to solve using deep learning models, for example:\n\nimage classification/segmentation\nstock price prediction\nweather forcasting\n\nIt could be something related to your thesis research.\nThe proposal should include:\n\nDiscription of your problem\nExample dataset\nSummary of 1-2 references\n\nGive a 20-minute presentation on 10/22"
  },
  {
    "objectID": "slides/01-intro.html#final-project",
    "href": "slides/01-intro.html#final-project",
    "title": "STAT 5011: Course Introduction",
    "section": "Final Project",
    "text": "Final Project\n\nOral Presentation (25%)\n\n30-minute presentation\nFocus the model you used, the dataset, and the results\nCompare to other models\n\nWritten Report (25%)\n\nUse the template: NeurIPS\n6-page including references; one report per group\nInclude: introduction, methods, results, and conclusion\n\nMore details will be provided later."
  },
  {
    "objectID": "slides/01-intro.html#what-is-deep-learning",
    "href": "slides/01-intro.html#what-is-deep-learning",
    "title": "STAT 5011: Course Introduction",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?"
  },
  {
    "objectID": "slides/01-intro.html#what-is-dlml",
    "href": "slides/01-intro.html#what-is-dlml",
    "title": "STAT 5011: Course Introduction",
    "section": "What is DL/ML?",
    "text": "What is DL/ML?\n\nDeep learning is a subfield of machine learning that is based on deep neural networks (DNN).\nDNN is a powerful approximating class of parametric class of functions.\nML is a field of study that focuses on automatic detection/extraction of patterns from raw data.\nTo achieve this, ML uses a variety of statistical models:\n\nlinear regression, logistic regression,\ntree models,\n\\(k\\)-nearest neighbors (kNN), etc."
  },
  {
    "objectID": "slides/01-intro.html#turing-test",
    "href": "slides/01-intro.html#turing-test",
    "title": "STAT 5011: Course Introduction",
    "section": "Turing Test",
    "text": "Turing Test\n\nThe Turing test, originally called the imitation game by Alan Turing in 1950, is a test of a machine’s ability to exhibit intelligent behaviour equivalent to that of a human.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956.\n\n\n\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "slides/01-intro.html#hebbs-theory",
    "href": "slides/01-intro.html#hebbs-theory",
    "title": "STAT 5011: Course Introduction",
    "section": "Hebb’s Theory",
    "text": "Hebb’s Theory\n\nIn 1949, Donald Hebb1 proposed a theory of learning in which the connection between two neurons is strengthened if they are activated simultaneously.\nHebbian learning rule:\n\nThe connection between two neurons: \\(w_{ij} \\leftarrow w_{ij} + \\Delta w_{ij}\\)\nThe change in the connection: \\(\\Delta w_{ij} = \\eta x_i x_j\\)\nwhere \\(\\eta\\) is the learning rate, \\(x_i\\) and \\(x_j\\) are the activities of the two neurons.\n\n\nHebb, D. O. (1949). The Organization of Behavior: A Neuropsychological Theory."
  },
  {
    "objectID": "slides/01-intro.html#biological-neuron-model",
    "href": "slides/01-intro.html#biological-neuron-model",
    "title": "STAT 5011: Course Introduction",
    "section": "Biological Neuron Model",
    "text": "Biological Neuron Model\n\n\nimage/svg+xml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendrite\n\n\n\n\n\n\n\n\nSoma (cell body)\n\n\n\n\n\n\n\n\n\n\nAxon terminal\n\n\n\n\n\n\n\n\n\n\n\n\nMyelinated axon trunk\n\n\n\n\n\n\n\n\n\n\nMyelin sheat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\n\nOutputs\n\n\n\n\n\nInput points = synapses\nOutput points = synapses\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "slides/01-intro.html#artificial-neuron",
    "href": "slides/01-intro.html#artificial-neuron",
    "title": "STAT 5011: Course Introduction",
    "section": "Artificial Neuron",
    "text": "Artificial Neuron\n\nMcCulloch and Pitts (1943) proposed a simple mathematical model for neurons.\nA neuron has \\(n\\) inputs \\(x = (x_1, ... ,x_n) \\in \\mathbb{R}^n\\) and one output \\(y \\in \\{-1, 1\\}\\).\n\\((u * v)\\) is the inner product of two vectors, \\(b\\) is a threshold value, and \\(\\text{sign}(u)= 1\\) if \\(u &gt; 0\\) and \\(\\text{sign}(u)= -1\\) if \\(u\\leq 0\\).\nDuring the learning process, the model chooses appropriate coefficients \\(w, b\\) of the neuron."
  },
  {
    "objectID": "slides/01-intro.html#rosenblatts-perceptron-1960s",
    "href": "slides/01-intro.html#rosenblatts-perceptron-1960s",
    "title": "STAT 5011: Course Introduction",
    "section": "Rosenblatt’s Perceptron (1960s)",
    "text": "Rosenblatt’s Perceptron (1960s)\n\nRosenblatt considered a model that is a composition of several neurons.\nEach neuron has its own weight \\(w\\) and threshold \\(b\\)."
  },
  {
    "objectID": "slides/01-intro.html#perceptron-learning-algorithm-pla",
    "href": "slides/01-intro.html#perceptron-learning-algorithm-pla",
    "title": "STAT 5011: Course Introduction",
    "section": "Perceptron Learning Algorithm (PLA)",
    "text": "Perceptron Learning Algorithm (PLA)\n\nThe weights and bias between the input and the hidden layer are random numbers and kept fixed.\nLet \\((x_1,y_1),\\ldots,(x_n,y_n)\\) be the training data and \\(z_i\\) be the transformation of the input \\(x_i\\) in the hidden layer.\n\nInitialize weights: \\(w^{(0)} = 0\\).\nIf the next example of the training data \\((z_{k+1}, y_{k+1})\\) is classified correctly, i.e., \\[\n      y_{k+1}(w^{(k)}\\cdot z_{k+1}) &gt; 0,\n  \\] then \\(w^{(k + 1)} = w^{(k)}\\).\nIf the next element is classified incorrectly, i.e., \\[\n     y_{k+1}(w^{(k)}\\cdot z_{k+1}) \\leq 0,\n\\] then \\(w^{(k +1)} = w^{(k)} +y_{k+1}z_{k+1}\\)."
  },
  {
    "objectID": "slides/01-intro.html#mark-i-perceptron",
    "href": "slides/01-intro.html#mark-i-perceptron",
    "title": "STAT 5011: Course Introduction",
    "section": "Mark I Perceptron",
    "text": "Mark I Perceptron\n\n\n\nMark I Perceptron (1960)"
  },
  {
    "objectID": "slides/01-intro.html#rosenblatts-experiment",
    "href": "slides/01-intro.html#rosenblatts-experiment",
    "title": "STAT 5011: Course Introduction",
    "section": "Rosenblatt’s Experiment",
    "text": "Rosenblatt’s Experiment\n\n\n\n\n\n\n\nRosenblatt, F. (1960). Perceptron simulation experiments. Proceedings of the IRE, 48(3), pages 301-309."
  },
  {
    "objectID": "slides/01-intro.html#theoretical-analysis-of-pla",
    "href": "slides/01-intro.html#theoretical-analysis-of-pla",
    "title": "STAT 5011: Course Introduction",
    "section": "Theoretical Analysis of PLA",
    "text": "Theoretical Analysis of PLA\nIn 1962, Novikoff1 proved the first theorem about the PLA. If\n\nthe norm of the training vectors \\(z\\) is bounded by some constant \\(R\\) (\\(|z| \\leq R\\)),and\n(linear separability) the training data can be separated with margin \\(\\rho\\): \\[\n     \\sup_w \\min_i y_i(z_i \\cdot w) &gt; \\rho\n\\]\n\nThen after at most \\(N \\leq \\frac{R^2}{\\rho^2}\\) steps, the hyperplane that separates the training data will be constructed.\nNovikoff, A. B. J. (1962). On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathematical Theory of Automata, Vol. XII, pages 615–622."
  },
  {
    "objectID": "slides/01-intro.html#learning-theory",
    "href": "slides/01-intro.html#learning-theory",
    "title": "STAT 5011: Course Introduction",
    "section": "Learning Theory",
    "text": "Learning Theory\n\nNovikoff’s result and Rosenblatt’s experiment raised several questions:\n\nWhat can be learned?\nWhat is the principle for designing learning algorithms?\nHow can we assure that the algorithm is actually learning, not just memorizing?\n\nThese questions led to the development of the statistical learning theory during 70s-80s.\nImportant results include:\n\nVapnik-Chervonenkis (VC) theory (for characterizing the capacity of a model)\nProbably Approximately Correct (PAC) learning theory (for characterizing whether a model can learn from a finite sample)\nEmpirical Risk Minimization (ERM) principle (for designing learning algorithms)"
  },
  {
    "objectID": "slides/01-intro.html#revival-of-neural-networks",
    "href": "slides/01-intro.html#revival-of-neural-networks",
    "title": "STAT 5011: Course Introduction",
    "section": "Revival of Neural Networks",
    "text": "Revival of Neural Networks\n\nIn 1986, several authors independently proposed a method for simultaneously constructing the vector coefficients for all neurons of the Perceptron using the so-called back-propagation method12.\nThe idea is to replace to McCulloch-Pitts neuron model with a sigmoid approximation, i.e., \\[\n     y = S(w\\cdot x - b)\n\\] where \\(S(x)\\) is a sigmoid function (differentiable, monotonic, \\(S(-\\infty) = -1\\) and \\(S(\\infty) = 1\\)).\nThis allows us to apply gradient-based optimization methods to find the optimal weights.\n\nLe Cun, Y. (1986). Learning processes in an asymmetric threshold network, Disordered systems and biological organizations, Les Houches, France, Springer, pages 233-240.Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propagation, Parallel distributed processing: Explorations in the microstructure of cognition, Vol. I, Badford Books, Cambridge, MA., pages 318-362."
  },
  {
    "objectID": "slides/01-intro.html#example-of-sigmoid-functions",
    "href": "slides/01-intro.html#example-of-sigmoid-functions",
    "title": "STAT 5011: Course Introduction",
    "section": "Example of sigmoid functions",
    "text": "Example of sigmoid functions\n\n\n\n\nSigmoidal Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\n\n\n  \n  \n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\n\n\n\n\n\t\n\t\n\t\n\n\n\n  \n  \n  \n  \n\n\n\n\t\n\n\n\n\n\t\n\t\t\n\t\t\n\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\t\n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t\n\n\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\n\n\n\n\t\n\t\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t\n\t\n\t\n\n\n\n\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "slides/01-intro.html#universal-approximation-theorem",
    "href": "slides/01-intro.html#universal-approximation-theorem",
    "title": "STAT 5011: Course Introduction",
    "section": "Universal Approximation Theorem",
    "text": "Universal Approximation Theorem\n\nIn 1989, Cybenko1 proved the universal approximation theorem for feedforward neural networks.\nThe theorem states that\n\n\n… networks with one internal layer and an arbitrary continuous sigmoidal function can approximate continuous functions wtih arbitrary precision providing that no constraints are placed on the number of nodes or the size of the weights.\n\n\nThat is, the finite sum \\(G(x) = \\sum_{i=1}^h a_i S(w_i \\cdot x - b_i)\\), \\(x \\in D \\subseteq \\mathbb{R}^n\\), is dense in the space of continuous functions on \\(D\\) where \\(D\\) is compact.\n\nCybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), pages 303-314."
  },
  {
    "objectID": "slides/01-intro.html#in-the-1990s",
    "href": "slides/01-intro.html#in-the-1990s",
    "title": "STAT 5011: Course Introduction",
    "section": "In the 1990s",
    "text": "In the 1990s\n\nLe Cun (1989)1 proposed convolutional network for data with grid-like structure, e.g., images.\nHochreiter and Schmidhuber (1997)2 introduced the Long Short-Term Memory (LSTM) network to model sequential data, e.g., language and time series data.\nDue to the difficulty in training, more attention is now focused on the alternatives to neural networks, for example,\n\nsupport vector machine (SVM, Cortes and Vapnik (1995))\nkernel methods3\ngraphical models4\n\n\nLe Cun, Y. (1989). Generalization and network design strategies. Technical Report CRG-TR-89-4, University of Toronto.Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), pages 1735-1780.Schölkopf, B., & Smola, A. J. (2002). Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.Jordan, M. I. (1999). Learning in graphical models. MIT press."
  },
  {
    "objectID": "slides/01-intro.html#s---present",
    "href": "slides/01-intro.html#s---present",
    "title": "STAT 5011: Course Introduction",
    "section": "2000s - present",
    "text": "2000s - present\n\nIn 2006, Geoffrey Hinton1 showed that a kind of neural network called a deep belief network could be efficiently trained using a strategy called greedy layer-wise pretraining.\nThis wave of neural networks research popularized the use of the term deep learning to emphasize that researchers were now able to train deeper neural networks than had been possible before.\nDeep neural networks started to outperform other ML models (e.g., AlexNet (2012), VGG (2014), ResNet (2015)).\nAlso the presence of big data motivates researchers and practitioners to develop complicated models.\nIn 2023, ChatGPT broke the Turing test2.\n\nHinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural computation, 18(7), pages 1527-1554.Biever, C. (2023). ChatGPT broke the Turing test-the race is on for new ways to assess AI. Nature, 619(7971), 686-689."
  },
  {
    "objectID": "slides/01-intro.html#three-waves-of-neural-networks",
    "href": "slides/01-intro.html#three-waves-of-neural-networks",
    "title": "STAT 5011: Course Introduction",
    "section": "Three Waves of Neural Networks",
    "text": "Three Waves of Neural Networks\n\nThe first wave: 1940s-1960s\n\nFundamental concepts: artificial neuron, perceptron\nPerceptron learning algorithm\n\nThe second wave: 1980s-1990s\n\nBack-propagation algorithm\nNetwork design strategies: convolutional networks, LSTM\n\nThe third wave: 2000s-present\n\nDeep neural networks\nLarge datasets and computational resources\nLarge Language Model (LLM), e.g., ChatGPT"
  },
  {
    "objectID": "slides/01-intro.html#the-end-of-the-second-wave",
    "href": "slides/01-intro.html#the-end-of-the-second-wave",
    "title": "STAT 5011: Course Introduction",
    "section": "The end of the second wave",
    "text": "The end of the second wave\nGoodfellow et al. (2016) pointed out\n\nThe second wave of neural networks research lasted until the mid-1990s. Ventures based on neural networks and other AI technologies began to make unrealistically ambitious claims while seeking investments. When AI research did not fulfill these unreasonable expectations, investors were disappointed."
  },
  {
    "objectID": "slides/01-intro.html#an-impending-ai-doom-model-collapse",
    "href": "slides/01-intro.html#an-impending-ai-doom-model-collapse",
    "title": "STAT 5011: Course Introduction",
    "section": "An Impending AI Doom: Model Collapse",
    "text": "An Impending AI Doom: Model Collapse\n\nShumailov et al. (2023)1 showed that training on generated data can make models forget.\nThey demonstrated that training on generated data can lead to catastrophic forgetting, a phenomenon where models forget how to perform well on real data.\n\n\n\n\n\n\nShumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., & Anderson, R. (2023). The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493."
  },
  {
    "objectID": "slides/01-intro.html#other-readings",
    "href": "slides/01-intro.html#other-readings",
    "title": "STAT 5011: Course Introduction",
    "section": "Other readings",
    "text": "Other readings\n\nThe story of Frank Rosenblatt: Professor’s perceptron paved the way for AI – 60 years too soon\n\nWhat is ‘model collapse’? An expert explains the rumours about an impending AI doom.\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/06-regularization.html#what-is-regularization",
    "href": "slides/06-regularization.html#what-is-regularization",
    "title": "Regularization for deep learning",
    "section": "What is Regularization?",
    "text": "What is Regularization?\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nRegularization is a technique designed to reduce the test error (generalization error), possibly at the expense of increased training error.\nIn this lecture, we will discuss several regularization techniques.\nSome are used for general machine learning models, while others are specific to deep learning models.\nPrinciple of regularization:\n\nencode specific prior knowledge\nexpress a preference for a simpler model class\nmake an underdetermined problem determined\ncombine multiple hypotheses (models) that explain the training data\n\nTypically, regularization techniques trade increased bias for reduced variance."
  },
  {
    "objectID": "slides/06-regularization.html#outline",
    "href": "slides/06-regularization.html#outline",
    "title": "Regularization for deep learning",
    "section": "Outline",
    "text": "Outline\n\nPenalty-Based Regularization\n\n\\(L_2\\) Regularization\n\\(L_1\\) Regularization\n\nEnsemble Methods\n\nBagging\nDropout\nData Perturbation Ensemble\n\nOther Regularization Techniques\n\nEarly Stopping\nData Augmentation\nParameter Sharing"
  },
  {
    "objectID": "slides/06-regularization.html#penalty-based-regularization",
    "href": "slides/06-regularization.html#penalty-based-regularization",
    "title": "Regularization for deep learning",
    "section": "Penalty-Based Regularization",
    "text": "Penalty-Based Regularization\n\nRecall that the generic objective function for training a neural network is \\[\nJ(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f(\\boldsymbol{x}_i, \\boldsymbol{\\theta})).\n\\]\nTo add a regularization term to the objective function, we modify it as follows: \\[\nJ_{\\alpha}(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f(\\boldsymbol{x}_i, \\boldsymbol{\\theta})) + \\alpha \\Omega(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}) + \\alpha \\Omega(\\boldsymbol{\\theta}),\n\\] where \\(\\alpha\\) is a hyperparameter that controls the strength of the regularization term.\nTypically, the regularization term \\(\\Omega(\\boldsymbol{\\theta})\\) is a norm of the parameter vector \\(\\boldsymbol{\\theta}\\).\nA norm expresses the concept of the length of a vector in a vector space, i.e., the distance from the origin.\nWhen we minimize the regularized objective function, we are looking for a model that fits the training data well and whose parameter is not far from zero."
  },
  {
    "objectID": "slides/06-regularization.html#l_2-regularization",
    "href": "slides/06-regularization.html#l_2-regularization",
    "title": "Regularization for deep learning",
    "section": "\\(L_2\\) Regularization",
    "text": "\\(L_2\\) Regularization\n\nNote that in a neural network model, we have weights and biases as parameters, and we don’t regularize the biases.\nWe write the parameter \\(\\boldsymbol{\\theta}\\) as \\(\\boldsymbol{\\theta} = (\\boldsymbol{w}, \\boldsymbol{b})\\), where \\(\\boldsymbol{w}\\) is the weight vector and \\(\\boldsymbol{b}\\) is the bias vector.\nThe most common form of regularization is \\(L_2\\) regularization, \\[\n\\Omega(\\boldsymbol{\\theta}) = \\frac{1}{2}\\|\\boldsymbol{w}\\|_2^2 = \\frac{1}{2}\\boldsymbol{w}^T\\boldsymbol{w} = \\frac{1}{2}\\sum_{j=1}^d w_{j}^2.\n\\]\nHence the objective function becomes \\[\nJ_{\\alpha}(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}) = \\frac{\\alpha}{2}\\boldsymbol{w}^T\\boldsymbol{w} + J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}).\n\\]\nThe corresponding gradient is \\[\n\\nabla_{\\boldsymbol{w}} J_{\\alpha}(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}) = \\alpha\\boldsymbol{w} + \\nabla_{\\boldsymbol{w}}J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}).\n\\]\nThe update rule for the weight vector is \\[\n\\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\eta \\left(\\alpha\\boldsymbol{w} + \\nabla_{\\boldsymbol{w}}J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\})\\right) = (1 - \\eta\\alpha)\\boldsymbol{w} - \\eta\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}).\n\\]"
  },
  {
    "objectID": "slides/06-regularization.html#weight-decay",
    "href": "slides/06-regularization.html#weight-decay",
    "title": "Regularization for deep learning",
    "section": "Weight Decay",
    "text": "Weight Decay\n\nThe \\(L_2\\) regularization is also known as weight decay since in the updating rule the weight vector is multiplied by a factor less than 1.\nDenote \\(\\boldsymbol{w}^* = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{w}} J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\})\\) as the optimal weight vector without regularization and \\(\\tilde{\\boldsymbol{w}} = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{w}} J_{\\alpha}(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\})\\) as the optimal weight vector with regularization.\nConsider the second-order approximation of \\(J\\) around \\(\\boldsymbol{w}^*\\): (ignore the bias term for simplicity) \\[\n\\hat{J}(\\boldsymbol{w}) = J(\\boldsymbol{w}^*) + \\frac{1}{2}(\\boldsymbol{w} - \\boldsymbol{w}^*)^T\\boldsymbol{H}(\\boldsymbol{w} - \\boldsymbol{w}^*).\n\\]\nHence the minimum of \\(\\hat{J}(\\boldsymbol{w})\\) occurs the its gradient is zero \\(\\nabla_{\\boldsymbol{w}}\\hat{J}(\\boldsymbol{w}) = \\boldsymbol{H}(\\boldsymbol{w} - \\boldsymbol{w}^*) = 0\\).\nTo study the effect of weight decay, we add the weight decay gradient, i.e., \\[\n\\alpha\\tilde{\\boldsymbol{w}} + \\boldsymbol{H}(\\tilde{\\boldsymbol{w}} - \\boldsymbol{w}^*) = 0 \\quad \\Rightarrow \\quad \\tilde{\\boldsymbol{w}} = (\\boldsymbol{H} + \\alpha\\boldsymbol{I})^{-1}\\boldsymbol{H}\\boldsymbol{w}^*.\n\\]\nConsider the eigen-decomposition \\(\\boldsymbol{H} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^T\\), then \\[\n\\tilde{\\boldsymbol{w}} = \\left(\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{\\top}+\\alpha \\boldsymbol{I}\\right)^{-1} \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{\\top} \\boldsymbol{w}^* = \\boldsymbol{Q}(\\boldsymbol{\\Lambda} + \\alpha\\boldsymbol{I})^{-1}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^T\\boldsymbol{w}^*,\n\\] that is, the weight decay shrinks the \\(\\boldsymbol{w}^*\\) along the eigenvectors of \\(\\boldsymbol{H}\\)."
  },
  {
    "objectID": "slides/06-regularization.html#noise-injection",
    "href": "slides/06-regularization.html#noise-injection",
    "title": "Regularization for deep learning",
    "section": "Noise Injection",
    "text": "Noise Injection\n\nAnother interpretation of weight decay is that it adds perturbation to the data.\nConsider the single training data \\(\\{\\boldsymbol{x}, y\\}\\) and a linear regression model \\(y = \\boldsymbol{w}^T\\boldsymbol{x}\\).\nWe add a random noise vector \\(\\boldsymbol{\\epsilon}\\) to the training data, i.e., \\(\\tilde{\\boldsymbol{x}} = \\boldsymbol{x} + \\sqrt{\\alpha}\\boldsymbol{\\epsilon}\\).\nUse the perturbed data to predict \\(y\\) as \\[\n\\hat{y} = \\boldsymbol{w}^T\\tilde{\\boldsymbol{x}} = \\boldsymbol{w}^T\\boldsymbol{x} + \\sqrt{\\alpha}\\boldsymbol{w}^T\\boldsymbol{\\epsilon}.\n\\]\nCompute the expected value of the loss \\(L = (y - \\hat{y})^2\\) as \\[\\begin{align*}\n\\mathbb{E}[L] & = \\mathbb{E}[(y - \\hat{y})^2] = \\mathbb{E}[(y - \\boldsymbol{w}^T\\boldsymbol{x} + \\sqrt{\\alpha}\\boldsymbol{w}^T\\boldsymbol{\\epsilon})^2]\\\\\n& = (y - \\boldsymbol{w}^T\\boldsymbol{x})^2 + \\sqrt{\\alpha}(y - \\boldsymbol{w}^T\\boldsymbol{x})\\mathbb{E}(\\boldsymbol{w}^T\\boldsymbol{\\epsilon}) + \\alpha\\mathbb{E}\\left[(\\boldsymbol{w}^T\\boldsymbol{\\epsilon})^2\\right]\n\\end{align*}\\]\nAssuming \\(\\mathbb{E}(\\boldsymbol{\\epsilon}) = 0\\) and \\(\\mathbb{E}(\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T) = \\boldsymbol{I}\\), we have \\(\\mathbb{E}(\\boldsymbol{w}^T\\boldsymbol{\\epsilon})\\) and \\(\\mathbb{E}\\left[(\\boldsymbol{w}^T\\boldsymbol{\\epsilon})^2\\right] = \\sum_{j=1}^d w_j^2\\). Therefore, \\[\n\\mathbb{E}[L] = (y - \\boldsymbol{w}^T\\boldsymbol{x})^2 + \\alpha\\sum_{j=1}^d w_j^2\n\\] which is the same as the \\(L_2\\) regularized loss."
  },
  {
    "objectID": "slides/06-regularization.html#l_1-regularization",
    "href": "slides/06-regularization.html#l_1-regularization",
    "title": "Regularization for deep learning",
    "section": "\\(L_1\\) Regularization",
    "text": "\\(L_1\\) Regularization\n\nThe \\(L_1\\) regularization is defined as \\[\n\\Omega(\\boldsymbol{\\theta}) = \\|\\boldsymbol{w}\\|_1 = \\sum_{j=1}^d |w_j|.\n\\]\nHence the objective function and its gradient are \\[\\begin{align*}\nJ_{\\alpha}(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}) & = \\alpha\\|\\boldsymbol{w}\\|_1 + J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}),\\\\\n\\nabla_{\\boldsymbol{w}} J_{\\alpha}(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}) & = \\alpha\\text{sign}(\\boldsymbol{w}) + \\nabla_{\\boldsymbol{w}}J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\}).\n\\end{align*}\\]\nThe update rule for the weight vector is \\[\n\\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\eta \\left(\\alpha\\text{sign}(\\boldsymbol{w}) + \\nabla_{\\boldsymbol{w}}J(\\boldsymbol{\\theta}; \\{\\boldsymbol{x}_i, y_i\\})\\right).\n\\]\nNote that when \\(w_j = 0\\), the gradient is not defined. In practice, we can use the subgradient method in which the gradient at \\(w_j=0\\) is set stochastically to a value in \\([−1,+1]\\)."
  },
  {
    "objectID": "slides/06-regularization.html#l_1-vs.-l_2-regularization",
    "href": "slides/06-regularization.html#l_1-vs.-l_2-regularization",
    "title": "Regularization for deep learning",
    "section": "\\(L_1\\) vs. \\(L_2\\) Regularization",
    "text": "\\(L_1\\) vs. \\(L_2\\) Regularization\n\nFrom an accuracy point of view, \\(L_2\\)-regularization usually outperforms \\(L_1\\)-regularization.\nHowever, \\(L_1\\)-regularization does have specific applications from an interpretability point of view.\nAn interesting property of \\(L_1\\)-regularization is that it creates sparse solutions in which the vast majority of the values of \\(w_i\\) are 0s, i.e., \\(L_1\\)-regularization gives a sparse neural network."
  },
  {
    "objectID": "slides/06-regularization.html#ensemble-methods",
    "href": "slides/06-regularization.html#ensemble-methods",
    "title": "Regularization for deep learning",
    "section": "Ensemble Methods",
    "text": "Ensemble Methods\n\nEnsemble method is a general strategy of regularization in machine learning that combines multiple models to improve the generalization performance.\nIt is also known as model averaging.\nExamples include:\n\nBagging (Bootstrap Aggregating)\nBoosting\nDropout, and many others.\n\nThe reason that model averaging works is that diﬀerent models will usually not make all the same errors on the test set."
  },
  {
    "objectID": "slides/06-regularization.html#why-ensemble-methods-work",
    "href": "slides/06-regularization.html#why-ensemble-methods-work",
    "title": "Regularization for deep learning",
    "section": "Why ensemble methods work?",
    "text": "Why ensemble methods work?\n\nConsider for example a set of \\(k\\) regression models. Suppose that each model makes an error \\(\\epsilon_i\\) on each example.\nAssume the errors are from a zero-mean multivariate normal distribution with variances \\(\\mathbb{E}\\left[\\epsilon_i^2\\right]=v\\) and covariances \\(\\mathbb{E}\\left[\\epsilon_i \\epsilon_j\\right]=c\\).\nThen the error made by the average prediction of all the ensemble models is \\(\\frac{1}{k} \\sum_i \\epsilon_i\\). The expected squared error of the ensemble predictor is \\[\n\\mathbb{E}\\left[\\left(\\frac{1}{k} \\sum_i \\epsilon_i\\right)^2\\right] =\\frac{1}{k^2} \\mathbb{E}\\left[\\sum_i\\left(\\epsilon_i^2+\\sum_{j \\neq i} \\epsilon_i \\epsilon_j\\right)\\right]\n=\\frac{1}{k} v+\\frac{k-1}{k} c\n\\]\n\nIf the errors are perfectly correlated and \\(c = v\\), the mean squared error reduces to \\(v\\), so the model averaging does not help at all.\nIf the errors are perfectly uncorrelated and \\(c = 0\\), the expected squared error of the ensemble is only \\(\\frac{1}{k}v\\)."
  },
  {
    "objectID": "slides/06-regularization.html#construct-an-ensemble",
    "href": "slides/06-regularization.html#construct-an-ensemble",
    "title": "Regularization for deep learning",
    "section": "Construct an Ensemble",
    "text": "Construct an Ensemble\n\nTypically, but not necessarily, we combine multiple weak learners to form a strong learner.\nThere are many ways to construct multiple models:\n\nDifferent datasets (e.g., bagging)\nDifferent architectures (e.g., dropout)\nDifferent hyperparameters and initializations\nDifferent training algorithms\n\nThe models to be ensembled should be as diverse as possible; otherwise the ensemble will not be effective."
  },
  {
    "objectID": "slides/06-regularization.html#bagging-bootstrap-aggregating",
    "href": "slides/06-regularization.html#bagging-bootstrap-aggregating",
    "title": "Regularization for deep learning",
    "section": "Bagging (Bootstrap Aggregating)",
    "text": "Bagging (Bootstrap Aggregating)\n\n\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "slides/06-regularization.html#bagging-bootstrap-aggregating-1",
    "href": "slides/06-regularization.html#bagging-bootstrap-aggregating-1",
    "title": "Regularization for deep learning",
    "section": "Bagging (Bootstrap Aggregating)",
    "text": "Bagging (Bootstrap Aggregating)\n\nThe bootstrap stage constructs \\(k\\) diﬀerent datasets. Each dataset has the same number of examples as the original dataset, but each dataset is constructed by sampling with replacement from the original dataset.\nFor each dataset, we train a model (usually the same model) and then average the predictions of all the models.\nHowever, this seems impractical when each model is a large neural network, since training and evaluating such networks is costly in terms of runtime and memory.\nTypically, bagging is used with simpler models, such as decision trees or linear models."
  },
  {
    "objectID": "slides/06-regularization.html#dropout",
    "href": "slides/06-regularization.html#dropout",
    "title": "Regularization for deep learning",
    "section": "Dropout",
    "text": "Dropout\n\nDropout is a method that uses node sampling to create a neural network ensemble.\nThe idea is to randomly set a fraction of the nodes to zero during each training iteration."
  },
  {
    "objectID": "slides/06-regularization.html#dropout-as-an-ensemble-method",
    "href": "slides/06-regularization.html#dropout-as-an-ensemble-method",
    "title": "Regularization for deep learning",
    "section": "Dropout as an Ensemble Method",
    "text": "Dropout as an Ensemble Method\nThat is, dropout is equivalent to training an ensemble of \\(2^d\\) models, where \\(d\\) is the number of nodes in the network.\n\n\n\n\n\n\n\nImage source: Figure 7.6 of DL."
  },
  {
    "objectID": "slides/06-regularization.html#forward-propagation-with-dropout",
    "href": "slides/06-regularization.html#forward-propagation-with-dropout",
    "title": "Regularization for deep learning",
    "section": "Forward Propagation with Dropout",
    "text": "Forward Propagation with Dropout\n\n\n\nLoad an example into a minibatch.\nSample a binary mask \\(\\boldsymbol{\\mu}\\) from a Bernoulli distribution with probability \\(p\\).\nCompute the loss \\(J(\\boldsymbol{\\theta}, \\boldsymbol{\\mu})\\) of the model defined by parameters \\(\\boldsymbol{\\theta}\\) and mask \\(\\boldsymbol{\\mu}\\).\nMinimize the expected loss over the mask \\(\\mathbb{E}_{\\boldsymbol{\\mu}}J(\\boldsymbol{\\theta}, \\boldsymbol{\\mu})\\).\n\nTypically, an input unit is included with probability 0.8, and a hidden unit is included with probability 0.5.\n\n\n\n\n\n\n\n\n\nImage source: Figure 7.7 of DL."
  },
  {
    "objectID": "slides/06-regularization.html#dropout-vs.-bagging",
    "href": "slides/06-regularization.html#dropout-vs.-bagging",
    "title": "Regularization for deep learning",
    "section": "Dropout vs. Bagging",
    "text": "Dropout vs. Bagging\nThere are some differences between dropout and bagging:\n\nIn bagging, the models are all independent, while in dropout, the models share parameters.\nIn the case of bagging, each model is trained to convergence on its respective training set. In dropout, subnetworks are each trained for a single step, and the parameter sharing causes the remaining subnetworks to arrive at good settings of the parameters."
  },
  {
    "objectID": "slides/06-regularization.html#inference-prediction-with-dropout",
    "href": "slides/06-regularization.html#inference-prediction-with-dropout",
    "title": "Regularization for deep learning",
    "section": "Inference (prediction) with Dropout",
    "text": "Inference (prediction) with Dropout\n\nIn the case of bagging, each model \\(i\\) produces a probability distribution \\(p^{(i)}(y \\mid \\boldsymbol{x})\\).\nThe prediction of the ensemble is given by the arithmetic mean of all these distributions, \\[\\begin{align*}\n\\frac{1}{k} \\sum_{i=1}^k p^{(i)}(y \\mid \\boldsymbol{x}).\n\\end{align*}\\]\nIn the case of dropout, each submodel defined by mask vector \\(\\boldsymbol{\\mu}\\) defines a probability distribution \\(p(y \\mid \\boldsymbol{x}, \\boldsymbol{\\mu})\\).\nThe arithmetic mean over all masks is given by \\[\\begin{align*}\n\\sum_{\\boldsymbol{\\mu}} p(\\boldsymbol{\\mu}) p(y \\mid \\boldsymbol{x}, \\boldsymbol{\\mu})\n\\end{align*}\\] where \\(p(\\boldsymbol{\\mu})\\) is the probability distribution that was used to sample \\(\\boldsymbol{\\mu}\\) at training time.\nIn practice, we can approximate the inference with sampling, by averaging together the output from many masks (10-20 would be enough)."
  },
  {
    "objectID": "slides/06-regularization.html#weight-scaling-inference-rule",
    "href": "slides/06-regularization.html#weight-scaling-inference-rule",
    "title": "Regularization for deep learning",
    "section": "Weight Scaling Inference Rule",
    "text": "Weight Scaling Inference Rule\n\nAnother approach to aggregating the predictions of the submodels is to use the geometric mean instead of the arithmetic mean, i.e., \\[\np_{\\text{ensemble}}(y \\mid \\boldsymbol{x}) = \\left(\\prod_{i=1}^k p(y \\mid \\boldsymbol{x}, \\boldsymbol{\\mu}^{(i)})\\right)^{1/k}.\n\\]\nHowever, the geometric mean is not guaranteed to be a proper distribution and hence normalization is needed \\[\np_{\\text{ensemble}}(y \\mid \\boldsymbol{x}) \\leftarrow \\frac{p_{\\text{ensemble}}(y \\mid \\boldsymbol{x})}{\\sum_{y^{\\prime}} p_{\\text{ensemble}}\\left(y^{\\prime} \\mid \\boldsymbol{x}\\right)}.\n\\]\nThis can be approximated by the weight scaling rule:\n\nuse a single neural net at test time without dropout\nreplace the weights by their expected values, i.e., \\(\\tilde{w}_j = p w_j\\) where \\(p\\) is the dropout probability"
  },
  {
    "objectID": "slides/06-regularization.html#monte-carlo-dropout-mc-dropout",
    "href": "slides/06-regularization.html#monte-carlo-dropout-mc-dropout",
    "title": "Regularization for deep learning",
    "section": "Monte Carlo Dropout (MC Dropout)",
    "text": "Monte Carlo Dropout (MC Dropout)\n\nGal and Ghahramani (2016)1 showed that the dropout training process is mathematically equivalent to approximate Bayesian inference in a deep Gaussian process.\nThis observation makes dropout the simplest and easiest way to perform approximate Bayesian inference in a deep model.\nAn immediate consequence of this equivalence is that we can use dropout at test (inference) time to obtain model uncertainty estimates.\nThis is known as Monte Carlo dropout (MC dropout) estimate of model uncertainty.\nThe uncertainty information is crucial for many applications:\n\nwith high uncertainty, the model should not be trusted and human intervention is needed.\nwith low uncertainty, the model can be trusted and used directly for decision-making.\n\n\nGal, Y., & Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML."
  },
  {
    "objectID": "slides/06-regularization.html#pros-and-cons-of-dropout",
    "href": "slides/06-regularization.html#pros-and-cons-of-dropout",
    "title": "Regularization for deep learning",
    "section": "Pros and Cons of Dropout",
    "text": "Pros and Cons of Dropout\nPros:\n\nEfficiency: low memory and computation cost.\nRegularization: better than weight decay.\nPrevent feature co-adaptation.\nMore robust: redundancy between the features.\n\nCons:\n\nIt might take longer to train (due to the stochastic nature).\nWorks better only for larger models since it reduces the eﬀective capacity of a model."
  },
  {
    "objectID": "slides/06-regularization.html#data-perturbation-ensemble",
    "href": "slides/06-regularization.html#data-perturbation-ensemble",
    "title": "Regularization for deep learning",
    "section": "Data Perturbation Ensemble",
    "text": "Data Perturbation Ensemble\n\nDropout can ne seen as a form of data perturbation ensemble, as it implicitly adds random noise to the input data.\nWe can also explicitly add noise to the input data to create an ensemble, e.g., in SGD, we add noise to the input data \\(\\boldsymbol{x}\\) as \\(\\tilde{\\boldsymbol{x}} = \\boldsymbol{x} + \\boldsymbol{\\epsilon}\\) before updating the model parameters.\nHowever, the noise distribution should be carefully calibrated:\n\nadding too much noise destroys the data\nadding too little noise does not help with generalization"
  },
  {
    "objectID": "slides/06-regularization.html#early-stopping",
    "href": "slides/06-regularization.html#early-stopping",
    "title": "Regularization for deep learning",
    "section": "Early Stopping",
    "text": "Early Stopping\n\nExecuting gradient descent to convergence optimizes the loss on the training data, but not necessarily on the out-of-sample test data.\nA natural solution to this dilemma is to use early stopping:\n\nA portion of the training data is held out as a validation set.\nStop training when the validation loss increases for a certain number of steps.\n\nWhy early stopping works?\n\nRestricting the number of steps is eﬀectively restricting the distance of the final solution from the initialization point"
  },
  {
    "objectID": "slides/06-regularization.html#data-augmentation",
    "href": "slides/06-regularization.html#data-augmentation",
    "title": "Regularization for deep learning",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nThe best way to make a machine learning model generalize better is to train it on more data.\nOne way is to create fake data and add it to the training set.\nThe generation of fake data is problem dependent. It is important that the fake data is realistic and does not introduce bias.\nFor example, in object detection tasks, we can flip the image horizontally, rotate it, or change the brightness."
  },
  {
    "objectID": "slides/06-regularization.html#parameter-sharing",
    "href": "slides/06-regularization.html#parameter-sharing",
    "title": "Regularization for deep learning",
    "section": "Parameter Sharing",
    "text": "Parameter Sharing\n\nA natural form of regularization that reduces the parameter footprint of the model is the sharing of parameters across diﬀerent connections.\nOften, this type of parameter sharing is enabled by domain-specific insights.\nThe main insight required to share parameters is that the function computed at two nodes should be related in some way.\nImportant examples of parameter sharing include:\n\nConvolutional layers\nRecurrent neural networks: assumes each time step uses the same parameters"
  },
  {
    "objectID": "slides/06-regularization.html#guidelines-for-regularization",
    "href": "slides/06-regularization.html#guidelines-for-regularization",
    "title": "Regularization for deep learning",
    "section": "Guidelines for Regularization",
    "text": "Guidelines for Regularization\n\nWe have only introduced some general regularization techniques.\nIn practice, the choice of regularization technique depends on the specific problem and the model architecture.\nStart with a simple model:\n\nunderfitting \\(\\Rightarrow\\) increase model capacity\noverfitting \\(\\Rightarrow\\) add regularization\n\nAlways monitor the training and validation loss to diagnose the model performance.\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/10-generative_model.html#statistical-modeling",
    "href": "slides/10-generative_model.html#statistical-modeling",
    "title": "Generative models: Autoencoder",
    "section": "Statistical modeling",
    "text": "Statistical modeling\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nIn general, statistical modeling represents the data by a probability distribution.\nWe can roughly categorize statistical models into two types (non-exclusively):\n\nDescriptive models: describe the data by the probability density/mass function.\nGenerative models: describe the data by the generating process of data points.\n\nFor example, the following hierarchical model is considered a generative model: \\[\n\\begin{aligned}\n\\eta & \\sim N(0, 1),\\\\\n\\tau^2 & \\sim \\text{Gamma}(1, 1),\\\\\n\\theta \\mid \\eta, \\tau^2 & \\sim N(\\eta, \\tau^2),\\\\\n\\sigma^2 & \\sim \\text{Gamma}(1, 1),\\\\\nx \\mid \\theta, \\sigma^2 & \\sim N(\\theta, \\sigma^2).\n\\end{aligned}\n\\]\nIt is easy to generate \\(x\\) by following the generative process. Yet the marginal distribution of \\(x\\) is very complicated."
  },
  {
    "objectID": "slides/10-generative_model.html#descriptive-vs.-generative-models",
    "href": "slides/10-generative_model.html#descriptive-vs.-generative-models",
    "title": "Generative models: Autoencoder",
    "section": "Descriptive vs. Generative models",
    "text": "Descriptive vs. Generative models\n\nThe main advantage of generative models is that they can very flexible.\nHowever, the flexibility comes at a cost: it is computationally expensive to make inference.\nIn contrast, descriptive models are usually simpler and computationally efficient, but require strong assumptions.\nMost generative models face at least one of the following issues:\n\nStrong assumptions: Some model make strong assumptions about the structure in the data, for example, linear assumption in PCA.\nApproximations: Some models make severe approximations, leading to sub-optimal models.\nExpensive computation: Some models rely on computationally expensive inference procedures like Markov Chain Monte Carlo."
  },
  {
    "objectID": "slides/10-generative_model.html#sampling-based-statistical-inference",
    "href": "slides/10-generative_model.html#sampling-based-statistical-inference",
    "title": "Generative models: Autoencoder",
    "section": "Sampling-based statistical inference",
    "text": "Sampling-based statistical inference\n\nInference for generative models is usually done by sampling-based methods.\nIf \\(x\\) follows a complicated generative model, we can use the following procedure to make inference:\n\nGenerate a large number of samples \\(\\{x^{(1)}, \\ldots, x^{(N)}\\}\\) from the generative model.\nCompute the empirical distribution of the samples, i.e., \\(\\hat{p}(x) = \\frac{1}{N}\\sum_{i=1}^N \\delta_{x^{(i)}}(x)\\).\nUse \\(\\hat{p}(x)\\) as an approximation of the true distribution \\(p(x)\\).\n\nFor example, we can use \\(\\frac{1}{N}\\sum_{i=1}^N f(x^{(i)})\\) to approximate \\(\\mathbb{E}[f(x)]\\).\nThis type of inference is very common in Bayesian statistics, where the posterior distribution is usually complicated.\nFor DL models, the forward propagation can be viewed as a sampling process."
  },
  {
    "objectID": "slides/10-generative_model.html#supervised-vs.-unsupervised-learning",
    "href": "slides/10-generative_model.html#supervised-vs.-unsupervised-learning",
    "title": "Generative models: Autoencoder",
    "section": "Supervised vs. Unsupervised learning",
    "text": "Supervised vs. Unsupervised learning\n\nIn supervised learning, we have a set of input-output pairs \\(\\{(\\boldsymbol{x}_1, y_1), \\ldots, (\\boldsymbol{x}_n, y_n)\\}\\).\nThe typical model is a conditional model \\(p(\\boldsymbol{y} \\mid \\boldsymbol{x})\\), for example the Gaussian linear regression model is \\(p(y \\mid \\boldsymbol{x}) = N(\\boldsymbol{x}^T\\boldsymbol{\\beta}, \\sigma^2)\\).\nFor unsupervised learning, we only have a set of input data points \\(\\{\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n\\}\\).\nWe need to learn the distribution of the data points, i.e., \\(p(\\boldsymbol{x})\\), for example, we can use the kernel density estimation to estimate \\(p(\\boldsymbol{x})\\).\nHowever, the data contains noise and a more important goal is to remove the noise and extract the underlying structure in the data.\nAs usual, we will start with the linear models."
  },
  {
    "objectID": "slides/10-generative_model.html#outline",
    "href": "slides/10-generative_model.html#outline",
    "title": "Generative models: Autoencoder",
    "section": "Outline",
    "text": "Outline\n\nLinear Factor model\n\nPrinciple Component Analysis (PCA)\nProbabilistic PCA (PPCA)\n\nAutoencoder\n\nLinear Autoencoder\nVariational Autoencoder (VAE)\nTraining the VAE"
  },
  {
    "objectID": "slides/10-generative_model.html#linear-factor-model-1",
    "href": "slides/10-generative_model.html#linear-factor-model-1",
    "title": "Generative models: Autoencoder",
    "section": "Linear Factor model",
    "text": "Linear Factor model\n\nA linear factor model assumes that the data points are generated by a linear transformation of a low-dimensional latent variable, i.e., \\[\n\\boldsymbol{x} = \\boldsymbol{W}\\boldsymbol{z} + \\boldsymbol{\\epsilon},\n\\] where\n\n\\(\\boldsymbol{x} \\in \\mathbb{R}^p\\) is the observed data point,\n\\(\\boldsymbol{z} \\in \\mathbb{R}^q\\) is the latent variable with \\(q \\ll p\\),\n\\(\\boldsymbol{W} \\in \\mathbb{R}^{p \\times q}\\) is the transformation matrix, and\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) is the noise term.\n\nThe latent variable \\(\\boldsymbol{z}\\) captures the underlying structure in the data points.\nFor simplicity, we assume the \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{z}\\) are centered."
  },
  {
    "objectID": "slides/10-generative_model.html#analysis-for-linear-factor-model",
    "href": "slides/10-generative_model.html#analysis-for-linear-factor-model",
    "title": "Generative models: Autoencoder",
    "section": "Analysis for linear factor model",
    "text": "Analysis for linear factor model\n\nFactor analysis:\n\nthe latent variable \\(\\boldsymbol{z}\\) represents common factors, typically independent, that describe the shared structure in the observations\nthe matrix \\(\\boldsymbol{W}\\) represents the factor loadings that describe how the factors are combined to generate the observations\n\nPrincipal component analysis (PCA):\n\ntransform the observations to a new coordinate system where the axes are the directions of maximum variance\nthe transformation matrix \\(\\boldsymbol{W}\\) is assumed to be orthogonal\nit is mainly used to reduce the dimensionality of observations"
  },
  {
    "objectID": "slides/10-generative_model.html#principal-component-analysis-pca",
    "href": "slides/10-generative_model.html#principal-component-analysis-pca",
    "title": "Generative models: Autoencoder",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nLet \\(\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n \\in \\mathbb{R}^p\\) be the data points and \\(\\boldsymbol{X} = [\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\ldots, \\boldsymbol{x}_n]^T \\in \\mathbb{R}^{n\\times p}\\) be the data matrix. Assume that the data points are centered, i.e., \\(\\boldsymbol{X}^T \\boldsymbol{1} = \\boldsymbol{0}\\).\nThe goal of PCA is to find a set of \\(q\\) orthogonal vectors \\(\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\ldots, \\boldsymbol{w}_q \\in \\mathbb{R}^p\\) such that the variance of the projected data points \\(\\boldsymbol{z}_i = \\boldsymbol{W}^T \\boldsymbol{x}_i\\) is maximized, where \\(\\boldsymbol{W} = [\\boldsymbol{w}_1, \\ldots, \\boldsymbol{w}_q] \\in \\mathbb{R}^{p \\times q}\\).\nLet \\(\\boldsymbol{Z} = [\\boldsymbol{z}_1, \\boldsymbol{z}_2, \\ldots, \\boldsymbol{z}_n]^T \\in \\mathbb{R}^{n \\times q}\\) be the projected data matrix. Then \\(\\boldsymbol{Z} = \\boldsymbol{X} \\boldsymbol{W}\\).\nIt is easy to see that the projected data points have zero mean, since \\(\\boldsymbol{Z}^T \\boldsymbol{1} = \\boldsymbol{W}^T \\boldsymbol{X}^T \\boldsymbol{1} = \\boldsymbol{0}\\).\nThe sum of variances of the projected data points is \\[\n\\sum_{i=1}^n \\|\\boldsymbol{z}_i\\|^2 = \\sum_{i=1}^n \\boldsymbol{z}_i^T \\boldsymbol{z}_i = {\\rm tr}(\\boldsymbol{Z}^T\\boldsymbol{Z}) = {\\rm tr}(\\boldsymbol{W}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{W}).\n\\]\nThe optimization problem is \\[\n\\max_{\\boldsymbol{W}}\\; {\\rm tr}(\\boldsymbol{W}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{W}) \\quad \\text{subject to} \\quad \\boldsymbol{W}^T\\boldsymbol{W} = \\boldsymbol{\\boldsymbol{I}}.\n\\]"
  },
  {
    "objectID": "slides/10-generative_model.html#decomposition-of-variance",
    "href": "slides/10-generative_model.html#decomposition-of-variance",
    "title": "Generative models: Autoencoder",
    "section": "Decomposition of variance",
    "text": "Decomposition of variance\n\nNote that the total variance of the data points is \\(\\sum_{i=1}^n \\|\\boldsymbol{x}_i\\|^2 = {\\rm tr}(\\boldsymbol{X}^T\\boldsymbol{X})\\).\nLet \\(P_{\\boldsymbol{W}} \\in \\mathbb{R}^{p \\times p}\\) be the projection matrix onto the subspace spanned by \\(\\boldsymbol{W}\\), i.e., \\(P_{\\boldsymbol{W}} = \\boldsymbol{W}(\\boldsymbol{W}^T\\boldsymbol{W})^{-1}\\boldsymbol{W}^T\\).\nSince \\(\\boldsymbol{W}\\) is orthogonal, \\(P_{\\boldsymbol{W}} = \\boldsymbol{W}\\boldsymbol{W}^T\\). A projection matrix is idempotent, i.e., \\(P_{\\boldsymbol{W}}^2 = P_{\\boldsymbol{W}}\\).\nWe can decompose the data matrix \\(\\boldsymbol{X}\\) as \\(\\boldsymbol{X} = \\boldsymbol{X}(\\boldsymbol{I} - P_{\\boldsymbol{W}}) + \\boldsymbol{X}P_{\\boldsymbol{W}}\\).\nSince \\(P_{\\boldsymbol{W}}\\) is idempotent and symmetric, we have \\[\n\\boldsymbol{X}^T\\boldsymbol{X} = (\\boldsymbol{I} - P_{\\boldsymbol{W}})\\boldsymbol{X}^T\\boldsymbol{X}(\\boldsymbol{I} - P_{\\boldsymbol{W}}) + P_{\\boldsymbol{W}}\\boldsymbol{X}^T\\boldsymbol{X}P_{\\boldsymbol{W}}.\n\\]\nTherefore the total variance can be decomposed as \\[\n{\\rm tr}(\\boldsymbol{X}^T\\boldsymbol{X}) = {\\rm tr}((\\boldsymbol{I} - P_{\\boldsymbol{W}})\\boldsymbol{X}^T\\boldsymbol{X}(\\boldsymbol{I} - P_{\\boldsymbol{W}})) + {\\rm tr}(P_{\\boldsymbol{W}}\\boldsymbol{X}^T\\boldsymbol{X}P_{\\boldsymbol{W}}).\n\\]\nThe second term is the variance of the projected data points \\[\n{\\rm tr}(P_{\\boldsymbol{W}}\\boldsymbol{X}^T\\boldsymbol{X}P_{\\boldsymbol{W}}) = {\\rm tr}(\\boldsymbol{W}\\boldsymbol{W}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{W}\\boldsymbol{W}^T) = {\\rm tr}(\\boldsymbol{W}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{W}).\n\\]"
  },
  {
    "objectID": "slides/10-generative_model.html#reconstruction-error",
    "href": "slides/10-generative_model.html#reconstruction-error",
    "title": "Generative models: Autoencoder",
    "section": "Reconstruction error",
    "text": "Reconstruction error\n\nLet \\(\\tilde{\\boldsymbol{x}}_i = \\boldsymbol{W}\\boldsymbol{z}_i = \\boldsymbol{W}\\boldsymbol{W}^T\\boldsymbol{x}_i \\in \\mathbb{R}^p\\) be the reconstructed data point.\nThe reconstructed data matrix is \\(\\widetilde{\\boldsymbol{X}} = \\boldsymbol{Z}\\boldsymbol{W}^T = \\boldsymbol{X}\\boldsymbol{W}\\boldsymbol{W}^T \\in \\mathbb{R}^{n \\times p}\\).\nThe reconstruction error is \\[\\begin{align*}\n\\sum_{i=1}^n \\|\\boldsymbol{x}_i - \\tilde{\\boldsymbol{x}}_i\\|^2 & = \\|\\boldsymbol{X} - \\widetilde{\\boldsymbol{X}}\\|^2 = {\\rm tr}((\\boldsymbol{X} - \\widetilde{\\boldsymbol{X}})^T(\\boldsymbol{X} - \\widetilde{\\boldsymbol{X}}))\\\\\n& = {\\rm tr}\\left[(\\boldsymbol{X} - \\boldsymbol{X}\\boldsymbol{W}\\boldsymbol{W}^T)^T(\\boldsymbol{X} - \\boldsymbol{X}\\boldsymbol{W}\\boldsymbol{W}^T)\\right]\\\\\n& = {\\rm tr}\\left[(\\boldsymbol{I} - \\boldsymbol{W}\\boldsymbol{W}^T)^T\\boldsymbol{X}^T\\boldsymbol{X}(\\boldsymbol{I} - \\boldsymbol{W}\\boldsymbol{W}^T)\\right]\\\\\n& = {\\rm tr}((\\boldsymbol{I} - P_{\\boldsymbol{W}})\\boldsymbol{X}^T\\boldsymbol{X}(\\boldsymbol{I} - P_{\\boldsymbol{W}})).\n\\end{align*}\\]\nThat is, the first term in the decomposition of the total variance is the reconstruction error."
  },
  {
    "objectID": "slides/10-generative_model.html#quick-summary-of-pca",
    "href": "slides/10-generative_model.html#quick-summary-of-pca",
    "title": "Generative models: Autoencoder",
    "section": "Quick summary of PCA",
    "text": "Quick summary of PCA\nGiven an orthogonal matrix \\(\\boldsymbol{W} \\in \\mathbb{R}^{p \\times q}\\), we can obtain\n\nthe projected data points \\(\\boldsymbol{Z} = \\boldsymbol{X}\\boldsymbol{W}\\),\nthe reconstructed data points based on \\(\\boldsymbol{Z}\\), \\(\\widetilde{\\boldsymbol{X}} = \\boldsymbol{Z}\\boldsymbol{W}^T = \\boldsymbol{X}\\boldsymbol{W}\\boldsymbol{W}^T\\),\nthe variance of the projected data points \\({\\rm tr}(\\boldsymbol{Z}^T\\boldsymbol{Z}) = {\\rm tr}(\\boldsymbol{W}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{W})\\),\nthe reconstruction error \\({\\rm tr}((\\boldsymbol{I} - P_{\\boldsymbol{W}})\\boldsymbol{X}^T\\boldsymbol{X}(\\boldsymbol{I} - P_{\\boldsymbol{W}}))\\),\nthe total variance \\({\\rm tr}(\\boldsymbol{X}^T\\boldsymbol{X})\\) can be decomposed as \\[\n\\text{total var.} = \\text{reconstruction error} + \\text{projected var.},\n\\]\nthe PCA finds the orthogonal matrix \\(\\boldsymbol{W}\\) that maximizes the projected variance, which is equivalent to minimizing the reconstruction error,\nthe solution is given by the eigenvectors of the covariance matrix \\(\\boldsymbol{X}^T\\boldsymbol{X}\\) (using Rayleigh quotients), or equivalently, the right singular vectors of the data matrix \\(\\boldsymbol{X}\\)."
  },
  {
    "objectID": "slides/10-generative_model.html#probabilistic-pca",
    "href": "slides/10-generative_model.html#probabilistic-pca",
    "title": "Generative models: Autoencoder",
    "section": "Probabilistic PCA",
    "text": "Probabilistic PCA\n\nProbabilistic PCA (PPCA) is to cast PCA in a probabilistic framework.\nRecall the linear factor model: \\(\\boldsymbol{x} = \\boldsymbol{W}\\boldsymbol{z} + \\boldsymbol{\\epsilon}\\). We now add additional distributional assumptions: \\[\n\\boldsymbol{z} \\sim N_q(0, \\boldsymbol{I}), \\qquad \\boldsymbol{\\epsilon} \\sim N_p(0, \\sigma^2 \\boldsymbol{I}).\n\\]\nUnder these assumptions, the distribution of \\(\\boldsymbol{x}\\) is given by \\(\\boldsymbol{x} \\sim N_p(0, \\boldsymbol{W}\\boldsymbol{W}^T +  \\sigma^2 \\boldsymbol{I})\\)\nThe MLE for \\(\\boldsymbol{W}\\) and \\(\\sigma^2\\) are given by \\[\n\\widehat{\\boldsymbol{W}}_{\\text{MLE}} = \\boldsymbol{U}_q (\\boldsymbol{\\Lambda}_q - \\hat{\\sigma}^2_{\\text{MLE}} \\boldsymbol{I})^{1/2} \\boldsymbol{R}, \\qquad \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{p-q}\\sum_{j=q+1}^p \\lambda_j,\n\\] where \\(\\boldsymbol{U}_q\\) and \\(\\boldsymbol{\\Lambda}_q = {\\rm diag}(\\lambda_1, \\ldots, \\lambda_q)\\) are the principal eigenvectors and eigenvalues of the sample covariance matrix, and \\(\\boldsymbol{R}\\) is an arbitrary \\(q \\times q\\) orthogonal matrix.1\n\nTipping, M. E., & Bishop, C. M. (1999). Probabilistic principal component analysis. Journal of the Royal Statistical Society Series B: Statistical Methodology, 61(3), 611-622."
  },
  {
    "objectID": "slides/10-generative_model.html#ppca-as-a-generative-model",
    "href": "slides/10-generative_model.html#ppca-as-a-generative-model",
    "title": "Generative models: Autoencoder",
    "section": "PPCA as a generative model",
    "text": "PPCA as a generative model\n\nOnce we obtain the MLEs \\(\\widehat{\\boldsymbol{W}}_{\\text{MLE}}\\) and \\(\\hat{\\sigma}^2_{\\text{MLE}}\\), the PPCA model can be viewed as a generative model:\n\nGenerate the latent variable \\(\\boldsymbol{z} \\sim N_q(0, \\boldsymbol{I})\\) and the noise term \\(\\boldsymbol{e} \\sim N_p(0, \\boldsymbol{I})\\).\nCompute \\(\\boldsymbol{x}^{\\star} = \\widehat{\\boldsymbol{W}}_{\\text{MLE}}\\boldsymbol{z} + \\hat{\\sigma}^2_{\\text{MLE}}\\boldsymbol{e}\\).\n\nBy the consistency of MLEs, we have \\(\\widehat{\\boldsymbol{W}}_{\\text{MLE}} \\stackrel{p}{\\to} \\boldsymbol{W}\\) and \\(\\hat{\\sigma}^2_{\\text{MLE}} \\stackrel{p}{\\to} \\sigma^2\\) as \\(n \\to \\infty\\).\nBy Slutsky’s theorem, we have \\(\\boldsymbol{x}^{\\star} \\stackrel{d}{\\to} \\boldsymbol{x}\\) as \\(n \\to \\infty\\).\nThat is, when the sample size is large, the generated data points \\(\\boldsymbol{x}^{\\star}\\) have the same distribution as the observed data points \\(\\boldsymbol{x}\\)."
  },
  {
    "objectID": "slides/10-generative_model.html#variants-of-pca",
    "href": "slides/10-generative_model.html#variants-of-pca",
    "title": "Generative models: Autoencoder",
    "section": "Variants of PCA",
    "text": "Variants of PCA\nThere are many variants of PCA/PPCA:\n\nSparse PCA1: adding the sparsity constraint (\\(L_1\\) penalty) to the projection matrix \\(\\boldsymbol{W}\\).\nRobust PCA2: minimizing a weighted combination of the nuclear norm and of the \\(\\ell_1\\) norm\nkernel PCA3: using the kernel trick to perform nonlinear dimensionality reduction.\nContrastive PCA4: performing PCA with reference information.\n\nZou, H., Hastie, T., & Tibshirani, R. (2006). Sparse principal component analysis. Journal of Computational and Graphical Statistics, 15(2), 265-286.Candès, E. J., Li, X., Ma, Y., & Wright, J. (2011). Robust principal component analysis?. Journal of the ACM (JACM), 58(3), 1-37.Schölkopf, B., Smola, A., & Müller, K. R. (1997, October). Kernel principal component analysis. In International Conference on Artificial Neural Networks (pp. 583-588). Berlin, Heidelberg: Springer Berlin Heidelberg.Abid, A., Zhang, M. J., Bagaria, V. K., & Zou, J. (2018). Exploring patterns enriched in a dataset with contrastive principal component analysis. Nature communications, 9(1), 2134."
  },
  {
    "objectID": "slides/10-generative_model.html#autoencoder-1",
    "href": "slides/10-generative_model.html#autoencoder-1",
    "title": "Generative models: Autoencoder",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nAn autoencoder is a neural network that is trained to attempt to copy its input to its output.\nIt has a hidden layer \\(\\boldsymbol{z}\\) that describes a code used to represent the input.\nThe network may be viewed as consisting of two parts: an encoder function \\(\\boldsymbol{z}= f(\\boldsymbol{x})\\) and a decoder that produces a reconstruction \\(\\boldsymbol{r} = g(\\boldsymbol{z})\\).\nUsually, the input \\(\\boldsymbol{x} \\in \\mathbb{R}^p\\) is high-dimensional, and the code \\(\\boldsymbol{z} \\in \\mathbb{R}^q\\) is low-dimensional, \\(q \\ll p\\).\nThe code can be used as a compact representation for the input data, and the decoder can be used to recover the inputs from the codes.\nAn autoencoder is trained to minimize the reconstruction error, i.e., the difference between the input and the reconstruction.\nFor example, if we use the squared error loss, the optimization problem is \\[\n\\min_{f,g} \\sum_{i=1}^n \\|\\boldsymbol{x}_i - g(f(\\boldsymbol{x}_i))\\|^2.\n\\]"
  },
  {
    "objectID": "slides/10-generative_model.html#linear-autoencoders",
    "href": "slides/10-generative_model.html#linear-autoencoders",
    "title": "Generative models: Autoencoder",
    "section": "Linear Autoencoders",
    "text": "Linear Autoencoders\n\nIf we use linear functions for the encoder and decoder, i.e., \\(f(\\boldsymbol{x}) = \\boldsymbol{W}_1 \\boldsymbol{x}\\) and \\(g(\\boldsymbol{z}) = \\boldsymbol{W}_2 \\boldsymbol{z}\\), the optimization problem becomes \\[\n\\min_{\\boldsymbol{W}_1, \\boldsymbol{W}_2} \\sum_{i=1}^n \\|\\boldsymbol{x}_i - \\boldsymbol{W}_2 \\boldsymbol{W}_1 \\boldsymbol{x}_i\\|^2\n\\] where \\(\\boldsymbol{W}_1 \\in \\mathbb{R}^{q \\times p}\\) and \\(\\boldsymbol{W}_2 \\in \\mathbb{R}^{p \\times q}\\).\nIf we additionally assume that \\(W_2 = W_1^T\\) and \\(W_1\\) is orthogonal, the optimization problem is equivalent to PCA."
  },
  {
    "objectID": "slides/10-generative_model.html#problem-with-autoencoders",
    "href": "slides/10-generative_model.html#problem-with-autoencoders",
    "title": "Generative models: Autoencoder",
    "section": "Problem with autoencoders",
    "text": "Problem with autoencoders\n\nIf the encoder and decoder are allowed too much capacity, the autoencoder can learn to perform the copying task without extracting useful information about the distribution of the data.\nFor example, we have a very powerful encoder that maps \\(\\boldsymbol{x}_i\\) to its index \\(i\\), and a very powerful decoder that maps \\(i\\) back to \\(\\boldsymbol{x}_i\\).\nIn this case, although the reconstruction error is zero, the encoder is not extracting any useful information about the data.\nTwo approaches to address this issue:\n\nAdd a regularization term to the optimization problem to encourage the model to have other properties besides the copying ability.\nConsider a probabilistic model for the autoencoder."
  },
  {
    "objectID": "slides/10-generative_model.html#probabilistic-model-for-autoencoder",
    "href": "slides/10-generative_model.html#probabilistic-model-for-autoencoder",
    "title": "Generative models: Autoencoder",
    "section": "Probabilistic model for autoencoder",
    "text": "Probabilistic model for autoencoder\n\n\n\nTo cast the autoencoder in a probabilistic framework, we consider the following model: \\[\\begin{align*}\n\\text{Latent variable} & : \\boldsymbol{z} \\sim p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}),\\\\\n\\text{Decoder} & : \\boldsymbol{x} \\mid \\boldsymbol{z} \\sim p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z}).\n\\end{align*}\\]\nBy Bayes’ theorem, the distribution of \\(\\boldsymbol{z} \\mid \\boldsymbol{x}\\) is given by \\[\np_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x}) = \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})p_{\\boldsymbol{\\theta}}(\\boldsymbol{z})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})}.\n\\]\nThe marginal distribution of \\(\\boldsymbol{x}\\) is given by \\[\np_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\int p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})p_{\\boldsymbol{\\theta}}(\\boldsymbol{z})d\\boldsymbol{z}.\n\\]\n\n\n\n\n\n\n\n\n\n\nImage source: Figure 2.1 in Diederik P. Kingma and Max Welling (2019), “An Introduction to Variational Autoencoders”, Foundations and Trends in Machine Learning: Vol. 12, No. 4, pp 307–392."
  },
  {
    "objectID": "slides/10-generative_model.html#probabilistic-model-for-autoencoder-1",
    "href": "slides/10-generative_model.html#probabilistic-model-for-autoencoder-1",
    "title": "Generative models: Autoencoder",
    "section": "Probabilistic model for autoencoder",
    "text": "Probabilistic model for autoencoder\n\n\n\nThe distribution of \\(\\boldsymbol{x}\\) is usually complicated and the latent space model \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{z})\\) is assumed to be simple.\nThe distribution \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) is intractable, but we can approximate it by a simple distribution \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\).\nThe approximating distribution \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) is called the variational distribution and the parameter \\(\\boldsymbol{\\phi}\\) is called the variational parameter.\nIn the context of autoencoder, it is called the variational encoder.\nTo summarize, the goal here is to find\n\nthe decoder \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})\\),\nthe variational encoder \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\),"
  },
  {
    "objectID": "slides/10-generative_model.html#assumptions",
    "href": "slides/10-generative_model.html#assumptions",
    "title": "Generative models: Autoencoder",
    "section": "Assumptions",
    "text": "Assumptions\n\nSince we are using \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) to approximate \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\), it is important to see what assumptions we pose on \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\).\nStrong assumptions:\n\nsimpler \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\)\neasier to optimize\nlarger approximation error\n\nWeak assumptions:\n\nmore complex \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\)\nharder to optimize\nsmaller approximation error\n\nA common choice is the simple factorized Gaussian encoder \\(z_i \\mid \\boldsymbol{x} \\stackrel{\\text{ind}}{\\sim} N(\\mu_i(\\boldsymbol{x}, \\boldsymbol{\\phi}), \\sigma_i^2(\\boldsymbol{x}, \\boldsymbol{\\phi}))\\) where \\(\\mu_i(\\boldsymbol{x}, \\boldsymbol{\\phi})\\) and \\(\\sigma_i^2(\\boldsymbol{x}, \\boldsymbol{\\phi})\\) are functions of \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{\\phi}\\)."
  },
  {
    "objectID": "slides/10-generative_model.html#variational-autoencoder-vae",
    "href": "slides/10-generative_model.html#variational-autoencoder-vae",
    "title": "Generative models: Autoencoder",
    "section": "Variational Autoencoder (VAE)",
    "text": "Variational Autoencoder (VAE)\n\nUsing neural networks to model \\(\\mu_i(\\boldsymbol{x}, \\boldsymbol{\\phi})\\) and \\(\\sigma_i^2(\\boldsymbol{x}, \\boldsymbol{\\phi})\\), the model diagram is as follows:\n\n\n\n\n\n\n\nThe next question is how to train the VAE, more specifically, what loss function to use."
  },
  {
    "objectID": "slides/10-generative_model.html#wrap-up-the-notations",
    "href": "slides/10-generative_model.html#wrap-up-the-notations",
    "title": "Generative models: Autoencoder",
    "section": "Wrap-up the notations",
    "text": "Wrap-up the notations\n\nA VAE contains the following components:\n\nThe variational encoder \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) with parameters \\(\\boldsymbol{\\phi}\\).\nThe decoder \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})\\) with parameters \\(\\boldsymbol{\\theta}\\).\n\nIf the encoder and decoder are parametrized by neural networks, the parameters \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\phi}\\) are the weights and biases of the encoder and decoder networks.\nTraining a VAE is to find the optimal parameters \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\phi}\\) that minimize the loss function (to be specified later).\nWe will introduce two criteria for training the VAE:\n\nthe variational inference approach (maximizing the evidence lower bound),\nthe maximum likelihood approach (maximizing the log-likelihood)."
  },
  {
    "objectID": "slides/10-generative_model.html#variational-inference",
    "href": "slides/10-generative_model.html#variational-inference",
    "title": "Generative models: Autoencoder",
    "section": "Variational Inference",
    "text": "Variational Inference\n\nVariational inference (VI) is an algorithmic framework for approximating distributions.\nWe want to approximate the distribution \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) with a simpler distribution \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) by minimizing the Kullback-Leibler (KL) divergence \\[\nD_{\\text{KL}}(q_{\\boldsymbol{\\phi}}\\|p_{\\boldsymbol{\\theta}}) = \\int \\log\\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})} q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})d\\boldsymbol{z} = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}}\\left[\\log\\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right].\n\\]\nThe main difficulty, as in other variational methods, is that \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) is intractable.\nHence we are not able to compute the KL divergence directly."
  },
  {
    "objectID": "slides/10-generative_model.html#why-the-kl-divergence",
    "href": "slides/10-generative_model.html#why-the-kl-divergence",
    "title": "Generative models: Autoencoder",
    "section": "Why the KL divergence?",
    "text": "Why the KL divergence?\n\nThe KL divergence is a measure of the difference between two distributions.\nSuppose you want to measure the difference between two normal distributions \\(N(\\mu_1, 1)\\) and \\(N(\\mu_2, 1)\\).\nThe most straightforward way is to compute the Euclidean distance between the means, i.e., \\(|\\mu_1 - \\mu_2|\\).\nHowever this is impossible when the two distributions are not of the same family, for example, a normal distribution and a Laplace distribution.\nThe KL divergence can be computed for any two distributions (with some minimal assumptions).\nFor two normal distributions \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\), the KL divergence is given by \\[\nD_{\\text{KL}}(N(\\mu_1, \\sigma_1^2)\\|N(\\mu_2, \\sigma_2^2)) = \\frac{1}{2}\\left(\\frac{\\sigma_1^2}{\\sigma_2^2} + \\frac{(\\mu_2 - \\mu_1)^2}{\\sigma_2^2} - 1 + \\log\\frac{\\sigma_2^2}{\\sigma_1^2}\\right).\n\\]"
  },
  {
    "objectID": "slides/10-generative_model.html#evidence-lower-bound-elbo",
    "href": "slides/10-generative_model.html#evidence-lower-bound-elbo",
    "title": "Generative models: Autoencoder",
    "section": "Evidence Lower Bound (ELBO)",
    "text": "Evidence Lower Bound (ELBO)\nThe actual optimization objective of the variational autoencoder, like in other variational methods, is the evidence lower bound (ELBO), which is derived as follows:\n\\[\n\\begin{aligned}\n\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) & =\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\right] \\\\\n& =\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log \\left[\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\\right] \\\\\n& =\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log \\left[\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})} \\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\\right] \\\\\n& =\\underbrace{\\mathbb{E}_{q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log \\left[\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})}{q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\\right]}_{\\substack{=\\mathcal{L}_{\\boldsymbol{\\theta}, \\phi}(\\boldsymbol{x})\\; (\\text{ELBO})}}\n+\\underbrace{\\mathbb{E}_{q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log \\left[\\frac{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}{p_\\theta(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\\right]}_{=D_{\\text{KL}}\\left(q_{\\boldsymbol{\\phi}} \\| p_{\\boldsymbol{\\theta}}\\right)}\n\\end{aligned}\n\\]\n\nSince \\(\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\) is fixed, minimizing the KL divergence is equivalent to maximizing the ELBO."
  },
  {
    "objectID": "slides/10-generative_model.html#maximize-the-elbo",
    "href": "slides/10-generative_model.html#maximize-the-elbo",
    "title": "Generative models: Autoencoder",
    "section": "Maximize the ELBO",
    "text": "Maximize the ELBO\n\nDue to the non-negativity of the KL divergence, the ELBO is a lower bound on the log-likelihood of the data \\[\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{\\theta}, \\boldsymbol{\\phi}}(\\boldsymbol{x}) & =\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})-D_{\\text{KL}}\\left(q_{\\boldsymbol{\\phi}} \\| p_{\\boldsymbol{\\theta}}\\right) \\\\\n& \\leq \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\n\\end{aligned}\n\\]\nHence maximization of the ELBO \\(\\mathcal{L}_{\\boldsymbol{\\theta}, \\phi}(\\boldsymbol{x})\\) w.r.t. the parameters \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\phi}\\), will concurrently optimize the two things:\n\nIt will approximately maximize the marginal likelihood \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\). This means that our generative model will become better.\nIt will minimize the KL divergence of the approximation \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) from the true distribution \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\), so \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\) becomes better."
  },
  {
    "objectID": "slides/10-generative_model.html#elbo-based-on-empirical-data",
    "href": "slides/10-generative_model.html#elbo-based-on-empirical-data",
    "title": "Generative models: Autoencoder",
    "section": "ELBO based on empirical data",
    "text": "ELBO based on empirical data\n\nGiven an i.i.d. dataset \\(\\mathcal{D} = \\{\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n\\}\\), the empirical distribution is \\[\nq_{\\mathcal{D}}(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i=1}^n \\delta_{\\boldsymbol{x}_i}\n\\] where \\(\\delta_{\\boldsymbol{x}_i}\\) is the Dirac delta function at \\(\\boldsymbol{x}_i\\).\nWith the empirical distribution \\(q_{\\mathcal{D}}(\\boldsymbol{x})\\) and the inference model \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\), we get a joint distribution \\[\nq_{\\mathcal{D}, \\phi}(\\boldsymbol{x}, \\boldsymbol{z})=q_{\\mathcal{D}}(\\boldsymbol{x}) q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x}).\n\\]"
  },
  {
    "objectID": "slides/10-generative_model.html#marginal-likelihood",
    "href": "slides/10-generative_model.html#marginal-likelihood",
    "title": "Generative models: Autoencoder",
    "section": "Marginal Likelihood",
    "text": "Marginal Likelihood\n\nWe can also train the VAE by maximizing the marginal log-likelihood \\(\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\).\nGiven an i.i.d. dataset \\(\\mathcal{D} = \\{\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n\\}\\), the empirical distribution is \\[\nq_{\\mathcal{D}}(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i=1}^n \\delta_{\\boldsymbol{x}_i}\n\\] where \\(\\delta_{\\boldsymbol{x}_i}\\) is the Dirac delta function at \\(\\boldsymbol{x}_i\\).\nThe KL divergence between the empirical distribution \\(q_{\\mathcal{D}}(\\boldsymbol{x})\\) and the marginal likelihood \\(p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\) \\[\n\\begin{aligned}\nD_{\\text{KL}}\\left(q_{\\mathcal{D}}(\\boldsymbol{x}) \\| p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\right) & =-\\mathbb{E}_{q_{\\mathcal{D}}(\\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\right]+\\mathbb{E}_{q_{\\mathcal{D}}(\\boldsymbol{x})}\\left[\\log q_{\\mathcal{D}}(\\boldsymbol{x})\\right] \\\\\n& =-\\frac{1}{n} \\sum_{i=1}^n \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_i)+\\mathrm{constant}.\n\\end{aligned}\n\\]\nThat is, maximizing the marginal log-likelihood is equivalent to minimizing the KL divergence between the empirical distribution and the marginal likelihood."
  },
  {
    "objectID": "slides/10-generative_model.html#estimating-the-marginal-likelihood",
    "href": "slides/10-generative_model.html#estimating-the-marginal-likelihood",
    "title": "Generative models: Autoencoder",
    "section": "Estimating the marginal likelihood",
    "text": "Estimating the marginal likelihood\n\nThe marginal log-likelihood \\(\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\) can be estimated using an importance sampling technique: \\[\n\\begin{aligned}\n\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})=\\log \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\n= \\log \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{z})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\n\\end{aligned}\n\\]\nWith random samples \\(\\boldsymbol{z}^{(1)}, \\ldots, \\boldsymbol{z}^{(L)}\\) from \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})\\), a Monte Carlo estimator of this is: \\[\n\\begin{aligned}\n\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) \\approx \\log \\frac{1}{L} \\sum_{l=1}^L p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z}^{(l)})\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}^{(l)})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}^{(l)} \\mid \\boldsymbol{x})}.\n\\end{aligned}\n\\]\nHence the loss for sample \\(\\boldsymbol{x}\\) is \\[\n\\begin{aligned}\nL_{\\text{ML}}({\\boldsymbol{\\theta}, \\boldsymbol{\\phi}}; \\boldsymbol{x}) & = - \\log \\frac{1}{L} \\sum_{l=1}^L p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z}^{(l)})\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}^{(l)})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}^{(l)} \\mid \\boldsymbol{x})}.\n\\end{aligned}\n\\]\nThis is the objective used in importance weighted autoencoders (IWAE)1.\nNote that for \\(L = 1\\), the ML loss and the ELBO loss are equivalent.\n\nBurda, Y., Grosse, R., & Salakhutdinov, R. (2015). Importance weighted autoencoders. arXiv preprint arXiv:1509.00519."
  },
  {
    "objectID": "slides/10-generative_model.html#image-generation-with-vae",
    "href": "slides/10-generative_model.html#image-generation-with-vae",
    "title": "Generative models: Autoencoder",
    "section": "Image generation with VAE",
    "text": "Image generation with VAE\nGenerated samples from a VAE trained on MNIST dataset:"
  },
  {
    "objectID": "slides/10-generative_model.html#different-types-of-generative-models",
    "href": "slides/10-generative_model.html#different-types-of-generative-models",
    "title": "Generative models: Autoencoder",
    "section": "Different types of generative models",
    "text": "Different types of generative models\n\n\n\n\n\n\n\nImage source: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"
  },
  {
    "objectID": "slides/10-generative_model.html#references",
    "href": "slides/10-generative_model.html#references",
    "title": "Generative models: Autoencoder",
    "section": "References",
    "text": "References\n\nFor more details about VAE, see\n\nOriginal paper by Kingma and Welling: https://arxiv.org/pdf/1312.6114\nIntroduction to VAE by Kingma and Welling: https://www.nowpublishers.com/article/Details/MAL-056\nTutorial on VAE by Doersch: https://arxiv.org/pdf/1606.05908\n\nFor implementation and examples of VAE, see\n\nhttps://keras.io/examples/generative/vae/\nhttps://github.com/AntixK/PyTorch-VAE\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/12-gnn.html#final-project",
    "href": "slides/12-gnn.html#final-project",
    "title": "Attention and Graph Neural Networks",
    "section": "Final Project",
    "text": "Final Project\nThe final project should include the following:\n\nA 25-min presentation (on 12/10 and 12/17):\n\nDataset and problem description\nModel architecture\nResults and analysis (comparison with a baseline model)\nConclusion\n\nA written report (due on 12/24):\n\nIntroduction\nRelated work\nModels and methods\nResults\nConclusion and References"
  },
  {
    "objectID": "slides/12-gnn.html#written-report",
    "href": "slides/12-gnn.html#written-report",
    "title": "Attention and Graph Neural Networks",
    "section": "Written report",
    "text": "Written report\n\nNeurIPS Template: https://www.overleaf.com/latex/templates/neurips-2024/tpsbbrdqcmsh\nUse bibtex to manage references.\nShould be 4-6 pages long (including references).\nImportant:\n\nDo not include code or output of the code in the report.\nThe code should be in a separate file.\nGive citations whenever you use code/figures/tables from other sources.\n\nYou should submit:\n\nA PDF file of the report.\nA ZIP file containing the code and other supplementary materials."
  },
  {
    "objectID": "slides/12-gnn.html#outline",
    "href": "slides/12-gnn.html#outline",
    "title": "Attention and Graph Neural Networks",
    "section": "Outline",
    "text": "Outline\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nGenerative models for sequential data\n\nSeq2Seq model\nAttention mechanism\nSelf-attention and transformer\n\nGraph Neural Networks\n\nGraph data\nGraph convolution\nOther GNN models"
  },
  {
    "objectID": "slides/12-gnn.html#autoencoder-for-sequential-data",
    "href": "slides/12-gnn.html#autoencoder-for-sequential-data",
    "title": "Attention and Graph Neural Networks",
    "section": "Autoencoder for sequential data",
    "text": "Autoencoder for sequential data\n\nRecall that an autoencoder contains an encoder and a decoder:\n\nThe encoder \\(f\\) maps the input data to a latent representation: \\(\\boldsymbol{z} = f(\\boldsymbol{x})\\).\nThe decoder \\(g\\) reconstructs the input data from the latent representation: \\(\\hat{\\boldsymbol{x}} = g(\\boldsymbol{z})\\).\n\nFor sequential data, we can use an RNN-based encoder and decoder:\n\nEncoder: \\(\\boldsymbol{h}_t = f(\\boldsymbol{h}_{t-1}, \\boldsymbol{x}_t)\\)\nDecoder: \\(\\hat{\\boldsymbol{x}}_t = g(\\boldsymbol{h}_t)\\)\nThe encoder and the decoder can be LSTM or RNN units.\n\nThe autoencoder is trained to minimize the reconstruction error: \\(\\mathcal{L} = \\sum_t \\|\\boldsymbol{x}_t - \\hat{\\boldsymbol{x}}_t\\|^2\\).\n\nSimilarly, we can design different generative models for sequential data, e.g., VAE, GAN."
  },
  {
    "objectID": "slides/12-gnn.html#seq2seq-model",
    "href": "slides/12-gnn.html#seq2seq-model",
    "title": "Attention and Graph Neural Networks",
    "section": "Seq2Seq model",
    "text": "Seq2Seq model\n\nThe sequence-to-sequence (Seq2Seq) model aims to map an input sequence to an target sequence, e.g., machine translation and question answering.\nA seq2seq model normally has an encoder-decoder architecture:\n\nThe encoder processes the input sequence and summarize the information into a context vector\nThe decoder generates the output seq based on the context vector.\n\nFor example, Sutskever et al. (2014)1 used an LSTM-based seq2seq model for machine translation.\n\n\n\n\n\n\nSutskever et al. (2014). Sequence to Sequence Learning with Neural Networks."
  },
  {
    "objectID": "slides/12-gnn.html#attention-mechanism",
    "href": "slides/12-gnn.html#attention-mechanism",
    "title": "Attention and Graph Neural Networks",
    "section": "Attention Mechanism",
    "text": "Attention Mechanism\n\nOne of the main limitation of the LSTM/GRU based encoder is that the output of the encoder is a fixed-length context vector.\nA fixed-length context vector may not be able to capture all the information in a long input sequence.\nThe attention mechanism was proposed to address this issue by allowing the decoder to focus on different parts of the input sequence at each decoding step.\nThat is, at each decoding step, the model tries to align the input sequence with the output element as much as possible."
  },
  {
    "objectID": "slides/12-gnn.html#additive-attention",
    "href": "slides/12-gnn.html#additive-attention",
    "title": "Attention and Graph Neural Networks",
    "section": "Additive Attention",
    "text": "Additive Attention\n\n\n\n\n\n\n\nImage source: Figure 4 in Attention? Attention! by Lilian Weng (2018)"
  },
  {
    "objectID": "slides/12-gnn.html#different-alignment-scores",
    "href": "slides/12-gnn.html#different-alignment-scores",
    "title": "Attention and Graph Neural Networks",
    "section": "Different Alignment Scores",
    "text": "Different Alignment Scores\n\n\n\n\n\n\n\n\nName\nAlignment Score function\nCitation\n\n\n\n\nContent-base attention\n\\(\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\frac{\\boldsymbol{s}_t \\cdot \\boldsymbol{h}_i}{\\|\\boldsymbol{s}_t\\| \\|\\boldsymbol{h}_i\\|}\\)\nGraves et al. (2014)\n\n\nAdditive Attention\n\\(\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}_1 \\boldsymbol{s}_t + \\boldsymbol{W}_2 \\boldsymbol{h}_i)\\)\nBahdanau et al. (2015)\n\n\nLocation-based Attention\n\\(\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{W}\\boldsymbol{s}_t\\)\nLuong et al. (2015)\n\n\nGeneral\n\\(\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top \\boldsymbol{W}\\boldsymbol{h}_i\\)\nLuong et al. (2015)\n\n\nDot-product\n\\(\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top \\boldsymbol{h}_i\\)\nLuong et al. (2015)\n\n\nScaled Dot-product\n\\(\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\frac{\\boldsymbol{s}_t^\\top \\boldsymbol{h}_i}{\\sqrt{n}}\\)\nVaswani et al. (2017)\n\n\n\n\n\nReproduced from the summary table in Attention? Attention! by Lilian Weng (2018)."
  },
  {
    "objectID": "slides/12-gnn.html#transformer",
    "href": "slides/12-gnn.html#transformer",
    "title": "Attention and Graph Neural Networks",
    "section": "Transformer",
    "text": "Transformer\n\nAttention is All you Need by Vaswani et al. (2017) proposed the transformer model that is entirely built on the self-attention mechanisms.\nThe vanilla transformer model has an encoder-decoder architecture:\n\nThe encoder generates an attention-based representation.\nThe decoder is to retrieve information from the encoded representation.\n\nLater simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only BERT1 or decoder-only GPT2.\n\nDelvin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingRadford et al. (2018) Improving Language Understanding by Generative Pre-Training"
  },
  {
    "objectID": "slides/12-gnn.html#self-attention",
    "href": "slides/12-gnn.html#self-attention",
    "title": "Attention and Graph Neural Networks",
    "section": "Self-attention",
    "text": "Self-attention\n\nSelf-attention, or the intra-attention, is an attention mechanism relating different positions of a single sequence to compute a representation of the sequence.\nIt has been shown to be very useful in machine reading, abstractive summarization, or image description generation.\nTheoretically the self-attention can adopt any score functions mentioned before, but just replace the target sequence with the same input sequence.\nIn the transformer model, the self-attention mechanism is used to compute the query, key, and value vectors from the input sequence:\n\na query is to represent the information of the current word,\na key is to represent information the other words in the sentence,\nwe use the key and the query to compute the attention weight,\nthe value is to represent the information that will be used to generate words."
  },
  {
    "objectID": "slides/12-gnn.html#multi-head-attention",
    "href": "slides/12-gnn.html#multi-head-attention",
    "title": "Attention and Graph Neural Networks",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\n\n\n\n\n\nImage source: Figure 2 in Vaswani et al. (2017)."
  },
  {
    "objectID": "slides/12-gnn.html#transformer-model",
    "href": "slides/12-gnn.html#transformer-model",
    "title": "Attention and Graph Neural Networks",
    "section": "Transformer model",
    "text": "Transformer model\n\n\n\n\n\n\n\nImage source: Figure 1 in Vaswani et al. (2017)."
  },
  {
    "objectID": "slides/12-gnn.html#local-vs.-global-attention",
    "href": "slides/12-gnn.html#local-vs.-global-attention",
    "title": "Attention and Graph Neural Networks",
    "section": "Local vs. Global Attention",
    "text": "Local vs. Global Attention\nInstead of using the full attention matrix, we can use a local attention mechanism to compute the attention weight matrix:\n\n\n\n\n\n\n\nImage source: Figure 2 and 3 in Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation."
  },
  {
    "objectID": "slides/12-gnn.html#convolution-and-attention",
    "href": "slides/12-gnn.html#convolution-and-attention",
    "title": "Attention and Graph Neural Networks",
    "section": "Convolution and attention",
    "text": "Convolution and attention\nThe attention mechanism can be applied to image data as well. In fact the convolution operation can be seen as a special case of the attention mechanism:\n\n\n\n\n\n\n\nImage source: Figure 1 in Cordonnier et al. (2020) On the relationship between self-attention and convolutional layers. In ICLR."
  },
  {
    "objectID": "slides/12-gnn.html#graph",
    "href": "slides/12-gnn.html#graph",
    "title": "Attention and Graph Neural Networks",
    "section": "Graph",
    "text": "Graph\n\nA graph \\(G = (V, E)\\) is a set of nodes \\(V = \\{v_1, \\ldots, v_n\\}\\) and edges \\(E = \\{(v_i, v_j) \\mid v_i, v_j \\in V\\}\\).\nA graph is undirected if \\((v_i, v_j) \\in E\\) implies \\((v_j, v_i) \\in E\\)."
  },
  {
    "objectID": "slides/12-gnn.html#adjacency-matrix",
    "href": "slides/12-gnn.html#adjacency-matrix",
    "title": "Attention and Graph Neural Networks",
    "section": "Adjacency matrix",
    "text": "Adjacency matrix\n\n\n\nThe adjacency matrix \\(\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}\\) of a graph is a binary matrix where \\(A_{ij} = 1\\) if \\((v_i, v_j) \\in E\\) and \\(A_{ij} = 0\\) otherwise.\nA graph is undirected if and only if the adjacency matrix is symmetric.\nFor example, the adjacency matrix of the graph on the right is: \\[\n    \\boldsymbol{A} = \\begin{bmatrix}\n    0 & 1 & 1 & 1 \\\\\n    1 & 0 & 1 & 0 \\\\\n    1 & 1 & 0 & 1 \\\\\n    1 & 0 & 1 & 0\n    \\end{bmatrix}.\n    \\]"
  },
  {
    "objectID": "slides/12-gnn.html#degree-matrix-and-graph-laplacian",
    "href": "slides/12-gnn.html#degree-matrix-and-graph-laplacian",
    "title": "Attention and Graph Neural Networks",
    "section": "Degree matrix and Graph Laplacian",
    "text": "Degree matrix and Graph Laplacian\n\nThe degree matrix \\(\\boldsymbol{D} \\in \\mathbb{R}^{n \\times n}\\) is a diagonal matrix where \\(D_{ii}\\) is the number of edges connected to node \\(v_i\\), also called the degree of \\(v_i\\).\nThe Laplacian matrix (or the graph Laplacian) is defined as \\(\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{A}\\).\nFor example, the degree matrix and the Laplacian matrix for the previous graphs are: \\[\n\\boldsymbol{D} = \\begin{bmatrix}\n3 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{bmatrix} \\quad \\text{and} \\quad\n\\boldsymbol{L} = \\begin{bmatrix}\n3 & -1 & -1 & -1 \\\\\n-1 & 2 & -1 & 0 \\\\\n-1 & -1 & 3 & -1 \\\\\n-1 & 0 & -1 & 2\n\\end{bmatrix}.\n\\]\nTo balance the influence of heavy nodes (nodes with large degree), we can use the normalized Laplacian matrix: \\[\n\\boldsymbol{L} = \\boldsymbol{I} - \\boldsymbol{D}^{-1/2} \\boldsymbol{A} \\boldsymbol{D}^{-1/2} =\n\\begin{bmatrix}\n1 & -\\frac{1}{3} & -\\frac{1}{3} & -\\frac{1}{3} \\\\\n-\\frac{1}{2} & 1 & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{3} & -\\frac{1}{3} & 1 & -\\frac{1}{3} \\\\\n-\\frac{1}{2} & 0 & -\\frac{1}{2} & 1\n\\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "slides/12-gnn.html#weighted-graphs",
    "href": "slides/12-gnn.html#weighted-graphs",
    "title": "Attention and Graph Neural Networks",
    "section": "Weighted Graphs",
    "text": "Weighted Graphs\n\n\n\nDepending on the application, we can have a weighted adjacency matrix where \\(A_{ij}\\) is the weight of the edge \\((v_i, v_j)\\).\nFor example, the weighted adjacency matrix of the graph on the right is: \\[\n\\boldsymbol{A} = \\begin{bmatrix}\n0 & 0.9 & -0.5 & -0.6 \\\\\n0.9 & 0 & -0.3 & 0 \\\\\n-0.5 & -0.3 & 0 & 0.2 \\\\\n-0.6 & 0 & 0.2 & 0\n\\end{bmatrix}.\n\\]\nThe weights can be\n\ncorrelation between nodes\ndistance between nodes\n\nThe degree matrix and the Laplacian matrix can be computed similarly."
  },
  {
    "objectID": "slides/12-gnn.html#properties-of-graph-laplacian",
    "href": "slides/12-gnn.html#properties-of-graph-laplacian",
    "title": "Attention and Graph Neural Networks",
    "section": "Properties of Graph Laplacian",
    "text": "Properties of Graph Laplacian\n\nThe Laplacian matrix relates to many useful properties of a graph. In particular, the spectral graph theory relates the properties of a graph to the spectrum (i.e., eigenvalues and eigenvectors) of the Laplacian matrix.\nSome of the most important properties of the Laplacian matrix are:\n\n\\(L\\) is symmetric1 and positive semi-definite.\nThe eigenvalues of \\(L\\) are non-negative.\nThe smallest eigenvalue of \\(L\\) is 0, and the corresponding eigenvector is the all-one vector, since the Laplacian matrix has a zero row sum.\nThe Laplacian matrix is singular.\nThe number of connected components of a graph is equal to the dimension of the null space of the Laplacian matrix.\n\n\nFor directed graphs, we use the symmetrized adjacency matrix \\((\\boldsymbol{A} + \\boldsymbol{A}^{\\top})/2\\) to compute the Laplacian matrix."
  },
  {
    "objectID": "slides/12-gnn.html#examples-of-graph-datasets",
    "href": "slides/12-gnn.html#examples-of-graph-datasets",
    "title": "Attention and Graph Neural Networks",
    "section": "Examples of Graph datasets",
    "text": "Examples of Graph datasets\n\nCommon examples of graph datasets include:\n\nSocial networks: nodes are individuals, and edges are relationships between them.\nCitation networks: nodes are papers, and edges are citations between them.\nCollaboration networks: nodes are authors, and edges are collaborations between them.\n\nIn many applications, nodes have attributes associated with them.\n\nFor example, in a social network, an individual can have attributes such as age, ethnicity, gender, etc.\nIn a citation network, a paper can have attributes such as the title, abstract, authors, etc.\n\nIn these case, the graph is called an attributed graph, \\(G = (V, E, X)\\), where \\(X \\in \\mathbb{R}^{n \\times p}\\) is the attribute matrix, where \\(n\\) is the number of nodes and \\(p\\) is the number of attributes.\nWhen the attributes change dynamically over time, it is called a spatial-temporal graph, denoted \\(G = (V, E, X^{(t)})\\), \\(t = 1, \\ldots, T\\)."
  },
  {
    "objectID": "slides/12-gnn.html#analysis-of-graph-data",
    "href": "slides/12-gnn.html#analysis-of-graph-data",
    "title": "Attention and Graph Neural Networks",
    "section": "Analysis of Graph data",
    "text": "Analysis of Graph data\n\nNode-level analysis:\n\nNode classification/regression: predict the label/attribute of a node.\nNode clustering: group nodes based on their attributes.\n\nEdge-level analysis:\n\nEdge regression: predict strength of an edge.\nLink prediction: predict the existence of an edge between two nodes.\n\nGraph-level analysis:\n\nGraph classification: predict the label of a graph.\nCommunity detection: identify communities in a graph.\n\nIn general, the analysis of graph data can be categorized into:\n\nspectral-based methods: based on the spectral information of the graph, i.e., the graph Laplacian.\nspatial-based methods: operate directly on the node attributes without transforming them to the spectral domain."
  },
  {
    "objectID": "slides/12-gnn.html#example-social-media-content-recommendation",
    "href": "slides/12-gnn.html#example-social-media-content-recommendation",
    "title": "Attention and Graph Neural Networks",
    "section": "Example: Social Media Content Recommendation",
    "text": "Example: Social Media Content Recommendation\n\nGraph construction:\n\nNodes represent users.\nEdges represent relationships or interactions (friendship, follows, likes, comments, etc.).\nNode attributes: user profiles, interests, demographics, etc.\n\nSocial Network Analysis Metrics: derive useful metrics to inform recommendations.\n\nCentrality: Identifies influential users (e.g., degree centrality, betweenness centrality).\nCommunity Detection: Identifies clusters of users who are closely connected.\nHomophily: Identifies similarities between users (e.g., interests, demographics).\nLink Prediction: Determines potential future interactions between users.\n\nRecommendation algorithms: Collaborative Filtering\n\nUser-based: If users A and B have similar preferences, you can recommend posts liked by B to A. This can be improved by considering the network neighbors of A.\nContent-based: If users who liked item X (e.g., a post or product) also liked item Y, the system can recommend item Y to users who liked item X."
  },
  {
    "objectID": "slides/12-gnn.html#graph-convolution",
    "href": "slides/12-gnn.html#graph-convolution",
    "title": "Attention and Graph Neural Networks",
    "section": "Graph Convolution",
    "text": "Graph Convolution\n\nA graph convolution can be seen as a generalization of the convolution operation to graph data:\n\nIn an image, each pixel is taken to be a node.\nAn edge connects two neighboring pixels.\n\n\n\n\n\n\n\n\n\nImage source: Figure 1 in Wu et al. (2021)."
  },
  {
    "objectID": "slides/12-gnn.html#convgnn-for-node-classification",
    "href": "slides/12-gnn.html#convgnn-for-node-classification",
    "title": "Attention and Graph Neural Networks",
    "section": "ConvGNN for Node Classification",
    "text": "ConvGNN for Node Classification\n\n\n\n\n\n\nThe Gconv is the graph convolution. By choosing different convolution filters, we can have different types of graph convolutions.\nThe nonlinear activation function ReLU is applied to the output attributes element-wise.\n\n\n\nImage source: Figure 2 (a) in Wu et al. (2021)."
  },
  {
    "objectID": "slides/12-gnn.html#spatial-based-convolution",
    "href": "slides/12-gnn.html#spatial-based-convolution",
    "title": "Attention and Graph Neural Networks",
    "section": "Spatial-based Convolution",
    "text": "Spatial-based Convolution\n\nAnalogous to the convolutional operation of a conventional CNN on an image, spatial-based methods define graph convolutions based on a node’s spatial relations.\nThe spatial-based graph convolutions convolve the central node’s representation with its neighbors’ representations to derive the updated representation for the central node.\nFor example, the NN4G1 used \\[\n\\mathbf{H}^{(k)}=f\\left(\\mathbf{X} \\mathbf{W}^{(k)}+\\sum_{i=1}^{k-1} \\mathbf{A} \\mathbf{H}^{(i)} \\mathbf{\\Theta}^{(k, i)}\\right)\n\\] where\n\n\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) is the input feature matrix,\n\\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is the adjacency matrix,\n\\(\\mathbf{H}^{(k)} \\in \\mathbb{R}^{n \\times p}\\) is the output feature matrix,\n\\(\\mathbf{W}^{(k)} \\in \\mathbb{R}^{p \\times p}\\) and \\(\\mathbf{\\Theta}^{(k, i)} \\in \\mathbb{R}^{p \\times p}\\) are the weight matrices,\n\\(f\\) is the activation function, e.g., ReLU.\n\n\nA. Micheli (2009). “Neural Network for Graphs: A Contextual Constructive Approach,” in IEEE Transactions on Neural Networks, vol. 20, no. 3, pp. 498-511."
  },
  {
    "objectID": "slides/12-gnn.html#spectral-based-convolution",
    "href": "slides/12-gnn.html#spectral-based-convolution",
    "title": "Attention and Graph Neural Networks",
    "section": "Spectral-based Convolution",
    "text": "Spectral-based Convolution\n\nThe spectral-based convolution are based on the spectral information of the nodes.\nLet \\(G = (V, E, \\boldsymbol{X})\\) be an attributed graph, \\(\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}\\) and \\(\\boldsymbol{L}\\) be the normalized Laplacian matrix.\nLet \\(\\boldsymbol{L} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^\\top\\) be the eigendecomposition of \\(\\boldsymbol{L}\\), where \\(\\boldsymbol{U} \\in \\mathbb{R}^{n \\times n}\\) is the matrix of eigenvectors and \\(\\boldsymbol{\\Lambda} \\in \\mathbb{R}^{n \\times n}\\) is the diagonal matrix of eigenvalues.\nThe attribute matrix \\(\\boldsymbol{X}\\) can be transformed to the spectral domain as \\(\\widetilde{\\boldsymbol{X}} = \\mathcal{F}(\\boldsymbol{X}) = \\boldsymbol{U}^\\top \\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}\\), which is called the graph Fourier transform.\nThe inverse graph Fourier transform is \\(\\boldsymbol{X} = \\mathcal{F}^{-1}(\\widetilde{\\boldsymbol{X}}) = \\boldsymbol{U} \\widetilde{\\boldsymbol{X}}\\).\nThe spectral convolution for a feature \\(\\boldsymbol{x} \\in \\mathbb{R}^{n}\\) is defined as: \\[\n\\boldsymbol{x} \\star_G \\boldsymbol{g} = \\mathcal{F}^{-1}(\\mathcal{F}(\\boldsymbol{x}) \\odot \\mathcal{F}(\\boldsymbol{g})) = \\boldsymbol{U} (\\boldsymbol{U}^\\top \\boldsymbol{x} \\odot \\boldsymbol{U}^\\top \\boldsymbol{g})\n\\] where \\(\\boldsymbol{g} \\in \\mathbb{R}^n\\) is the convolution filter and \\(\\odot\\) is the element-wise product.\nDenote \\(\\boldsymbol{g}_{\\theta} = \\text{diag}(\\boldsymbol{U}^{\\top}\\boldsymbol{g})\\), the spectral convolution can be written as: \\[\n\\boldsymbol{x} \\star_G \\boldsymbol{g} = \\boldsymbol{U} \\boldsymbol{g}_{\\theta} \\boldsymbol{U}^\\top \\boldsymbol{x}.\n\\]"
  },
  {
    "objectID": "slides/12-gnn.html#convgnn-for-graph-classification",
    "href": "slides/12-gnn.html#convgnn-for-graph-classification",
    "title": "Attention and Graph Neural Networks",
    "section": "ConvGNN for Graph Classification",
    "text": "ConvGNN for Graph Classification\n\n\n\n\n\n\n\nImage source: Figure 2 (b) in Wu et al. (2021)."
  },
  {
    "objectID": "slides/12-gnn.html#graph-pooling",
    "href": "slides/12-gnn.html#graph-pooling",
    "title": "Attention and Graph Neural Networks",
    "section": "Graph Pooling",
    "text": "Graph Pooling\n\nGraph pooling is used to reduce the size of the graph while preserving the important information.\nThere are many ways to perform graph pooling, e.g., selecting the top-\\(k\\) nodes, clustering nodes, etc.\nThe Readout permutation-invariant graph operation that outputs a fixed-length representation of graphs.\n\n\n\n\n\n\n\n\nImage source: Figure 1 in Liu et al. (2023) Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities"
  },
  {
    "objectID": "slides/12-gnn.html#graph-autoencoder-gae",
    "href": "slides/12-gnn.html#graph-autoencoder-gae",
    "title": "Attention and Graph Neural Networks",
    "section": "Graph Autoencoder (GAE)",
    "text": "Graph Autoencoder (GAE)\n\n\n\n\n\n\nGAEs are deep neural architectures that map nodes into a latent feature space and decode graph information from latent representations.\nGAEs can be used to learn network embeddings or generate new graphs\n\n\n\nImage source: Figure 2 (c) in Wu et al. (2021)."
  },
  {
    "objectID": "slides/12-gnn.html#graph-generation",
    "href": "slides/12-gnn.html#graph-generation",
    "title": "Attention and Graph Neural Networks",
    "section": "Graph Generation",
    "text": "Graph Generation\n\nGraph generative models are used to generate new graphs that are similar to the training graphs.\nAll the generative models we have seen so far (e.g., VAE, GAN, diffusion model, and transformer) can be modified to generate graphs.\nAlphaFold, developed by Google DeepMind, is a deep generative model for graphs.\nCo-founder and CEO of Google DeepMind and Isomorphic Labs Sir Demis Hassabis, and Google DeepMind Director Dr. John Jumper were co-awarded the 2024 Nobel Prize in Chemistry for their work developing AlphaFold.\nProtein structure as a graph:\n\nnodes are amino acids,\nedges are interactions between them."
  },
  {
    "objectID": "slides/12-gnn.html#spatial-temporal-gnn-stgnn",
    "href": "slides/12-gnn.html#spatial-temporal-gnn-stgnn",
    "title": "Attention and Graph Neural Networks",
    "section": "Spatial-Temporal GNN (STGNN)",
    "text": "Spatial-Temporal GNN (STGNN)\n\n\n\n\n\n\nA graph convolutional layer is followed by a 1-D-CNN layer.\nThe graph convolutional layer operates on \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{X}^{(t)}\\) to capture the spatial dependence.\nThe 1-D-CNN layer slides over \\(\\boldsymbol{X}\\) along the time axis to capture the temporal dependence.\n\n\n\nImage source: Figure 2 (d) in Wu et al. (2021)."
  },
  {
    "objectID": "slides/12-gnn.html#references",
    "href": "slides/12-gnn.html#references",
    "title": "Attention and Graph Neural Networks",
    "section": "References",
    "text": "References\nAttention and Transformer\n\nVaswani et al. (2017) Attention is all you need. In NeurIPS.\nDevlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nCordonnier et al. (2020) On the relationship between self-attention and convolutional layers. In ICLR.\nAttention? Attention! by Lilian Weng (2018)\nThe Transformer Family by Lilian Weng (2020)\nThe Transformer Family Version 2.0 by Lilian Weng (2023)\n\nGraph Neural Networks\n\nWu et al. (2021) A Comprehensive Survey on Graph Neural Networks.\n\nDatasets\n\nStanford Large Network Dataset Collection: https://snap.stanford.edu\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/03-ml_basics.html#recap-of-the-last-lecture",
    "href": "slides/03-ml_basics.html#recap-of-the-last-lecture",
    "title": "Machine Learning Basics",
    "section": "Recap of the Last Lecture",
    "text": "Recap of the Last Lecture\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nRelationship between likelihood and loss function\n\nNormal likelihood \\(\\leftrightarrow\\) squared error loss\nMultinomial/Binomial likelihood \\(\\leftrightarrow\\) cross-entropy loss\n\nPenalization/Regularization: \\(L_1\\) and \\(L_2\\) regularization\nLink function: a function that connects the conditional mean \\(\\E(Y \\mid \\boldsymbol{x})\\) and the linear predictor \\(\\boldsymbol{x}^T \\boldsymbol{\\beta}\\):\n\nReal-valued response: identity link \\(\\E(Y \\mid \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}\\)\nBinary response: logit link \\(\\E(Y \\mid \\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}\\)\nMultinomial response: softmax link \\[\n\\E(Y \\mid \\boldsymbol{x}) = \\left[\\frac{\\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta_1})}{\\sum_{k=1}^K \\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta}_k)}, \\ldots, \\frac{\\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta_K})}{\\sum_{k=1}^K \\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta}_k)}\\right]^T\n\\]"
  },
  {
    "objectID": "slides/03-ml_basics.html#outline",
    "href": "slides/03-ml_basics.html#outline",
    "title": "Machine Learning Basics",
    "section": "Outline",
    "text": "Outline\n\nEmpirical Risk Minimization (ERM)\n\nA general framework for machine learning\nDecomposition of the generalization error of a model\n\nVapnik-Chervonenkis (VC) Theory\n\nMeasuring the complexity of a set of models\nProviding an upper bound for the generalization error\n\nValidation of a trained model\n\nEstimating the generalization error\n\\(k\\)-fold cross-validation\nCross-validation for hyperparameter tuning"
  },
  {
    "objectID": "slides/03-ml_basics.html#different-types-of-learning",
    "href": "slides/03-ml_basics.html#different-types-of-learning",
    "title": "Machine Learning Basics",
    "section": "Different Types of Learning",
    "text": "Different Types of Learning\nThere are many types of learning:\n\nSupervised Learning\nUnsupervised Learning\nReinforcement Learning\nSemi-supervised Learning\nActive Learning\nOnline Learning\nTransfer Learning\nMulti-task Learning\nFederated Learning, etc."
  },
  {
    "objectID": "slides/03-ml_basics.html#supervised-learning",
    "href": "slides/03-ml_basics.html#supervised-learning",
    "title": "Machine Learning Basics",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nThe dataset consists of pairs \\((x_i, y_i)\\), \\(x_i \\in \\mathcal{X}\\), \\(y_i \\in \\mathcal{Y}\\), where \\(x_i\\) is called the feature and \\(y_i\\) is the associated label.\n\n\\(\\mathcal{X} \\subseteq \\R^p\\) is called the feature space (usually \\(\\mathcal{X} = \\R^p\\))\n\\(\\mathcal{Y} \\subseteq \\R^K\\) is called the label space\n\nThe goal is to learn a function \\(f: \\mathcal{X} \\to \\mathcal{Y}\\) that maps the feature to the label.\nExamples:\n\nimage/text classification\nprediction\n\nCommonly used models:\n\nLinear regression/Logistic regression\nSupport vector machine (SVM)\nNeural network, and many others"
  },
  {
    "objectID": "slides/03-ml_basics.html#general-framework-of-supervised-learning",
    "href": "slides/03-ml_basics.html#general-framework-of-supervised-learning",
    "title": "Machine Learning Basics",
    "section": "General Framework of Supervised Learning",
    "text": "General Framework of Supervised Learning\n\nIn this course, we will mainly focus on supervised learning.\nSupervised learning can also be viewed as a function estimation problem, i.e., estimating the function \\(f\\) that maps the feature \\(x\\) to the label \\(y\\).\nDepending the types of labels, many different models have been developed.\nInstead of focusing on individual models, we will discuss a general framework for supervised learning, called Empirical Risk Minimization (ERM)."
  },
  {
    "objectID": "slides/03-ml_basics.html#empirical-risk-minimization-erm",
    "href": "slides/03-ml_basics.html#empirical-risk-minimization-erm",
    "title": "Machine Learning Basics",
    "section": "Empirical Risk Minimization (ERM)",
    "text": "Empirical Risk Minimization (ERM)\n\nThe ERM principle for supervised learning requires:\n\nA loss function \\(L(y, g(x))\\) that measures the discrepancy between the true label \\(y\\) and the predicted label \\(g(x)\\).\nA hypothesis class \\(\\mathcal{G}\\) which is a class of functions \\(g: \\mathcal{X} \\to \\mathcal{Y}\\).\nA training dataset \\((x_1, y_1), \\ldots, (x_n, y_n)\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#loss-function",
    "href": "slides/03-ml_basics.html#loss-function",
    "title": "Machine Learning Basics",
    "section": "Loss function",
    "text": "Loss function\n\nA loss function \\(L: \\mathcal{Y} \\times \\R^K \\to \\R\\) quantifies how well \\(\\hat{y}\\) approximates \\(y\\):\n\nsmaller values of \\(L(y, \\hat{y}\\)) indicate that \\(\\hat{y}\\) is a good approximation of \\(y\\)\ntypically (but not always) \\(L(y, y) = 0\\) and \\(L(y, \\hat{y}) \\geq 0\\) for all \\(\\hat{y}\\), and \\(y\\)\n\nExamples:\n\nQuadratic loss: \\(L(y, \\hat{y}) = (y - \\hat{y})^2\\) or \\(L(y, \\hat{y}) = \\|y - \\hat{y}\\|^2\\)\nAbsolute loss: \\(L(y, \\hat{y}) = |y - \\hat{y}|\\)\nCross-Entropy loss: \\(L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\\) or \\(L(y, \\hat{y}) = -\\sum_{i=1}^K y_i\\log\\hat{y}_i\\)"
  },
  {
    "objectID": "slides/03-ml_basics.html#risk-function",
    "href": "slides/03-ml_basics.html#risk-function",
    "title": "Machine Learning Basics",
    "section": "Risk Function",
    "text": "Risk Function\n\nAssume that \\((X, Y) \\sim F\\) and \\(F\\) is an unknown distribution.\nGiven a loss function ,the risk function of a model \\(h\\) is \\[\nR(h) = \\E_{(X, Y) \\sim F}[L(Y, h(X))].\n\\]\nThe optimal \\(h\\) is the one that minimizes the risk function: \\[\nh^{\\star} = \\argmin_{h: \\mathcal{X} \\to \\mathcal{Y}} R(h).\n\\]\nDenote the optimal risk as \\(R^{\\star} = R(h^{\\star})\\).\nHowever, it is impossible to obtain either \\(h^{\\star}\\) or \\(R^{\\star}\\) because:\n\nthe space of all possible functions \\(\\{h: \\mathcal{X} \\to \\mathcal{Y}\\}\\) is too large, and\nthe data distribution \\(F\\) is unknown."
  },
  {
    "objectID": "slides/03-ml_basics.html#hypothesis-class",
    "href": "slides/03-ml_basics.html#hypothesis-class",
    "title": "Machine Learning Basics",
    "section": "Hypothesis Class",
    "text": "Hypothesis Class\n\nTo make the problem tractable, we restrict the space of functions to a hypothesis class \\(\\mathcal{H}\\).\nWe denote the best model in \\(\\mathcal{H}\\) as \\(h_{\\mathcal{H}}^{\\star} = \\argmin_{h \\in \\mathcal{H}} R(h)\\).\nIts associated risk is \\(R_{\\mathcal{H}}^{\\star} = R(h_{\\mathcal{H}}^{\\star})\\).\nBy definition, it is obvious that \\(R_{\\mathcal{H}}^{\\star} \\geq R^{\\star}\\).\nExamples:\n\nLinear regression: \\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\} = \\R^p\\)\nLogistic regression: \\(\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\} = \\R^p\\)\n\nThe difference between \\(R_{\\mathcal{H}}^{\\star}\\) and \\(R^{\\star}\\) is called the approximation error.\nIntuitively, the larger the hypothesis class, the smaller the approximation error."
  },
  {
    "objectID": "slides/03-ml_basics.html#empirical-risk",
    "href": "slides/03-ml_basics.html#empirical-risk",
    "title": "Machine Learning Basics",
    "section": "Empirical Risk",
    "text": "Empirical Risk\n\nAssuming that \\((x_1, y_1), \\ldots, (x_n, y_n) \\iid F\\), the empirical risk is \\[\nR_{\\text{emp}}(h) = \\E_{(X, Y) \\sim \\widehat{F}_n}[L(Y, h(X))]\n        = \\frac{1}{n} \\sum_{i=1}^n L(y_i, h(x_i))\n\\] where \\(\\widehat{F}_n = \\frac{1}{n}\\sum_{i=1}^n \\delta_{(x_i, y_i)}\\) is the empirical distribution of the data.\nWe choose the \\(h\\) that minimizes the empirical risk function, i.e., the empirical risk minimizer: \\[\n\\hat{h}_{n, \\mathcal{H}} = \\argmin_{h \\in \\mathcal{H}} R_{\\text{emp}}(h)\n\\] where \\(\\mathcal{H}\\) is the hypothesis class.\nDenote the empirical risk associated with \\(\\hat{h}_{n, \\mathcal{H}}\\) as \\(\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})\\) and this is what we obtain in practice."
  },
  {
    "objectID": "slides/03-ml_basics.html#quick-summary",
    "href": "slides/03-ml_basics.html#quick-summary",
    "title": "Machine Learning Basics",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nGoal: find the best model \\(h^{\\star} = \\argmin_h R(h)\\), which is impossible since\n\nthe space of all possible functions is too large \\(\\textcolor{red}{\\rightarrow}\\) restrict to hypothesis class\nthe data distribution is unknown \\(\\textcolor{red}{\\rightarrow}\\) use empirical data\n\nWe have three models:\n\n\\(h^{\\star}\\): the best model (associated risk \\(R^{\\star} = R(h^{\\star})\\))\n\\(h_{\\mathcal{H}}^{\\star}\\): the best model in the hypothesis class \\(\\mathcal{H}\\) (associated risk \\(R_{\\mathcal{H}}^{\\star} = R(h^{\\star}_{\\mathcal{H}})\\))\n\\(\\hat{h}_{n, \\mathcal{H}}\\): the empirical risk minimizer, i.e., the trained model (empirical risk, or the training error, \\(\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})\\))\n\nWe want \\(\\hat{h}_{n, \\mathcal{H}}\\) to be as close as possible to \\(h^{\\star}\\) in terms of the risk function \\(R(h)\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#error-decomposition",
    "href": "slides/03-ml_basics.html#error-decomposition",
    "title": "Machine Learning Basics",
    "section": "Error Decomposition",
    "text": "Error Decomposition\n\nGoal: \\(R(\\hat{h}_{n,\\mathcal{H}}) - R^{\\star} = R(\\hat{h}_{n,\\mathcal{H}}) - R(h^{\\star}) = 0\\).\nDecomposition: \\[\\begin{align*}\nR(\\hat{h}_{n,\\mathcal{H}}) - R^{\\star} & = \\underbrace{R(\\hat{h}_{n,\\mathcal{H}}) - R_{\\mathcal{H}}^{\\star}}_{\\textcolor{blue}{\\text{estimation error}}} \\quad + \\underbrace{R_{\\mathcal{H}}^{\\star} - R^{\\star}}_{\\textcolor{blue}{\\text{approximation error}}}\\\\\n& = \\underbrace{R(\\hat{h}_{n,\\mathcal{H}}) - R(h_{\\mathcal{H}}^{\\star})}_{\\textcolor{blue}{\\text{estimation error}}} \\quad + \\quad \\underbrace{R(h_{\\mathcal{H}}^{\\star}) - R(h^{\\star})}_{\\textcolor{blue}{\\text{approximation error}}}\\\\\n\\end{align*}\\]\nThe approximation error comes from the use of a hypothesis class \\(\\mathcal{H}\\).\n\nLarger \\(\\mathcal{H}\\) \\(\\rightarrow\\) smaller approximation error\n\nThe estimation error comes from the use of empirical data.\n\nMore data \\(\\rightarrow\\) smaller estimation error"
  },
  {
    "objectID": "slides/03-ml_basics.html#error-decomposition-1",
    "href": "slides/03-ml_basics.html#error-decomposition-1",
    "title": "Machine Learning Basics",
    "section": "Error Decomposition",
    "text": "Error Decomposition"
  },
  {
    "objectID": "slides/03-ml_basics.html#example",
    "href": "slides/03-ml_basics.html#example",
    "title": "Machine Learning Basics",
    "section": "Example",
    "text": "Example\n\nLinear Regression:\n\n\\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}\\)\n\\(L(y, h(\\boldsymbol{x})) = (y - h(\\boldsymbol{x}))^2\\)\n\nLogistic Regression:\n\n\\(\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\}\\)\n\\(L(y, h(\\boldsymbol{x})) = -y \\log(h(\\boldsymbol{x})) - (1 - y) \\log(1 - h(\\boldsymbol{x}))\\)\n\n(Linear) Support Vector Machine:\n\n\\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{w}^T \\boldsymbol{x} + b, \\boldsymbol{w} \\in \\R^p, b \\in \\R\\}\\)\n\\(L(y, h(\\boldsymbol{x})) = \\max(0, 1 - y \\cdot h(\\boldsymbol{x}))\\)"
  },
  {
    "objectID": "slides/03-ml_basics.html#maximum-likelihood-ml-v.s.-erm",
    "href": "slides/03-ml_basics.html#maximum-likelihood-ml-v.s.-erm",
    "title": "Machine Learning Basics",
    "section": "Maximum Likelihood (ML) v.s. ERM",
    "text": "Maximum Likelihood (ML) v.s. ERM\n\nIn fact, the ML principle is a special case of the ERM principle.\nThat is, specifying a likelihood function gives a loss function, i.e., use the negative log-likelihood as the loss function.\nML:\n\nStronger assumptions\nStronger guarantees (consistency, asymptotic normality, etc.)\nAllow us to do more things (e.g., hypothesis testing and confidence intervals)\nLinear regression and logistic regression are ML and hence ERM\n\nERM:\n\nMore flexible and practical, but weaker guarantees\nUsually provide only a point estimate\nSVM is ERM but not ML"
  },
  {
    "objectID": "slides/03-ml_basics.html#constructing-learning-algorithms",
    "href": "slides/03-ml_basics.html#constructing-learning-algorithms",
    "title": "Machine Learning Basics",
    "section": "Constructing Learning Algorithms",
    "text": "Constructing Learning Algorithms\n\nFollowing the ERM principle, we need to specify a loss function and a hypothesis class in order to construct a learning algorithm.\nThe choice of the loss function is based on the types of labels and the problem.\nThe choice of the hypothesis class is more challenging:\n\nSmaller \\(\\mathcal{H}\\) \\(\\rightarrow\\) larger approximation error, smaller estimation error, and less overfitting\nLarger \\(\\mathcal{H}\\) \\(\\rightarrow\\) smaller approximation error, larger estimation error, more overfitting, and requires more data\n\nNext, we will discuss:\n\nhow to measure the “size” (capacity/complexity) of a hypothesis class\nhow to choose an appropriate hypothesis class"
  },
  {
    "objectID": "slides/03-ml_basics.html#complexity-v.s.-dimension",
    "href": "slides/03-ml_basics.html#complexity-v.s.-dimension",
    "title": "Machine Learning Basics",
    "section": "Complexity v.s. Dimension",
    "text": "Complexity v.s. Dimension\n\nLet \\(\\mathcal{H}\\) be a parametric hypothesis class ,e.g., \\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}\\).\nAn intuitive way to measure the complexity of \\(\\mathcal{H}\\) is to count the number of unknown parameters, i.e., the dimension of \\(\\mathcal{H}\\).\nIn this case, the dimension of \\(\\dim(\\mathcal{H}) = p\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#shattering",
    "href": "slides/03-ml_basics.html#shattering",
    "title": "Machine Learning Basics",
    "section": "Shattering",
    "text": "Shattering\nA hypothesis class \\(\\mathcal{H}\\) is said to shatter a set of points \\(S = \\{x_1, \\ldots, x_n\\}\\) if for all possible binary labelings (0/1) of these points, there exists a function \\(h \\in \\mathcal{H}\\) that can perfectly separate the points.\n\n\n\nImage Source: Figure 7.6 of ESL"
  },
  {
    "objectID": "slides/03-ml_basics.html#shattering-1",
    "href": "slides/03-ml_basics.html#shattering-1",
    "title": "Machine Learning Basics",
    "section": "Shattering",
    "text": "Shattering\nDefinition (Restriction of \\(\\mathcal{H}\\) to \\(S\\)) Let \\(\\mathcal{H}\\) be a class of functions from \\(\\mathcal{X}\\) to \\(\\{0,1\\}\\) and let \\(S = \\{x_1, \\ldots, x_n\\} \\subset \\mathcal{X}\\). The restriction of \\(\\mathcal{H}\\) to \\(S\\) is the set of functions from \\(S\\) to \\(\\{0, 1\\}\\) that can be derived from \\(\\mathcal{H}\\). That is, \\[\n   \\mathcal{H}_S = \\{(h(x_1), \\ldots, h(x_n)): h \\in \\mathcal{H}\\}\n\\]\nDefinition (Shattering) A hypothesis class \\(\\mathcal{H}\\) shatters a finite set \\(S \\subset \\mathcal{X}\\) if the restriction of \\(\\mathcal{H}\\) to \\(S\\) is the set of all functions from \\(S\\) to \\(\\{0, 1\\}\\). That is, \\(|\\mathcal{H}_S| = 2^{|S|}\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#vapnik-chervonenkis-vc-dimension",
    "href": "slides/03-ml_basics.html#vapnik-chervonenkis-vc-dimension",
    "title": "Machine Learning Basics",
    "section": "Vapnik-Chervonenkis (VC) Dimension",
    "text": "Vapnik-Chervonenkis (VC) Dimension\nDefinition (VC-dimension) The VC-dimension of a hypothesis class \\(\\mathcal{H}\\), denoted \\(\\text{VC-dim}(\\mathcal{H})\\), is the maximal size of a set \\(S \\subset \\mathcal{X}\\) that can be shattered by \\(\\mathcal{H}\\). If \\(\\mathcal{H}\\) can shatter sets of arbitrarily large size we say that \\(\\mathcal{H}\\) has infinite VC-dimension.\n\nOne can show that for linear models, e.g., \\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\beta_0 + \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}\\), the VC-dimension is \\(p+1\\) (the same as the number of parameters).\nHowever, for nonlinear models, the calculation of the VC-dimension is often challenging."
  },
  {
    "objectID": "slides/03-ml_basics.html#example-infinite-vc-dimension",
    "href": "slides/03-ml_basics.html#example-infinite-vc-dimension",
    "title": "Machine Learning Basics",
    "section": "Example (Infinite VC-dimension)",
    "text": "Example (Infinite VC-dimension)\n\nLet \\(\\mathcal{H} = \\{h: h(x) = \\mathbb{I}(\\sin(\\alpha x) &gt; 0), \\alpha &gt; 0\\}\\). Then \\(\\text{VC-dim}(\\mathcal{H}) = \\infty\\).\nProof:\n\nFor any \\(n\\), let \\(x_1 = 2\\pi 10^{-1}, \\ldots, x_n = 2\\pi 10^{-n}\\).\nThen the parameter \\(\\alpha = \\frac{1}{2}\\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right)\\) can perfectly separate the points, where \\(y_i \\in \\{0, 1\\}\\) is any labeling of the points.\n\n\n\n\n\n\n\n\n\nImage Source: Figure 7.5 of ESL"
  },
  {
    "objectID": "slides/03-ml_basics.html#goodness-of-fit-v.s.-generalization-ability",
    "href": "slides/03-ml_basics.html#goodness-of-fit-v.s.-generalization-ability",
    "title": "Machine Learning Basics",
    "section": "Goodness-of-fit v.s. Generalization ability",
    "text": "Goodness-of-fit v.s. Generalization ability\n\nGoodness-of-fit: how well the model fits the data.\nGeneralization ability: how well the model generalizes to unseen data.\nRecall that for an ERM \\(\\hat{h}_{n, \\mathcal{H}}\\), we have\n\ntraining error (error on training data) \\(\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})\\)\ngeneralization error (error on unseen data) \\(R(\\hat{h}_{n, \\mathcal{H}}) = \\E_{(X, Y) \\sim F} \\left[L(Y, \\hat{h}_{n,\\mathcal{H}}(X)) \\mid \\mathcal{T}\\right]\\), where \\(\\mathcal{T}\\) denotes the training dataset.\n\nWe can write \\[\\begin{align*}\n   R(\\hat{h}_{n, \\mathcal{H}}) & = \\hat{R}_n + \\left(R(\\hat{h}_{n, \\mathcal{H}}) - \\hat{R}_n\\right)\\\\\n   \\textcolor{blue}{\\text{generalization error}} & = \\textcolor{blue}{\\text{training error}} + \\textcolor{blue}{\\text{generalization gap}}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/03-ml_basics.html#overfitting-and-underfitting",
    "href": "slides/03-ml_basics.html#overfitting-and-underfitting",
    "title": "Machine Learning Basics",
    "section": "Overfitting and Underfitting",
    "text": "Overfitting and Underfitting\n\nTo have low generalization error, we need to have both low training error and small generalization gap.\n\nLarge training error \\(\\rightarrow\\) underfitting\nLow training error but large generalization gap \\(\\rightarrow\\) overfitting\n\n\n\n\n\n\n\n\n\nImage Source: Figure 5.3 in DL"
  },
  {
    "objectID": "slides/03-ml_basics.html#vc-inequality",
    "href": "slides/03-ml_basics.html#vc-inequality",
    "title": "Machine Learning Basics",
    "section": "VC Inequality",
    "text": "VC Inequality\n\nThe VC theory provides an upper bound for the generalization gap, known as the VC inequality: with probability at least \\(1 - \\delta\\) \\[\nR(h) \\leq R_{\\text{emp}}(h)+\\varepsilon \\sqrt{1+\\frac{4 R_{\\text{emp}}(h)}{\\varepsilon}}, \\quad \\varepsilon = O\\left(\\frac{d - \\log \\delta}{n}\\right)\n\\] simultaneously for all \\(h \\in \\mathcal{H}\\), where \\(\\text{VC-dim}(\\mathcal{H}) = d &lt; \\infty\\).\nThe generalization gap increases as\n\nthe VC-dimension increases\nthe samples size \\(n\\) decreases\n\nThis upper bound is only a loose bound and does not work for models with infinite VC-dimension."
  },
  {
    "objectID": "slides/03-ml_basics.html#regularized-erm",
    "href": "slides/03-ml_basics.html#regularized-erm",
    "title": "Machine Learning Basics",
    "section": "Regularized ERM",
    "text": "Regularized ERM\n\nTo prevent overfitting, we can add a regularization term to the empirical risk: \\[\nR_{\\text{reg}}(h) = R_{\\text{emp}}(h) + \\lambda \\Omega(h)\n\\] where \\(\\Omega(h)\\) is a regularization term and \\(\\lambda\\) is the regularization parameter.\nTypically, \\(\\Omega(h)\\) measures the smoothness or complexity of the model \\(h\\).\n\n\n\n\n  \n  \n  \n    \n      \n        image/svg+xml\n        \n        \n      \n    \n  \n  \n    \n      \n        \n        \n        \n        \n        \n        \n        \n        \n        \n      \n      \n      x\n      y\n    \n  \n\n\n\n\n\nImage Source: https://en.wikipedia.org/wiki/Regularization_(mathematics)"
  },
  {
    "objectID": "slides/03-ml_basics.html#regularized-erm-1",
    "href": "slides/03-ml_basics.html#regularized-erm-1",
    "title": "Machine Learning Basics",
    "section": "Regularized ERM",
    "text": "Regularized ERM\n\nFor example, \\(\\Omega(h) = \\|h^{\\prime}(x)\\|_2^2 = \\int \\left(h^{\\prime}(x)\\right)^2dx\\). (\\(L_2\\) Regularization)\nIf \\(h(x) = \\beta_0 + \\beta_1 x\\), then \\(h^{\\prime}(x) = \\beta_1\\) and \\(\\Omega(h) = \\beta_1^2\\).\nThe \\(L_1\\) regularization is \\(\\Omega(h) = \\|h^{\\prime}(x)\\|_1 = \\int |h^{\\prime}(x)|dx\\).\nIf \\(h(x) = \\beta_0 + \\beta_1 x\\), then \\(h^{\\prime}(x) = \\beta_1\\) and \\(\\Omega(h) = |\\beta_1|\\).\nUsing \\(L_1\\) gives you sparsity; using \\(L_2\\) gives you smoothness/insensitivity:\n\nConsider a linear model \\(h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}\\).\nA good model should not be too sensitive to the input, i.e., small changes in the input should not lead to large changes in the output.\nThat is, if \\(\\boldsymbol{x} \\approx \\tilde{\\boldsymbol{x}}\\) , then \\(|\\boldsymbol{x}^T \\boldsymbol{\\beta} - \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}|\\) should be small.\nNote that \\[\n|\\boldsymbol{x}^T \\boldsymbol{\\beta} - \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}| = |(\\boldsymbol{x} - \\tilde{\\boldsymbol{x}})^T \\boldsymbol{\\beta}| \\leq \\|\\boldsymbol{x} - \\tilde{\\boldsymbol{x}}\\|_2 \\|\\boldsymbol{\\beta}\\|_2\n\\]"
  },
  {
    "objectID": "slides/03-ml_basics.html#bias-variance-tradeoff",
    "href": "slides/03-ml_basics.html#bias-variance-tradeoff",
    "title": "Machine Learning Basics",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nAdding a reguralization term often increases the bias but reduces the variance of the model.\nThis tradeoff is known as the bias-variance tradeoff.\n\n\n\n\n\n\n\n\nImage Source: Figure 5.6 in DL"
  },
  {
    "objectID": "slides/03-ml_basics.html#quick-summary-1",
    "href": "slides/03-ml_basics.html#quick-summary-1",
    "title": "Machine Learning Basics",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nThe VC dimension measures the complexity of a hypothesis class.\nThe VC inequality provides an upper bound for the generalization gap, provided that the VC dimension is finite.\nThe bound is often criticized for being too loose and does not work for models with infinite VC dimension.\nExample of infinite VC dimension:\n\nNeural Networks\nKernel methods (e.g., kernel SVM, kernel regression)\n\\(K\\)-nearest neighbors (with small \\(K\\), say \\(K = 1\\))"
  },
  {
    "objectID": "slides/03-ml_basics.html#double-descent-curve",
    "href": "slides/03-ml_basics.html#double-descent-curve",
    "title": "Machine Learning Basics",
    "section": "Double Descent Curve",
    "text": "Double Descent Curve\n\n\n\nBelkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32), 15849-15854."
  },
  {
    "objectID": "slides/03-ml_basics.html#estimating-the-generalization-error",
    "href": "slides/03-ml_basics.html#estimating-the-generalization-error",
    "title": "Machine Learning Basics",
    "section": "Estimating the Generalization Error",
    "text": "Estimating the Generalization Error\n\nAlthough the VC inequality provides an upper bound for the generalization gap, it is often too loose.\nIn order to have a more accurate insight into the model’s generalization ability, we need to estimate the generalization error.\nTo achieve this, we need to have an extra dataset, called the validation dataset \\(\\mathcal{V} = \\{(\\tilde{x}_i, \\tilde{y}_i)\\}_{i=1}^m\\).\nThe generalization error is then estimated as \\[\n\\hat{R}_{\\text{gen}} = \\frac{1}{m} \\sum_{i=1}^m L(\\tilde{y}_i, \\hat{h}_{n, \\mathcal{H}}(\\tilde{x}_i)).\n\\]\nAssuming the \\(\\mathcal{V}\\) and \\(\\mathcal{T}\\) (training dataset) are i.i.d, \\(\\hat{R}_{\\text{gen}}\\) is an unbiased estimate of the generalization error, i.e., \\(\\E[\\hat{R}_{\\text{gen}} \\mid \\mathcal{T}] = R(\\hat{h}_{n, \\mathcal{H}})\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#k-fold-cross-validation-cv",
    "href": "slides/03-ml_basics.html#k-fold-cross-validation-cv",
    "title": "Machine Learning Basics",
    "section": "\\(k\\)-fold Cross-Validation (CV)",
    "text": "\\(k\\)-fold Cross-Validation (CV)\n\nIn practice, we often do not have an extra validation dataset and hence we need to use the training dataset to estimate the generalization error.\nOne common method is the \\(k\\)-fold cross-validation:\n\nSplit the training dataset \\(\\mathcal{T}\\) into \\(k\\) equal-sized folds.\nFor each fold \\(i = 1, \\ldots, k\\), train the model on the remaining \\(k-1\\) folds and evaluate the model on the \\(i\\)th fold.\nAverage the \\(k\\) validation errors to obtain the estimated generalization error."
  },
  {
    "objectID": "slides/03-ml_basics.html#k-fold-cross-validation",
    "href": "slides/03-ml_basics.html#k-fold-cross-validation",
    "title": "Machine Learning Basics",
    "section": "\\(k\\)-fold Cross-Validation",
    "text": "\\(k\\)-fold Cross-Validation\n\nWhen \\(k = n\\), it is called the leave-one-out cross-validation (LOOCV), i.e., train the model on \\(n-1\\) samples and evaluate on the remaining one.\nChoice of \\(k\\)?\n\nLarger \\(k\\) \\(\\rightarrow\\) low bias, high variance (the model is trained on a larger dataset and validated on a smaller dataset)\nSmaller \\(k\\) \\(\\rightarrow\\) high bias, low variance (the model is trained on a smaller dataset and validated on a larger dataset)\n\\(k = 5\\) or \\(k = 10\\) are common choices."
  },
  {
    "objectID": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning",
    "href": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning",
    "title": "Machine Learning Basics",
    "section": "CV for Hyperparameter Tuning",
    "text": "CV for Hyperparameter Tuning\n\nIn practice, the models often have hyperparameters that need to be tuned, e.g., the regularization parameter \\(\\lambda\\).\nWe can use CV to choose the best hyperparameters:\n\nFor each hyperparameter value, perform \\(k\\)-fold CV to estimate the generalization error.\nChoose the hyperparameter value that minimizes the CV error.\n\nHowever, the CV error after the selection will overestimate the generalization error.\nSuch bias is known as the selection bias."
  },
  {
    "objectID": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning-1",
    "href": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning-1",
    "title": "Machine Learning Basics",
    "section": "CV for Hyperparameter Tuning",
    "text": "CV for Hyperparameter Tuning\n\nTo avoid the selection bias, we first split the dataset into two parts: the training dataset and the test dataset.\nThe test dataset should not be used in the neither the traing process nor hyperparameter tuning process.\nThe training dataset is further split into \\(k\\)-folds for CV.\nAfter all the processes, including training, hyperparameter tuning, model selection, etc., we evaluate the final model on the test dataset to estimate the generalization error."
  },
  {
    "objectID": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter",
    "href": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter",
    "title": "Machine Learning Basics",
    "section": "Example: Using CV to Choose the Regularization Parameter",
    "text": "Example: Using CV to Choose the Regularization Parameter\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import datasets\n\nX, y = datasets.load_diabetes(return_X_y=True)\n\nX /= X.std(axis=0)\n\nalpha_seq = np.logspace(-2, 2, 100)\nreg = LassoCV(alphas = alpha_seq, cv = 5, random_state = 42)\nreg.fit(X, y)\n\nprint(\"best alpha:\", np.round(reg.alpha_, 4))\n\nbest alpha: 0.0774"
  },
  {
    "objectID": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter-1",
    "href": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter-1",
    "title": "Machine Learning Basics",
    "section": "Example: Using CV to Choose the Regularization Parameter",
    "text": "Example: Using CV to Choose the Regularization Parameter"
  },
  {
    "objectID": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension",
    "href": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension",
    "title": "Machine Learning Basics",
    "section": "Proof of Example (Infinite VC-dimension)",
    "text": "Proof of Example (Infinite VC-dimension)\n\nWe need to show that the model \\(h(x) = \\mathbb{I}(\\sin(\\alpha x) &gt; 0)\\) with \\(\\alpha = \\frac{1}{2}\\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right)\\) can perfectly separate the \\(n\\) points.\nConsider the \\(j\\)th sample \\(x_j = 2\\pi 10^{-j}\\).\nIf \\(y_j = 0\\), then \\[\\begin{align*}\n   \\alpha x_j & = \\pi 10^{-j} \\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right) = \\pi 10^{-j} \\left(1 + \\sum_{\\{i: y_i = 0\\}} 10^i\\right)\\\\\n   & = \\pi 10^{-j}\\left(1 + 10^j + \\sum_{\\{i: y_i = 0, i \\neq j\\}} 10^i\\right)\\\\\n   & = \\pi \\left(10^{-j} + 1 + \\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j}\\right)\n\\end{align*}\\]\n\n\n\nReference: https://mlweb.loria.fr/book/en/VCdiminfinite.html"
  },
  {
    "objectID": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-1",
    "href": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-1",
    "title": "Machine Learning Basics",
    "section": "Proof of Example (Infinite VC-dimension)",
    "text": "Proof of Example (Infinite VC-dimension)\n\nFor \\(i&gt;j\\), \\(10^{i-j}\\) is even and so is \\(\\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j}\\), say \\[\n\\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j} = 2m, \\quad m \\in \\mathbb{N}.\n\\]\nNote that \\[\n\\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j} &lt; \\sum_{i=1}^{\\infty} 10^{-i} = \\sum_{i=0}^{\\infty} 10^{-i} - 1 = \\frac{1}{1-0.1} - 1 = \\frac{1}{9}.  \n\\]\nTherefore, \\(\\alpha x_j = \\pi(1 + 2m +\\epsilon)\\), where \\[\n0 &lt; \\epsilon = 10^{-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j} &lt; \\frac{1}{10} + \\frac{1}{9} &lt; 1.\n\\]\nHence \\(\\sin(\\alpha x_j) &lt; 0\\) and \\(h(x_j) = 0\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-2",
    "href": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-2",
    "title": "Machine Learning Basics",
    "section": "Proof of Example (Infinite VC-dimension)",
    "text": "Proof of Example (Infinite VC-dimension)\n\nIf \\(y_j = 1\\), then \\[\\begin{align*}\n   \\alpha x_j & = \\pi 10^{-j} \\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right) = \\pi 10^{-j} \\left(1 + \\sum_{\\{i: y_i = 0\\}} 10^i\\right)\\\\\n   & = \\pi \\left(10^{-j} + \\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j}\\right)\n\\end{align*}\\]\nSimilarly, we have \\(\\alpha x_j = \\pi(2m +\\epsilon)\\), where \\[\n0 &lt; \\epsilon = 10^{-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j} &lt; \\frac{1}{10} + \\frac{1}{9} &lt; 1.\n\\]\nHence \\(\\sin(\\alpha x_j) &gt; 0\\) and \\(h(x_j) = 1\\).\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/08-cnn.html#outline",
    "href": "slides/08-cnn.html#outline",
    "title": "Convolutional Neural Networks",
    "section": "Outline",
    "text": "Outline\n\n$$\n% trace % Variance % MSE % MSE % Covariance\n% Choose\n% Small choose \n$$\n\n\nConvolution\n\nGaussian Filter\nCross-Correlation and Auto-Correlation\nEdge Detection\n\nBasic CNN Architecture\n\nPadding and Stride\nPooling\nAlexNet\n\nApplications\n\nObject Detection\nImage Segmentation"
  },
  {
    "objectID": "slides/08-cnn.html#convolutional-networks",
    "href": "slides/08-cnn.html#convolutional-networks",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Networks",
    "text": "Convolutional Networks\n\nConvolutional networks, also known as convolutional neural networks, or CNNs, are a specialized kind of neural network for processing data that has a known grid-like topology.\nExamples:\n\ntime-series data: 1-D grid taking samples at regular time intervals\nimages: 2-D grid of pixels\n3-D images: 3-D grid of voxels\nvideo: 3-D grid = 2-D of pixels + 1-D of time\n\nConvolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers."
  },
  {
    "objectID": "slides/08-cnn.html#convolution-operation",
    "href": "slides/08-cnn.html#convolution-operation",
    "title": "Convolutional Neural Networks",
    "section": "Convolution Operation",
    "text": "Convolution Operation\n\nGiven two functions \\(f(t)\\) and \\(g(t)\\), the convolution of \\(f\\) and \\(g\\) is defined as: \\[\n(f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau.\n\\]\nOne way to think about convolution is as a weighted average of the function \\(f\\) at time \\(t\\) where the weights are given by the function \\(g\\).\nIn practice, we work with discrete data. That is, we only observe \\(f(t)\\) for a finite set of values of \\(t\\), e.g., \\(t \\in \\mathbb{Z}\\).\nIn this case, the convolution is defined as: \\[\n(f * g)(t) = \\sum_{\\tau=-\\infty}^{\\infty} f(\\tau)g(t-\\tau).\n\\]\nThe function \\(f\\) is often called the input and the function \\(g\\) is called the kernel or filter."
  },
  {
    "objectID": "slides/08-cnn.html#example",
    "href": "slides/08-cnn.html#example",
    "title": "Convolutional Neural Networks",
    "section": "Example",
    "text": "Example\n\nLet \\(f(t) = [1, 2, 3, 4]\\) and \\(g(t) = [1, 1, 1]\\). That is,\n\n\\(f(0) = 1, f(1) = 2, f(2) = 3, f(3) = 4\\), and \\(f(t) = 0\\) for all other values of \\(t\\)\n\\(g(0) = 1, g(1) = 1, g(2) = 1\\), and \\(g(t) = 0\\) for all other values of \\(t\\).\n\nThe convolution of \\(f\\) and \\(g\\) is: \\[\\begin{align*}\n(f * g)(t) & = \\sum_{\\tau=-\\infty}^{\\infty} f(\\tau)g(t-\\tau) = f(0)g(t) + f(1)g(t-1) + f(2)g(t-2) + f(3)g(t-3).\n\\end{align*}\\]\nTherefore \\[\\begin{align*}\n  (f * g)(0) & = f(0)g(0) + f(1)g(-1) + f(2)g(-2) + f(3)g(-3)\n  = 1 \\times 1 + 2 \\times 0 + 3 \\times 0 + 4 \\times 0 = 1\\\\\n  (f * g)(1) & = f(0)g(1) + f(1)g(0) + f(2)g(-1) + f(3)g(-2)\n  = 1 \\times 1 + 2 \\times 1 + 3 \\times 0 + 4 \\times 0 = 3\\\\\n  (f * g)(2) & = f(0)g(2) + f(1)g(1) + f(2)g(0) + f(3)g(-1)\n  = 1 \\times 1 + 2 \\times 1 + 3 \\times 1 + 4 \\times 0 = 6\n  \\end{align*}\\]\nThat is, \\((f * g)(t) = [1, 3, 6, 9, 7, 4]\\).\nIf we convolve again with \\(g\\), we get \\(((f * g) * g)(t) = [1, 4, 10, 18, 22, 20, 11, 4]\\)."
  },
  {
    "objectID": "slides/08-cnn.html#gaussian-filter",
    "href": "slides/08-cnn.html#gaussian-filter",
    "title": "Convolutional Neural Networks",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\nA Gaussian filter is the convolution with \\(g(t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{t^2}{2\\sigma^2}}\\).\nRoughly speaking, the Gaussian filter smooths the function \\(f\\) by computing the weighted average the values of \\(f\\) in a neighborhood of \\(t\\).\nThe parameter \\(\\sigma\\) controls the width of the Gaussian filter and hence the smoothness of the output."
  },
  {
    "objectID": "slides/08-cnn.html#cross-correlation-and-auto-correlation",
    "href": "slides/08-cnn.html#cross-correlation-and-auto-correlation",
    "title": "Convolutional Neural Networks",
    "section": "Cross-Correlation and Auto-Correlation",
    "text": "Cross-Correlation and Auto-Correlation\n\nThe cross-correlation of \\(f\\) and \\(g\\) is similar to convolution. It is defined as: \\[\n(f \\star g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t+\\tau)d\\tau.\n\\]\nIt is used to measure the similarity between two signals.\nThe autocorrelation of a function \\(f\\) is the cross-correlation of \\(f\\) with itself: \\[\n(f \\star f)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)f(t+\\tau)d\\tau.\n\\]\nThe autocorrelation of a function is a measure of how similar the function is to a shifted version of itself."
  },
  {
    "objectID": "slides/08-cnn.html#applications-of-convolution",
    "href": "slides/08-cnn.html#applications-of-convolution",
    "title": "Convolutional Neural Networks",
    "section": "Applications of convolution",
    "text": "Applications of convolution\n\nIn signal processing, convolution is used to filter signals, extract features, and analyze time or frequency characteristics.\nExamples include:\n\nNoise Reduction: Removes unwanted components from signals.\nImage Sharpening: Enhances edges and detail.\nFeature Detection: Identifies patterns like edges in images or specific frequencies in audio.\n\nIn statistics, convolution is used to model probability distributions and smooth data.\nExamples::\n\nThe density of the sum of two independent random variables is the convolution of the two density functions.\nKernel Density Estimation: Smooths data to estimate probability densities."
  },
  {
    "objectID": "slides/08-cnn.html#multi-dimensional-convolution",
    "href": "slides/08-cnn.html#multi-dimensional-convolution",
    "title": "Convolutional Neural Networks",
    "section": "Multi-dimensional Convolution",
    "text": "Multi-dimensional Convolution\n\nIn higher dimensions, the convolution of two functions \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) and \\(g: \\mathbb{R}^d \\to \\mathbb{R}\\) is defined as: \\[\n(f * g)(\\boldsymbol{t}) = \\int_{\\mathbb{R}^d} f(\\boldsymbol{\\tau})g(\\boldsymbol{t}-\\boldsymbol{\\tau})d\\boldsymbol{\\tau}.\n\\]\nFor discrete observations (e.g., images), the convolution is defined as: \\[\n(f * g)(\\boldsymbol{t}) = \\sum_{\\boldsymbol{\\tau} \\in \\mathbb{Z}^d} f(\\boldsymbol{\\tau})g(\\boldsymbol{t}-\\boldsymbol{\\tau}).\n\\]\nFor \\(d = 2\\), \\[\n(f * g)(t_1, t_2) = \\sum_{\\tau_1=-\\infty}^{\\infty}\\sum_{\\tau_2=-\\infty}^{\\infty} f(\\tau_1, \\tau_2)g(t_1-\\tau_1, t_2-\\tau_2).\n\\]\nThe cross-correlation and autocorrelation are defined similarly."
  },
  {
    "objectID": "slides/08-cnn.html#d-convolution",
    "href": "slides/08-cnn.html#d-convolution",
    "title": "Convolutional Neural Networks",
    "section": "2-D Convolution",
    "text": "2-D Convolution\n\nFor images, the input \\(I\\) is a 2D grid of pixel values.\nThe convolutional kernel \\(K\\) is a 2D grid of weights.\nThe kernel size is typically \\(3 \\times 3\\) or \\(5\\times 5\\).\n\nSmaller kernels are computationally efficient and can capture fine details."
  },
  {
    "objectID": "slides/08-cnn.html#image-size-after-convolution",
    "href": "slides/08-cnn.html#image-size-after-convolution",
    "title": "Convolutional Neural Networks",
    "section": "Image size after convolution",
    "text": "Image size after convolution\n\nSuppose the size of the input image is \\(H_{\\text{in}} \\times W_{\\text{in}}\\) and the size of the kernel is \\(k_h \\times k_w\\).\nThe stride (i.e., the step size) is \\((s_h, s_w)\\).\nThe size of the output image is: \\[\nH_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} - k_h}{s_h} + 1 \\right\\rfloor, \\quad W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} - k_w}{s_w} + 1 \\right\\rfloor.\n\\]\nIn the previous example, the input image is \\(7 \\times 7\\) and the kernel size is \\(3 \\times 3\\) with stride \\((1, 1)\\). Therefore the output image is \\(5 \\times 5\\).\nThe image size can be preserved by using padding, i.e., adding zeros around the input image."
  },
  {
    "objectID": "slides/08-cnn.html#d-gaussian-filter",
    "href": "slides/08-cnn.html#d-gaussian-filter",
    "title": "Convolutional Neural Networks",
    "section": "2-D Gaussian Filter",
    "text": "2-D Gaussian Filter\nApply a 2D Gaussian filter to an image to smooth it. The filter is defined as:\n\nfrom scipy import datasets\nfrom scipy.ndimage import gaussian_filter\n\norig = datasets.ascent()\n\n# Apply Gaussian filters\nresult1 = gaussian_filter(orig, sigma=3)\nresult2 = gaussian_filter(orig, sigma=10)"
  },
  {
    "objectID": "slides/08-cnn.html#filter-for-edge-detection",
    "href": "slides/08-cnn.html#filter-for-edge-detection",
    "title": "Convolutional Neural Networks",
    "section": "Filter for edge detection",
    "text": "Filter for edge detection\n\nWe can also use convolution for edge detection: \\(k_h\\) is for detecting horizontal edges and \\(k_v\\) is for detecting vertical edges \\[\nk_{h} = \\begin{bmatrix} -1 \\\\ 0\\\\ 1 \\end{bmatrix}, \\quad k_{v} = \\begin{bmatrix} -1 & 0 & 1 \\end{bmatrix}\n\\]\nIn fact, the filter compute the approximate gradient of the image in the horizontal and vertical directions: \\[\\begin{align*}\n\\frac{\\partial}{\\partial x} I(x, y) &\\approx \\frac{1}{2}\\left[I(x+1, y) - I(x-1, y)\\right] = \\frac{1}{2}[-1, 0, 1] \\cdot [I(x-1, y), I(x, y), I(x+1, y)] \\\\\n\\frac{\\partial}{\\partial y} I(x, y) &\\approx \\frac{1}{2}\\left[I(x, y+1) - I(x, y-1)\\right] = \\frac{1}{2}\\begin{bmatrix} -1 \\\\ 0\\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} I(x, y-1) \\\\ I(x, y) \\\\ I(x, y+1) \\end{bmatrix}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-cnn.html#sobel-filter",
    "href": "slides/08-cnn.html#sobel-filter",
    "title": "Convolutional Neural Networks",
    "section": "Sobel Filter",
    "text": "Sobel Filter\n\nThe Sobel filter is a combination of gradient filters and smoothing filters, \\[\\begin{align*}\nk_{v} & = \\begin{bmatrix} 1 \\\\ 2\\\\ 1 \\end{bmatrix} \\begin{bmatrix} -1 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}\\\\\nk_h & = \\begin{bmatrix} -1 \\\\ 0\\\\ 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2 & 1 \\end{bmatrix} = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\\\\n\\end{align*}\\]\nThat is, if we want to find vertical edges, we add a vertical smoothing filter and vice versa."
  },
  {
    "objectID": "slides/08-cnn.html#high-pass-filter-and-low-pass-filter",
    "href": "slides/08-cnn.html#high-pass-filter-and-low-pass-filter",
    "title": "Convolutional Neural Networks",
    "section": "High-pass filter and low-pass filter",
    "text": "High-pass filter and low-pass filter\n\nWe can roughly classify filters into two categories:\n\nHigh-pass filters: remove low-frequency components and preserve high-frequency components.\nLow-pass filters: remove high-frequency components and preserve low-frequency components.\n\nHigh-pass filters keep fine details and sharpen images, e.g., the Sobel filter.\nLow-pass filters smooth images and remove noise, e.g., the Gaussian filter.\nFilter size (kernel size) also affects the filter’s behavior:\n\nSmall filters capture fine details but are sensitive to noise.\nLarge filters smooth images but may blur fine details."
  },
  {
    "objectID": "slides/08-cnn.html#quick-summary",
    "href": "slides/08-cnn.html#quick-summary",
    "title": "Convolutional Neural Networks",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nIn image processing, convolution is the operation of applying a filter to an image.\nFilters are used to:\n\nBlur: Smooth images by averaging pixel values.\nSharpen: Enhance edges and detail.\nEdge Detection: Identify edges in images.\nNoise Reduction: Remove unwanted noise from images.\nFeature Extraction: Detect specific patterns or features in images.\n\nTherefore, we can choose different filters to achieve different effects."
  },
  {
    "objectID": "slides/08-cnn.html#motivation-of-convolution",
    "href": "slides/08-cnn.html#motivation-of-convolution",
    "title": "Convolutional Neural Networks",
    "section": "Motivation of convolution",
    "text": "Motivation of convolution\nThere are several reasons to used convolution operations in neural networks:\n\nEase of Interpretation: convolutional has long been used in signal processing and image processing.\nTaking account of spatial structure: Convolutional networks are designed to take advantage of the spatial structure of images.\nParameter Sharing: A feature detector (e.g., edge detector) that’s useful in one part of the image is likely useful in another part.\nSparsity of Connections: In each layer, each output value depends only on a small number of inputs.\nTranslation Invariance: A feature learned in one part of the image can be applied to other parts.\nEfficient Computation: Convolution can be implemented efficiently using matrix multiplication."
  },
  {
    "objectID": "slides/08-cnn.html#convolution-layer",
    "href": "slides/08-cnn.html#convolution-layer",
    "title": "Convolutional Neural Networks",
    "section": "Convolution layer",
    "text": "Convolution layer\n\n\nIn PyTorch, the convolutional layer is defined as torch.nn.Conv2d and it requires the following parameters:\n\nin_channels: number of input channels (e.g., 3 for RGB images).\nout_channels: number of output channels (i.e., number of filters).\nkernel_size: size of the convolutional kernel (e.g., 3 for a \\(3 \\times 3\\) kernel).\nstride: step size for the kernel.\npadding: number of zeros to add around the input image.\nbias: whether to include a bias term.\ndilation: spacing between kernel elements (\\(\\text{default} = 1\\) means no dilation).\n\n\n\n\n\n\n\n\n\n\nImage sourse: https://github.com/vdumoulin/conv_arithmetic/tree/master"
  },
  {
    "objectID": "slides/08-cnn.html#pooling",
    "href": "slides/08-cnn.html#pooling",
    "title": "Convolutional Neural Networks",
    "section": "Pooling",
    "text": "Pooling\n\nPooling layer is another important layer in CNNs.\nIt has several benefits:\n\nDownsampling: Reduces the spatial dimensions of the feature map.\nReduces Overfitting: By summarizing the features, it reduces the number of parameters.\nTranslation Invariance: Makes the network more robust to small translations in the input.\n\nThe most common pooling operation are max pooling and average pooling."
  },
  {
    "objectID": "slides/08-cnn.html#max-pooling-and-average-pooling",
    "href": "slides/08-cnn.html#max-pooling-and-average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max Pooling and Average Pooling",
    "text": "Max Pooling and Average Pooling\n\nMax pooling takes the maximum value in a patch.\n\n\nm = nn.MaxPool2d(kernel_size = 2, stride = 1)\ninput = torch.randn(1, 4, 4)\nprint(input)\noutput = m(input)\nprint(output)\n\ntensor([[[-1.1195, -2.3749, -0.4088,  0.7293],\n         [ 0.2177,  2.2577,  1.3515, -2.0341],\n         [ 0.2819,  0.7313,  0.3734, -0.6688],\n         [-0.3654,  0.5156, -2.4215,  0.4078]]])\ntensor([[[2.2577, 2.2577, 1.3515],\n         [2.2577, 2.2577, 1.3515],\n         [0.7313, 0.7313, 0.4078]]])\n\n\n\nSimilarly, average pooling takes the average value in a patch."
  },
  {
    "objectID": "slides/08-cnn.html#adaptive-pooling",
    "href": "slides/08-cnn.html#adaptive-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Adaptive Pooling",
    "text": "Adaptive Pooling\nIn PyTorch, adaptive pooling (AdaptiveAvgPool or AdaptiveMaxPool) allows you to specify the output size instead of the kernel size.\n\nAdaptiveAvgPool1d\n\n\nm = nn.AdaptiveAvgPool1d(output_size=8)\nx = torch.tensor([[1,2,3]], dtype = torch.float32)\nm(x)\n\ntensor([[1.0000, 1.0000, 1.5000, 2.0000, 2.0000, 2.5000, 3.0000, 3.0000]])\n\n\n\nAdaptiveMaxPool1d\n\n\nm = nn.AdaptiveMaxPool1d(output_size=10)\nx = torch.tensor([[1,2,3]], dtype = torch.float32)\nm(x)\n\ntensor([[1., 1., 1., 2., 2., 2., 3., 3., 3., 3.]])\n\n\n\nThis layer presents us from hard-coding the feature sizes."
  },
  {
    "objectID": "slides/08-cnn.html#how-does-it-work-for-1-d-case",
    "href": "slides/08-cnn.html#how-does-it-work-for-1-d-case",
    "title": "Convolutional Neural Networks",
    "section": "How does it work? (for 1-D case)",
    "text": "How does it work? (for 1-D case)\n\nThe output size for a convolution/pooling layer is: \\[\nL_{\\text{out}} = \\left\\lfloor \\frac{L_{\\text{in}} + 2\\times\\text{padding} - \\text{dilation} \\times (\\text{kernel size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor.\n\\]\nIf \\(L_{\\text{out}} \\leq L_{\\text{in}}\\), define\n\n\\(\\text{padding} = 0\\), \\(\\text{dilation} = 1\\)\n\\(\\text{stride} = \\left\\lfloor \\frac{L_{\\text{in}} - L_{\\text{out}}}{L_{\\text{out}}} + 1\\right\\rfloor\\)\n\\(\\text{kernel size} = L_{\\text{in}} - (L_{\\text{out}} - 1)\\times\\text{stride}\\)\n\nIf \\(L_{\\text{out}} &gt; L_{\\text{in}}\\), upsample the input by the factor \\(\\left\\lceil \\frac{L_{\\text{out}}}{L_{\\text{in}}}\\right\\rceil\\), e.g., if \\(L_{\\text{in}} = 3\\) and \\(L_{\\text{out}} = 5\\), upsample the input by a factor of \\(2\\): \\[\n[1,2,3] \\Rightarrow [1,1,2,2,3,3]\n\\] and then apply the previous case."
  },
  {
    "objectID": "slides/08-cnn.html#cnn-architecture",
    "href": "slides/08-cnn.html#cnn-architecture",
    "title": "Convolutional Neural Networks",
    "section": "CNN Architecture",
    "text": "CNN Architecture\nA typical CNN architecture consists of two parts:\n\nFeature Extraction: Consists of convolutional layers and activation functions.\n\nConvolutional Layers: Extract features from the input image.\nActivation Function: Introduce non-linearity into the model.\nPooling Layers: Downsample the feature maps.\n\nClassification (or regression): Consists of fully connected layers.\n\nFully Connected Layers: Perform classification based on the features.\nActivation Function\nDropout: Regularize the model to prevent overfitting.\nOutput Layer: softmax layer for classification or linear layer for regression."
  },
  {
    "objectID": "slides/08-cnn.html#example-1",
    "href": "slides/08-cnn.html#example-1",
    "title": "Convolutional Neural Networks",
    "section": "Example",
    "text": "Example\n\nclass MyConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feature = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveMaxPool2d(output_size=(4, 4)),\n            nn.Flatten()\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(32 * 4 * 4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )   \n    def forward(self, x):\n        x = self.feature(x)\n        x = self.classifier(x)\n        return x"
  },
  {
    "objectID": "slides/08-cnn.html#alexnet",
    "href": "slides/08-cnn.html#alexnet",
    "title": "Convolutional Neural Networks",
    "section": "AlexNet",
    "text": "AlexNet\n\nAlexNet1 is a popular CNN architecture that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012.\nIt consists of 5 convolutional layers and 3 fully connected layers.\n\n\n\n\n\n\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems (NeurIPS), 25."
  },
  {
    "objectID": "slides/08-cnn.html#computer-vision",
    "href": "slides/08-cnn.html#computer-vision",
    "title": "Convolutional Neural Networks",
    "section": "Computer Vision",
    "text": "Computer Vision\n\nComputer Vision is a field of computer science that works on enabling computers to see, identify, and process images in the same way that human vision does, and then provide appropriate output.\nTypical tasks in computer vision include:\n\nImage/video Classification\nObject Detection\nSegmentation\nReconstruction\nPose estimation, etc."
  },
  {
    "objectID": "slides/08-cnn.html#classical-cv-vs.-dl",
    "href": "slides/08-cnn.html#classical-cv-vs.-dl",
    "title": "Convolutional Neural Networks",
    "section": "Classical CV vs. DL",
    "text": "Classical CV vs. DL\n\nClassical CV requires hand-crafted features:\n\n\n\n\n\n\nflowchart LR\n  A(Image) ---&gt;|Feature Extraction| B(Feature Vector) ---&gt;|Classifier| C(Output)\n\n\n\n\n\n\n\nFor example, we can use the Scale-invariant feature transform (SIFT) algorithm to extract features from an image and use a classifier (e.g., SVM or random forest) to classify the image.\nUsually, the feature extraction step depends on the problem.\nIn contrast, DL solves problems in an end-to-end manner, that is, it uses one model to simultaneously learn the feature extraction and classification steps.\n\n\n\n\n\n\nflowchart LR\n  A(Image) ---&gt;|DL Models| B(Output)\n\n\n\n\n\n\n\nOf course, the model architecture depends on the problem and compare to classical CV, DL approaches requires more data."
  },
  {
    "objectID": "slides/08-cnn.html#imagenet-database",
    "href": "slides/08-cnn.html#imagenet-database",
    "title": "Convolutional Neural Networks",
    "section": "ImageNet Database",
    "text": "ImageNet Database\n\nImageNet is a large-scale hierarchical image database containing over 14 million images hand-annotated with 20,000 categories.\nThe dataset is used in ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n\n\n\n\n\n\n\n\nImage Source: Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009, June). Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition (pp. 248-255)."
  },
  {
    "objectID": "slides/08-cnn.html#imagenet-database-1",
    "href": "slides/08-cnn.html#imagenet-database-1",
    "title": "Convolutional Neural Networks",
    "section": "ImageNet Database",
    "text": "ImageNet Database\n\nIt is developed to support visual recognition tasks in computer vision.\n\nModel Development: Most models in torchvision are pretrained on ImageNet.\nTransfer Learning: Use the pretrained models to solve other tasks, e.g., object detection, segmentation, etc."
  },
  {
    "objectID": "slides/08-cnn.html#other-public-datasets",
    "href": "slides/08-cnn.html#other-public-datasets",
    "title": "Convolutional Neural Networks",
    "section": "Other public datasets",
    "text": "Other public datasets\n\nImage classification:\n\nCIFAR-10 and CIFAR-100: Small-scale datasets for image classification with 10 or 100 categories; popular for educational purposes.\nMNIST: Handwritten digits (0-9) dataset; ideal for simple classification and neural network testing.\n\nObject detection/segmentation:\n\nCOCO (Common Objects in Context): Over 330,000 images with 80 object categories; supports object detection, segmentation, and keypoint detection.\nOpen Images v7: Large dataset with 9 million images and bounding boxes for 600 categories; supports object detection and visual relationship detection."
  },
  {
    "objectID": "slides/08-cnn.html#other-public-datasets-1",
    "href": "slides/08-cnn.html#other-public-datasets-1",
    "title": "Convolutional Neural Networks",
    "section": "Other public datasets",
    "text": "Other public datasets\n\nFor autonomous driving:\n\nCityscapes: Urban scene dataset with high-quality pixel annotations for 30 classes, used for autonomous driving.\nCamVid: Road scene dataset with pixel-level annotations, commonly used in autonomous vehicle research.\nKITTI: Dataset for autonomous driving with images, lidar, and GPS data; supports object detection, tracking, and stereo vision."
  },
  {
    "objectID": "slides/08-cnn.html#object-localization",
    "href": "slides/08-cnn.html#object-localization",
    "title": "Convolutional Neural Networks",
    "section": "Object Localization",
    "text": "Object Localization\n\nThe goal is to identify and classify objects in an image by drawing bounding boxes around them.\nDefine bounding box with four values:\n\nTop-left corner coordinates \\((x, y)\\)\nBox width and height \\((w, h)\\)\n\nIt can be treated as a regression task:\n\nInput: Image\nOutput: Bounding box coordinates \\((x, y, w, h)\\).\n\nIf we have multiple objects in an image, we can predict multiple bounding boxes with class labels."
  },
  {
    "objectID": "slides/08-cnn.html#region-based-cnns",
    "href": "slides/08-cnn.html#region-based-cnns",
    "title": "Convolutional Neural Networks",
    "section": "Region-based CNNs",
    "text": "Region-based CNNs\n\nThe region-based CNNs (R-CNN)1 family of models are used for object detection and localization.\nThe key idea is to use a region proposal algorithm to generate candidate regions in an image and then use a CNN to classify the regions.\nThe Faster R-CNN2 model improves the speed of R-CNN by sharing computation between the region proposal and classification.\n\n\n\n\n\n\nGirshick, R., Donahue, J., Darrell, T., & Malik, J. (2015). Region-based convolutional networks for accurate object detection and segmentation. *IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(1), 142-158.Ren, S., He, K., Girshick, R., & Sun, J. (2016). Faster R-CNN: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6), 1137-1149."
  },
  {
    "objectID": "slides/08-cnn.html#image-segmentation",
    "href": "slides/08-cnn.html#image-segmentation",
    "title": "Convolutional Neural Networks",
    "section": "Image Segmentation",
    "text": "Image Segmentation\n\nImage segmentation is the process of partitioning an image into multiple segments (sets of pixels) to simplify the representation of an image.\nIt can be achieved using both unsupervised and supervised methods.\nUnsupervised Segmentation: cluster pixels based on color, intensity, or texture.\nSupervised Segmentation: use labeled data to train a model to segment images."
  },
  {
    "objectID": "slides/08-cnn.html#u-netunet",
    "href": "slides/08-cnn.html#u-netunet",
    "title": "Convolutional Neural Networks",
    "section": "U-Net1",
    "text": "U-Net1\n\n\n\n\n\nRonneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In MICCAI."
  },
  {
    "objectID": "slides/08-cnn.html#model-zoos",
    "href": "slides/08-cnn.html#model-zoos",
    "title": "Convolutional Neural Networks",
    "section": "Model zoos",
    "text": "Model zoos\n\nYou can find a huge collection of model architectures and pretrained models, for example,\n\ntorchvision: PyTorch’s official model zoo with pretrained models for image classification, object detection, segmentation, etc.\ntimm: PyTorch Image Models (timm) library with various model architectures and pretrained models.\n\nNetwork backbone vs. head:\n\nBackbone: Convolutional layers for feature extraction.\nHead: Final few layers for classification, detection, or segmentation.\n\nStrategies:\n\nuse a pretrained backbone and add your custom head for specific tasks\nfine-tune the entire model (backbone and head)\n\nDepending on the size of your new dataset, you can choose the strategy that best fits your needs.\n\n\n\n\nHome"
  }
]