[
  {
    "objectID": "slides/03-ml_basics.html#recap-of-the-last-lecture",
    "href": "slides/03-ml_basics.html#recap-of-the-last-lecture",
    "title": "Machine Learning Basics",
    "section": "Recap of the Last Lecture",
    "text": "Recap of the Last Lecture\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nRelationship between likelihood and loss function\n\nNormal likelihood \\(\\leftrightarrow\\) squared error loss\nMultinomial/Binomial likelihood \\(\\leftrightarrow\\) cross-entropy loss\n\nPenalization/Regularization: \\(L_1\\) and \\(L_2\\) regularization\nLink function: a function that connects the conditional mean \\(\\E(Y \\mid \\boldsymbol{x})\\) and the linear predictor \\(\\boldsymbol{x}^T \\boldsymbol{\\beta}\\):\n\nReal-valued response: identity link \\(\\E(Y \\mid \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}\\)\nBinary response: logit link \\(\\E(Y \\mid \\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}\\)\nMultinomial response: softmax link \\[\n\\E(Y \\mid \\boldsymbol{x}) = \\left[\\frac{\\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta_1})}{\\sum_{k=1}^K \\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta}_k)}, \\ldots, \\frac{\\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta_K})}{\\sum_{k=1}^K \\exp(\\boldsymbol{x}^T \\boldsymbol{\\beta}_k)}\\right]^T\n\\]"
  },
  {
    "objectID": "slides/03-ml_basics.html#outline",
    "href": "slides/03-ml_basics.html#outline",
    "title": "Machine Learning Basics",
    "section": "Outline",
    "text": "Outline\n\nEmpirical Risk Minimization (ERM)\n\nA general framework for machine learning\nDecomposition of the generalization error of a model\n\nVapnik-Chervonenkis (VC) Theory\n\nMeasuring the complexity of a set of models\nProviding an upper bound for the generalization error\n\nValidation of a trained model\n\nEstimating the generalization error\n\\(k\\)-fold cross-validation\nCross-validation for hyperparameter tuning"
  },
  {
    "objectID": "slides/03-ml_basics.html#different-types-of-learning",
    "href": "slides/03-ml_basics.html#different-types-of-learning",
    "title": "Machine Learning Basics",
    "section": "Different Types of Learning",
    "text": "Different Types of Learning\nThere are many types of learning:\n\nSupervised Learning\nUnsupervised Learning\nReinforcement Learning\nSemi-supervised Learning\nActive Learning\nOnline Learning\nTransfer Learning\nMulti-task Learning\nFederated Learning, etc."
  },
  {
    "objectID": "slides/03-ml_basics.html#supervised-learning",
    "href": "slides/03-ml_basics.html#supervised-learning",
    "title": "Machine Learning Basics",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nThe dataset consists of pairs \\((x_i, y_i)\\), \\(x_i \\in \\mathcal{X}\\), \\(y_i \\in \\mathcal{Y}\\), where \\(x_i\\) is called the feature and \\(y_i\\) is the associated label.\n\n\\(\\mathcal{X} \\subseteq \\R^p\\) is called the feature space (usually \\(\\mathcal{X} = \\R^p\\))\n\\(\\mathcal{Y} \\subseteq \\R^K\\) is called the label space\n\nThe goal is to learn a function \\(f: \\mathcal{X} \\to \\mathcal{Y}\\) that maps the feature to the label.\nExamples:\n\nimage/text classification\nprediction\n\nCommonly used models:\n\nLinear regression/Logistic regression\nSupport vector machine (SVM)\nNeural network, and many others"
  },
  {
    "objectID": "slides/03-ml_basics.html#general-framework-of-supervised-learning",
    "href": "slides/03-ml_basics.html#general-framework-of-supervised-learning",
    "title": "Machine Learning Basics",
    "section": "General Framework of Supervised Learning",
    "text": "General Framework of Supervised Learning\n\nIn this course, we will mainly focus on supervised learning.\nSupervised learning can also be viewed as a function estimation problem, i.e., estimating the function \\(f\\) that maps the feature \\(x\\) to the label \\(y\\).\nDepending the types of labels, many different models have been developed.\nInstead of focusing on individual models, we will discuss a general framework for supervised learning, called Empirical Risk Minimization (ERM)."
  },
  {
    "objectID": "slides/03-ml_basics.html#empirical-risk-minimization-erm",
    "href": "slides/03-ml_basics.html#empirical-risk-minimization-erm",
    "title": "Machine Learning Basics",
    "section": "Empirical Risk Minimization (ERM)",
    "text": "Empirical Risk Minimization (ERM)\n\nThe ERM principle for supervised learning requires:\n\nA loss function \\(L(y, g(x))\\) that measures the discrepancy between the true label \\(y\\) and the predicted label \\(g(x)\\).\nA hypothesis class \\(\\mathcal{G}\\) which is a class of functions \\(g: \\mathcal{X} \\to \\mathcal{Y}\\).\nA training dataset \\((x_1, y_1), \\ldots, (x_n, y_n)\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#loss-function",
    "href": "slides/03-ml_basics.html#loss-function",
    "title": "Machine Learning Basics",
    "section": "Loss function",
    "text": "Loss function\n\nA loss function \\(L: \\mathcal{Y} \\times \\R^K \\to \\R\\) quantifies how well \\(\\hat{y}\\) approximates \\(y\\):\n\nsmaller values of \\(L(y, \\hat{y}\\)) indicate that \\(\\hat{y}\\) is a good approximation of \\(y\\)\ntypically (but not always) \\(L(y, y) = 0\\) and \\(L(y, \\hat{y}) \\geq 0\\) for all \\(\\hat{y}\\), and \\(y\\)\n\nExamples:\n\nQuadratic loss: \\(L(y, \\hat{y}) = (y - \\hat{y})^2\\) or \\(L(y, \\hat{y}) = \\|y - \\hat{y}\\|^2\\)\nAbsolute loss: \\(L(y, \\hat{y}) = |y - \\hat{y}|\\)\nCross-Entropy loss: \\(L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\\) or \\(L(y, \\hat{y}) = -\\sum_{i=1}^K y_i\\log\\hat{y}_i\\)"
  },
  {
    "objectID": "slides/03-ml_basics.html#risk-function",
    "href": "slides/03-ml_basics.html#risk-function",
    "title": "Machine Learning Basics",
    "section": "Risk Function",
    "text": "Risk Function\n\nAssume that \\((X, Y) \\sim F\\) and \\(F\\) is an unknown distribution.\nGiven a loss function ,the risk function of a model \\(h\\) is \\[\nR(h) = \\E_{(X, Y) \\sim F}[L(Y, h(X))].\n\\]\nThe optimal \\(h\\) is the one that minimizes the risk function: \\[\nh^{\\star} = \\argmin_{h: \\mathcal{X} \\to \\mathcal{Y}} R(h).\n\\]\nDenote the optimal risk as \\(R^{\\star} = R(h^{\\star})\\).\nHowever, it is impossible to obtain either \\(h^{\\star}\\) or \\(R^{\\star}\\) because:\n\nthe space of all possible functions \\(\\{h: \\mathcal{X} \\to \\mathcal{Y}\\}\\) is too large, and\nthe data distribution \\(F\\) is unknown."
  },
  {
    "objectID": "slides/03-ml_basics.html#hypothesis-class",
    "href": "slides/03-ml_basics.html#hypothesis-class",
    "title": "Machine Learning Basics",
    "section": "Hypothesis Class",
    "text": "Hypothesis Class\n\nTo make the problem tractable, we restrict the space of functions to a hypothesis class \\(\\mathcal{H}\\).\nWe denote the best model in \\(\\mathcal{H}\\) as \\(h_{\\mathcal{H}}^{\\star} = \\argmin_{h \\in \\mathcal{H}} R(h)\\).\nIts associated risk is \\(R_{\\mathcal{H}}^{\\star} = R(h_{\\mathcal{H}}^{\\star})\\).\nBy definition, it is obvious that \\(R_{\\mathcal{H}}^{\\star} \\geq R^{\\star}\\).\nExamples:\n\nLinear regression: \\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\} = \\R^p\\)\nLogistic regression: \\(\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\} = \\R^p\\)\n\nThe difference between \\(R_{\\mathcal{H}}^{\\star}\\) and \\(R^{\\star}\\) is called the approximation error.\nIntuitively, the larger the hypothesis class, the smaller the approximation error."
  },
  {
    "objectID": "slides/03-ml_basics.html#empirical-risk",
    "href": "slides/03-ml_basics.html#empirical-risk",
    "title": "Machine Learning Basics",
    "section": "Empirical Risk",
    "text": "Empirical Risk\n\nAssuming that \\((x_1, y_1), \\ldots, (x_n, y_n) \\iid F\\), the empirical risk is \\[\nR_{\\text{emp}}(h) = \\E_{(X, Y) \\sim \\widehat{F}_n}[L(Y, h(X))]\n        = \\frac{1}{n} \\sum_{i=1}^n L(y_i, h(x_i))\n\\] where \\(\\widehat{F}_n = \\frac{1}{n}\\sum_{i=1}^n \\delta_{(x_i, y_i)}\\) is the empirical distribution of the data.\nWe choose the \\(h\\) that minimizes the empirical risk function, i.e., the empirical risk minimizer: \\[\n\\hat{h}_{n, \\mathcal{H}} = \\argmin_{h \\in \\mathcal{H}} R_{\\text{emp}}(h)\n\\] where \\(\\mathcal{H}\\) is the hypothesis class.\nDenote the empirical risk associated with \\(\\hat{h}_{n, \\mathcal{H}}\\) as \\(\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})\\) and this is what we obtain in practice."
  },
  {
    "objectID": "slides/03-ml_basics.html#quick-summary",
    "href": "slides/03-ml_basics.html#quick-summary",
    "title": "Machine Learning Basics",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nGoal: find the best model \\(h^{\\star} = \\argmin_h R(h)\\), which is impossible since\n\nthe space of all possible functions is too large \\(\\textcolor{red}{\\rightarrow}\\) restrict to hypothesis class\nthe data distribution is unknown \\(\\textcolor{red}{\\rightarrow}\\) use empirical data\n\nWe have three models:\n\n\\(h^{\\star}\\): the best model (associated risk \\(R^{\\star} = R(h^{\\star})\\))\n\\(h_{\\mathcal{H}}^{\\star}\\): the best model in the hypothesis class \\(\\mathcal{H}\\) (associated risk \\(R_{\\mathcal{H}}^{\\star} = R(h^{\\star}_{\\mathcal{H}})\\))\n\\(\\hat{h}_{n, \\mathcal{H}}\\): the empirical risk minimizer, i.e., the trained model (empirical risk, or the training error, \\(\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})\\))\n\nWe want \\(\\hat{h}_{n, \\mathcal{H}}\\) to be as close as possible to \\(h^{\\star}\\) in terms of the risk function \\(R(h)\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#error-decomposition",
    "href": "slides/03-ml_basics.html#error-decomposition",
    "title": "Machine Learning Basics",
    "section": "Error Decomposition",
    "text": "Error Decomposition\n\nGoal: \\(R(\\hat{h}_{n,\\mathcal{H}}) - R^{\\star} = R(\\hat{h}_{n,\\mathcal{H}}) - R(h^{\\star}) = 0\\).\nDecomposition: \\[\\begin{align*}\nR(\\hat{h}_{n,\\mathcal{H}}) - R^{\\star} & = \\underbrace{R(\\hat{h}_{n,\\mathcal{H}}) - R_{\\mathcal{H}}^{\\star}}_{\\textcolor{blue}{\\text{estimation error}}} \\quad + \\underbrace{R_{\\mathcal{H}}^{\\star} - R^{\\star}}_{\\textcolor{blue}{\\text{approximation error}}}\\\\\n& = \\underbrace{R(\\hat{h}_{n,\\mathcal{H}}) - R(h_{\\mathcal{H}}^{\\star})}_{\\textcolor{blue}{\\text{estimation error}}} \\quad + \\quad \\underbrace{R(h_{\\mathcal{H}}^{\\star}) - R(h^{\\star})}_{\\textcolor{blue}{\\text{approximation error}}}\\\\\n\\end{align*}\\]\nThe approximation error comes from the use of a hypothesis class \\(\\mathcal{H}\\).\n\nLarger \\(\\mathcal{H}\\) \\(\\rightarrow\\) smaller approximation error\n\nThe estimation error comes from the use of empirical data.\n\nMore data \\(\\rightarrow\\) smaller estimation error"
  },
  {
    "objectID": "slides/03-ml_basics.html#error-decomposition-1",
    "href": "slides/03-ml_basics.html#error-decomposition-1",
    "title": "Machine Learning Basics",
    "section": "Error Decomposition",
    "text": "Error Decomposition"
  },
  {
    "objectID": "slides/03-ml_basics.html#example",
    "href": "slides/03-ml_basics.html#example",
    "title": "Machine Learning Basics",
    "section": "Example",
    "text": "Example\n\nLinear Regression:\n\n\\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}\\)\n\\(L(y, h(\\boldsymbol{x})) = (y - h(\\boldsymbol{x}))^2\\)\n\nLogistic Regression:\n\n\\(\\mathcal{H} = \\left\\{h: h(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T \\boldsymbol{\\beta})}, \\boldsymbol{\\beta} \\in \\R^p\\right\\}\\)\n\\(L(y, h(\\boldsymbol{x})) = -y \\log(h(\\boldsymbol{x})) - (1 - y) \\log(1 - h(\\boldsymbol{x}))\\)\n\n(Linear) Support Vector Machine:\n\n\\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{w}^T \\boldsymbol{x} + b, \\boldsymbol{w} \\in \\R^p, b \\in \\R\\}\\)\n\\(L(y, h(\\boldsymbol{x})) = \\max(0, 1 - y \\cdot h(\\boldsymbol{x}))\\)"
  },
  {
    "objectID": "slides/03-ml_basics.html#maximum-likelihood-ml-v.s.-erm",
    "href": "slides/03-ml_basics.html#maximum-likelihood-ml-v.s.-erm",
    "title": "Machine Learning Basics",
    "section": "Maximum Likelihood (ML) v.s. ERM",
    "text": "Maximum Likelihood (ML) v.s. ERM\n\nIn fact, the ML principle is a special case of the ERM principle.\nThat is, specifying a likelihood function gives a loss function, i.e., use the negative log-likelihood as the loss function.\nML:\n\nStronger assumptions\nStronger guarantees (consistency, asymptotic normality, etc.)\nAllow us to do more things (e.g., hypothesis testing and confidence intervals)\nLinear regression and logistic regression are ML and hence ERM\n\nERM:\n\nMore flexible and practical, but weaker guarantees\nUsually provide only a point estimate\nSVM is ERM but not ML"
  },
  {
    "objectID": "slides/03-ml_basics.html#constructing-learning-algorithms",
    "href": "slides/03-ml_basics.html#constructing-learning-algorithms",
    "title": "Machine Learning Basics",
    "section": "Constructing Learning Algorithms",
    "text": "Constructing Learning Algorithms\n\nFollowing the ERM principle, we need to specify a loss function and a hypothesis class in order to construct a learning algorithm.\nThe choice of the loss function is based on the types of labels and the problem.\nThe choice of the hypothesis class is more challenging:\n\nSmaller \\(\\mathcal{H}\\) \\(\\rightarrow\\) larger approximation error, smaller estimation error, and less overfitting\nLarger \\(\\mathcal{H}\\) \\(\\rightarrow\\) smaller approximation error, larger estimation error, more overfitting, and requires more data\n\nNext, we will discuss:\n\nhow to measure the “size” (capacity/complexity) of a hypothesis class\nhow to choose an appropriate hypothesis class"
  },
  {
    "objectID": "slides/03-ml_basics.html#complexity-v.s.-dimension",
    "href": "slides/03-ml_basics.html#complexity-v.s.-dimension",
    "title": "Machine Learning Basics",
    "section": "Complexity v.s. Dimension",
    "text": "Complexity v.s. Dimension\n\nLet \\(\\mathcal{H}\\) be a parametric hypothesis class ,e.g., \\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}\\).\nAn intuitive way to measure the complexity of \\(\\mathcal{H}\\) is to count the number of unknown parameters, i.e., the dimension of \\(\\mathcal{H}\\).\nIn this case, the dimension of \\(\\dim(\\mathcal{H}) = p\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#shattering",
    "href": "slides/03-ml_basics.html#shattering",
    "title": "Machine Learning Basics",
    "section": "Shattering",
    "text": "Shattering\nA hypothesis class \\(\\mathcal{H}\\) is said to shatter a set of points \\(S = \\{x_1, \\ldots, x_n\\}\\) if for all possible binary labelings (0/1) of these points, there exists a function \\(h \\in \\mathcal{H}\\) that can perfectly separate the points.\n\n\n\nImage Source: Figure 7.6 of ESL"
  },
  {
    "objectID": "slides/03-ml_basics.html#shattering-1",
    "href": "slides/03-ml_basics.html#shattering-1",
    "title": "Machine Learning Basics",
    "section": "Shattering",
    "text": "Shattering\nDefinition (Restriction of \\(\\mathcal{H}\\) to \\(S\\)) Let \\(\\mathcal{H}\\) be a class of functions from \\(\\mathcal{X}\\) to \\(\\{0,1\\}\\) and let \\(S = \\{x_1, \\ldots, x_n\\} \\subset \\mathcal{X}\\). The restriction of \\(\\mathcal{H}\\) to \\(S\\) is the set of functions from \\(S\\) to \\(\\{0, 1\\}\\) that can be derived from \\(\\mathcal{H}\\). That is, \\[\n   \\mathcal{H}_S = \\{(h(x_1), \\ldots, h(x_n)): h \\in \\mathcal{H}\\}\n\\]\nDefinition (Shattering) A hypothesis class \\(\\mathcal{H}\\) shatters a finite set \\(S \\subset \\mathcal{X}\\) if the restriction of \\(\\mathcal{H}\\) to \\(S\\) is the set of all functions from \\(S\\) to \\(\\{0, 1\\}\\). That is, \\(|\\mathcal{H}_S| = 2^{|S|}\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#vapnik-chervonenkis-vc-dimension",
    "href": "slides/03-ml_basics.html#vapnik-chervonenkis-vc-dimension",
    "title": "Machine Learning Basics",
    "section": "Vapnik-Chervonenkis (VC) Dimension",
    "text": "Vapnik-Chervonenkis (VC) Dimension\nDefinition (VC-dimension) The VC-dimension of a hypothesis class \\(\\mathcal{H}\\), denoted \\(\\text{VC-dim}(\\mathcal{H})\\), is the maximal size of a set \\(S \\subset \\mathcal{X}\\) that can be shattered by \\(\\mathcal{H}\\). If \\(\\mathcal{H}\\) can shatter sets of arbitrarily large size we say that \\(\\mathcal{H}\\) has infinite VC-dimension.\n\nOne can show that for linear models, e.g., \\(\\mathcal{H} = \\{h: h(\\boldsymbol{x}) = \\beta_0 + \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\boldsymbol{\\beta} \\in \\R^p\\}\\), the VC-dimension is \\(p+1\\) (the same as the number of parameters).\nHowever, for nonlinear models, the calculation of the VC-dimension is often challenging."
  },
  {
    "objectID": "slides/03-ml_basics.html#example-infinite-vc-dimension",
    "href": "slides/03-ml_basics.html#example-infinite-vc-dimension",
    "title": "Machine Learning Basics",
    "section": "Example (Infinite VC-dimension)",
    "text": "Example (Infinite VC-dimension)\n\nLet \\(\\mathcal{H} = \\{h: h(x) = \\mathbb{I}(\\sin(\\alpha x) &gt; 0), \\alpha &gt; 0\\}\\). Then \\(\\text{VC-dim}(\\mathcal{H}) = \\infty\\).\nProof:\n\nFor any \\(n\\), let \\(x_1 = 2\\pi 10^{-1}, \\ldots, x_n = 2\\pi 10^{-n}\\).\nThen the parameter \\(\\alpha = \\frac{1}{2}\\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right)\\) can perfectly separate the points, where \\(y_i \\in \\{0, 1\\}\\) is any labeling of the points.\n\n\n\n\n\n\n\n\n\nImage Source: Figure 7.5 of ESL"
  },
  {
    "objectID": "slides/03-ml_basics.html#goodness-of-fit-v.s.-generalization-ability",
    "href": "slides/03-ml_basics.html#goodness-of-fit-v.s.-generalization-ability",
    "title": "Machine Learning Basics",
    "section": "Goodness-of-fit v.s. Generalization ability",
    "text": "Goodness-of-fit v.s. Generalization ability\n\nGoodness-of-fit: how well the model fits the data.\nGeneralization ability: how well the model generalizes to unseen data.\nRecall that for an ERM \\(\\hat{h}_{n, \\mathcal{H}}\\), we have\n\ntraining error (error on training data) \\(\\hat{R}_n = R_{\\text{emp}}(\\hat{h}_{n, \\mathcal{H}})\\)\ngeneralization error (error on unseen data) \\(R(\\hat{h}_{n, \\mathcal{H}}) = \\E_{(X, Y) \\sim F} \\left[L(Y, \\hat{h}_{n,\\mathcal{H}}(X)) \\mid \\mathcal{T}\\right]\\), where \\(\\mathcal{T}\\) denotes the training dataset.\n\nWe can write \\[\\begin{align*}\n   R(\\hat{h}_{n, \\mathcal{H}}) & = \\hat{R}_n + \\left(R(\\hat{h}_{n, \\mathcal{H}}) - \\hat{R}_n\\right)\\\\\n   \\textcolor{blue}{\\text{generalization error}} & = \\textcolor{blue}{\\text{training error}} + \\textcolor{blue}{\\text{generalization gap}}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/03-ml_basics.html#overfitting-and-underfitting",
    "href": "slides/03-ml_basics.html#overfitting-and-underfitting",
    "title": "Machine Learning Basics",
    "section": "Overfitting and Underfitting",
    "text": "Overfitting and Underfitting\n\nTo have low generalization error, we need to have both low training error and small generalization gap.\n\nLarge training error \\(\\rightarrow\\) underfitting\nLow training error but large generalization gap \\(\\rightarrow\\) overfitting\n\n\n\n\n\n\n\n\n\nImage Source: Figure 5.3 in DL"
  },
  {
    "objectID": "slides/03-ml_basics.html#vc-inequality",
    "href": "slides/03-ml_basics.html#vc-inequality",
    "title": "Machine Learning Basics",
    "section": "VC Inequality",
    "text": "VC Inequality\n\nThe VC theory provides an upper bound for the generalization gap, known as the VC inequality: with probability at least \\(1 - \\delta\\) \\[\nR(h) \\leq R_{\\text{emp}}(h)+\\varepsilon \\sqrt{1+\\frac{4 R_{\\text{emp}}(h)}{\\varepsilon}}, \\quad \\varepsilon = O\\left(\\frac{d - \\log \\delta}{n}\\right)\n\\] simultaneously for all \\(h \\in \\mathcal{H}\\), where \\(\\text{VC-dim}(\\mathcal{H}) = d &lt; \\infty\\).\nThe generalization gap increases as\n\nthe VC-dimension increases\nthe samples size \\(n\\) decreases\n\nThis upper bound is only a loose bound and does not work for models with infinite VC-dimension."
  },
  {
    "objectID": "slides/03-ml_basics.html#regularized-erm",
    "href": "slides/03-ml_basics.html#regularized-erm",
    "title": "Machine Learning Basics",
    "section": "Regularized ERM",
    "text": "Regularized ERM\n\nTo prevent overfitting, we can add a regularization term to the empirical risk: \\[\nR_{\\text{reg}}(h) = R_{\\text{emp}}(h) + \\lambda \\Omega(h)\n\\] where \\(\\Omega(h)\\) is a regularization term and \\(\\lambda\\) is the regularization parameter.\nTypically, \\(\\Omega(h)\\) measures the smoothness or complexity of the model \\(h\\).\n\n\n\n\n  \n  \n  \n    \n      \n        image/svg+xml\n        \n        \n      \n    \n  \n  \n    \n      \n        \n        \n        \n        \n        \n        \n        \n        \n        \n      \n      \n      x\n      y\n    \n  \n\n\n\n\n\nImage Source: https://en.wikipedia.org/wiki/Regularization_(mathematics)"
  },
  {
    "objectID": "slides/03-ml_basics.html#regularized-erm-1",
    "href": "slides/03-ml_basics.html#regularized-erm-1",
    "title": "Machine Learning Basics",
    "section": "Regularized ERM",
    "text": "Regularized ERM\n\nFor example, \\(\\Omega(h) = \\|h^{\\prime}(x)\\|_2^2 = \\int \\left(h^{\\prime}(x)\\right)^2dx\\). (\\(L_2\\) Regularization)\nIf \\(h(x) = \\beta_0 + \\beta_1 x\\), then \\(h^{\\prime}(x) = \\beta_1\\) and \\(\\Omega(h) = \\beta_1^2\\).\nThe \\(L_1\\) regularization is \\(\\Omega(h) = \\|h^{\\prime}(x)\\|_1 = \\int |h^{\\prime}(x)|dx\\).\nIf \\(h(x) = \\beta_0 + \\beta_1 x\\), then \\(h^{\\prime}(x) = \\beta_1\\) and \\(\\Omega(h) = |\\beta_1|\\).\nUsing \\(L_1\\) gives you sparsity; using \\(L_2\\) gives you smoothness/insensitivity:\n\nConsider a linear model \\(h(\\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{\\beta}\\).\nA good model should not be too sensitive to the input, i.e., small changes in the input should not lead to large changes in the output.\nThat is, if \\(\\boldsymbol{x} \\approx \\tilde{\\boldsymbol{x}}\\) , then \\(|\\boldsymbol{x}^T \\boldsymbol{\\beta} - \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}|\\) should be small.\nNote that \\[\n|\\boldsymbol{x}^T \\boldsymbol{\\beta} - \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}| = |(\\boldsymbol{x} - \\tilde{\\boldsymbol{x}})^T \\boldsymbol{\\beta}| \\leq \\|\\boldsymbol{x} - \\tilde{\\boldsymbol{x}}\\|_2 \\|\\boldsymbol{\\beta}\\|_2\n\\]"
  },
  {
    "objectID": "slides/03-ml_basics.html#bias-variance-tradeoff",
    "href": "slides/03-ml_basics.html#bias-variance-tradeoff",
    "title": "Machine Learning Basics",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nAdding a reguralization term often increases the bias but reduces the variance of the model.\nThis tradeoff is known as the bias-variance tradeoff.\n\n\n\n\n\n\n\n\nImage Source: Figure 5.6 in DL"
  },
  {
    "objectID": "slides/03-ml_basics.html#quick-summary-1",
    "href": "slides/03-ml_basics.html#quick-summary-1",
    "title": "Machine Learning Basics",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nThe VC dimension measures the complexity of a hypothesis class.\nThe VC inequality provides an upper bound for the generalization gap, provided that the VC dimension is finite.\nThe bound is often criticized for being too loose and does not work for models with infinite VC dimension.\nExample of infinite VC dimension:\n\nNeural Networks\nKernel methods (e.g., kernel SVM, kernel regression)\n\\(K\\)-nearest neighbors (with small \\(K\\), say \\(K = 1\\))"
  },
  {
    "objectID": "slides/03-ml_basics.html#double-descent-curve",
    "href": "slides/03-ml_basics.html#double-descent-curve",
    "title": "Machine Learning Basics",
    "section": "Double Descent Curve",
    "text": "Double Descent Curve\n\n\n\nBelkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32), 15849-15854."
  },
  {
    "objectID": "slides/03-ml_basics.html#estimating-the-generalization-error",
    "href": "slides/03-ml_basics.html#estimating-the-generalization-error",
    "title": "Machine Learning Basics",
    "section": "Estimating the Generalization Error",
    "text": "Estimating the Generalization Error\n\nAlthough the VC inequality provides an upper bound for the generalization gap, it is often too loose.\nIn order to have a more accurate insight into the model’s generalization ability, we need to estimate the generalization error.\nTo achieve this, we need to have an extra dataset, called the validation dataset \\(\\mathcal{V} = \\{(\\tilde{x}_i, \\tilde{y}_i)\\}_{i=1}^m\\).\nThe generalization error is then estimated as \\[\n\\hat{R}_{\\text{gen}} = \\frac{1}{m} \\sum_{i=1}^m L(\\tilde{y}_i, \\hat{h}_{n, \\mathcal{H}}(\\tilde{x}_i)).\n\\]\nAssuming the \\(\\mathcal{V}\\) and \\(\\mathcal{T}\\) (training dataset) are i.i.d, \\(\\hat{R}_{\\text{gen}}\\) is an unbiased estimate of the generalization error, i.e., \\(\\E[\\hat{R}_{\\text{gen}} \\mid \\mathcal{T}] = R(\\hat{h}_{n, \\mathcal{H}})\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#k-fold-cross-validation-cv",
    "href": "slides/03-ml_basics.html#k-fold-cross-validation-cv",
    "title": "Machine Learning Basics",
    "section": "\\(k\\)-fold Cross-Validation (CV)",
    "text": "\\(k\\)-fold Cross-Validation (CV)\n\nIn practice, we often do not have an extra validation dataset and hence we need to use the training dataset to estimate the generalization error.\nOne common method is the \\(k\\)-fold cross-validation:\n\nSplit the training dataset \\(\\mathcal{T}\\) into \\(k\\) equal-sized folds.\nFor each fold \\(i = 1, \\ldots, k\\), train the model on the remaining \\(k-1\\) folds and evaluate the model on the \\(i\\)th fold.\nAverage the \\(k\\) validation errors to obtain the estimated generalization error."
  },
  {
    "objectID": "slides/03-ml_basics.html#k-fold-cross-validation",
    "href": "slides/03-ml_basics.html#k-fold-cross-validation",
    "title": "Machine Learning Basics",
    "section": "\\(k\\)-fold Cross-Validation",
    "text": "\\(k\\)-fold Cross-Validation\n\nWhen \\(k = n\\), it is called the leave-one-out cross-validation (LOOCV), i.e., train the model on \\(n-1\\) samples and evaluate on the remaining one.\nChoice of \\(k\\)?\n\nLarger \\(k\\) \\(\\rightarrow\\) low bias, high variance (the model is trained on a larger dataset and validated on a smaller dataset)\nSmaller \\(k\\) \\(\\rightarrow\\) high bias, low variance (the model is trained on a smaller dataset and validated on a larger dataset)\n\\(k = 5\\) or \\(k = 10\\) are common choices."
  },
  {
    "objectID": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning",
    "href": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning",
    "title": "Machine Learning Basics",
    "section": "CV for Hyperparameter Tuning",
    "text": "CV for Hyperparameter Tuning\n\nIn practice, the models often have hyperparameters that need to be tuned, e.g., the regularization parameter \\(\\lambda\\).\nWe can use CV to choose the best hyperparameters:\n\nFor each hyperparameter value, perform \\(k\\)-fold CV to estimate the generalization error.\nChoose the hyperparameter value that minimizes the CV error.\n\nHowever, the CV error after the selection will overestimate the generalization error.\nSuch bias is known as the selection bias."
  },
  {
    "objectID": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning-1",
    "href": "slides/03-ml_basics.html#cv-for-hyperparameter-tuning-1",
    "title": "Machine Learning Basics",
    "section": "CV for Hyperparameter Tuning",
    "text": "CV for Hyperparameter Tuning\n\nTo avoid the selection bias, we first split the dataset into two parts: the training dataset and the test dataset.\nThe test dataset should not be used in the neither the traing process nor hyperparameter tuning process.\nThe training dataset is further split into \\(k\\)-folds for CV.\nAfter all the processes, including training, hyperparameter tuning, model selection, etc., we evaluate the final model on the test dataset to estimate the generalization error."
  },
  {
    "objectID": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter",
    "href": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter",
    "title": "Machine Learning Basics",
    "section": "Example: Using CV to Choose the Regularization Parameter",
    "text": "Example: Using CV to Choose the Regularization Parameter\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import datasets\n\nX, y = datasets.load_diabetes(return_X_y=True)\n\nX /= X.std(axis=0)\n\nalpha_seq = np.logspace(-2, 2, 100)\nreg = LassoCV(alphas = alpha_seq, cv = 5, random_state = 42)\nreg.fit(X, y)\n\nprint(\"best alpha:\", np.round(reg.alpha_, 4))\n\nbest alpha: 0.0774"
  },
  {
    "objectID": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter-1",
    "href": "slides/03-ml_basics.html#example-using-cv-to-choose-the-regularization-parameter-1",
    "title": "Machine Learning Basics",
    "section": "Example: Using CV to Choose the Regularization Parameter",
    "text": "Example: Using CV to Choose the Regularization Parameter"
  },
  {
    "objectID": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension",
    "href": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension",
    "title": "Machine Learning Basics",
    "section": "Proof of Example (Infinite VC-dimension)",
    "text": "Proof of Example (Infinite VC-dimension)\n\nWe need to show that the model \\(h(x) = \\mathbb{I}(\\sin(\\alpha x) &gt; 0)\\) with \\(\\alpha = \\frac{1}{2}\\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right)\\) can perfectly separate the \\(n\\) points.\nConsider the \\(j\\)th sample \\(x_j = 2\\pi 10^{-j}\\).\nIf \\(y_j = 0\\), then \\[\\begin{align*}\n   \\alpha x_j & = \\pi 10^{-j} \\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right) = \\pi 10^{-j} \\left(1 + \\sum_{\\{i: y_i = 0\\}} 10^i\\right)\\\\\n   & = \\pi 10^{-j}\\left(1 + 10^j + \\sum_{\\{i: y_i = 0, i \\neq j\\}} 10^i\\right)\\\\\n   & = \\pi \\left(10^{-j} + 1 + \\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j}\\right)\n\\end{align*}\\]\n\n\n\nReference: https://mlweb.loria.fr/book/en/VCdiminfinite.html"
  },
  {
    "objectID": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-1",
    "href": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-1",
    "title": "Machine Learning Basics",
    "section": "Proof of Example (Infinite VC-dimension)",
    "text": "Proof of Example (Infinite VC-dimension)\n\nFor \\(i&gt;j\\), \\(10^{i-j}\\) is even and so is \\(\\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j}\\), say \\[\n\\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j} = 2m, \\quad m \\in \\mathbb{N}.\n\\]\nNote that \\[\n\\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j} &lt; \\sum_{i=1}^{\\infty} 10^{-i} = \\sum_{i=0}^{\\infty} 10^{-i} - 1 = \\frac{1}{1-0.1} - 1 = \\frac{1}{9}.  \n\\]\nTherefore, \\(\\alpha x_j = \\pi(1 + 2m +\\epsilon)\\), where \\[\n0 &lt; \\epsilon = 10^{-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j} &lt; \\frac{1}{10} + \\frac{1}{9} &lt; 1.\n\\]\nHence \\(\\sin(\\alpha x_j) &lt; 0\\) and \\(h(x_j) = 0\\)."
  },
  {
    "objectID": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-2",
    "href": "slides/03-ml_basics.html#proof-of-example-infinite-vc-dimension-2",
    "title": "Machine Learning Basics",
    "section": "Proof of Example (Infinite VC-dimension)",
    "text": "Proof of Example (Infinite VC-dimension)\n\nIf \\(y_j = 1\\), then \\[\\begin{align*}\n   \\alpha x_j & = \\pi 10^{-j} \\left(1 + \\sum_{i=1}^n (1-y_i)10^i\\right) = \\pi 10^{-j} \\left(1 + \\sum_{\\{i: y_i = 0\\}} 10^i\\right)\\\\\n   & = \\pi \\left(10^{-j} + \\sum_{\\{i: y_i = 0, i &gt; j\\}} 10^{i-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j}\\right)\n\\end{align*}\\]\nSimilarly, we have \\(\\alpha x_j = \\pi(2m +\\epsilon)\\), where \\[\n0 &lt; \\epsilon = 10^{-j} + \\sum_{\\{i: y_i = 0, i &lt; j\\}} 10^{i-j} &lt; \\frac{1}{10} + \\frac{1}{9} &lt; 1.\n\\]\nHence \\(\\sin(\\alpha x_j) &gt; 0\\) and \\(h(x_j) = 1\\).\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/01-intro.html#course-description",
    "href": "slides/01-intro.html#course-description",
    "title": "STAT 5011: Course Introduction",
    "section": "Course Description",
    "text": "Course Description\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nThis course provides an introduction to some commonly used models in deep learning:\n\nMultilayer Perceptron (MLP) or Fully-connected neural network (FCN)\nConvolutional Neural Network (CNN)\nRecurrent Neural Network (RNN)\nGenerative models\n\nThe course will cover the basic theory, practical implementation, and some applications of these models."
  },
  {
    "objectID": "slides/01-intro.html#prerequisites",
    "href": "slides/01-intro.html#prerequisites",
    "title": "STAT 5011: Course Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nKnowledge of linear algebra, calculus, probability, and statistics is required.\nExperiences in Python programming is also required (import libraries, write functions, etc.)\nKnowledge of object-oriented programming is a plus.\nKnowledge of machine learning would also be helpful (we will cover some basics in the course)."
  },
  {
    "objectID": "slides/01-intro.html#references",
    "href": "slides/01-intro.html#references",
    "title": "STAT 5011: Course Introduction",
    "section": "References",
    "text": "References\nDeep Learning: https://www.deeplearningbook.org"
  },
  {
    "objectID": "slides/01-intro.html#references-1",
    "href": "slides/01-intro.html#references-1",
    "title": "STAT 5011: Course Introduction",
    "section": "References",
    "text": "References\nDive into Deep Learning: https://d2l.ai"
  },
  {
    "objectID": "slides/01-intro.html#other-resources",
    "href": "slides/01-intro.html#other-resources",
    "title": "STAT 5011: Course Introduction",
    "section": "Other Resources",
    "text": "Other Resources\n\nT. Hastie, R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning.\nC. M. Bishop (2006). Pattern Recognition and Machine Learning.\nS. Shalev-Shwartz and S. Ben-David (2014). Understanding Machine Learning: From Theory to Algorithms.\nV. Vapnik (2000). The Nature of Statistical Learning Theory\nC. C. Aggarwal (2023). Neural Networks and Deep Learning"
  },
  {
    "objectID": "slides/01-intro.html#schedule",
    "href": "slides/01-intro.html#schedule",
    "title": "STAT 5011: Course Introduction",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReading\n\n\n\n\n1\n9/3\nCourse Introduction\n\n\n\n2\n9/10\nReview of Linear Models\n\n\n\n3\n9/17\nNo class (Mid-Autumn Festival)\n\n\n\n4\n9/24\nMachine Learning Basics\nDL Ch. 5\n\n\n5\n10/1\nMultilayer Perceptron\nD2L Ch. 5 & DL Ch. 6\n\n\n6\n10/8\nRegularization for Deep Learning\nDL Ch. 7\n\n\n7\n10/15\nOptimization for DL Models\nD2L Ch. 12 & DL Ch. 8\n\n\n8\n10/22\nProject Proposal\n\n\n\n9\n10/29\nImplementation of DL Models\nD2L Ch. 6\n\n\n10\n11/5\nConvolutional Networks\nD2L Ch. 7, 8 & DL Ch. 9\n\n\n11\n11/12\nRecurrent Networks\nD2L Ch. 9, 10 & DL Ch. 10\n\n\n12\n11/19\nHyperparameter Optimization and Tuning\nD2L Ch. 19 & DL Ch. 11\n\n\n13\n11/26\nGenerative Models: Autoencoders, GAN, Diffusion models\nD2L Ch. 20 & DL Ch. 14\n\n\n14\n12/3\nAdditional Topics: Attention Mechanisms and Gaussian Process\nD2L Ch. 11, 18\n\n\n15-16\n12/10-17\nFinal Project Presentation"
  },
  {
    "objectID": "slides/01-intro.html#grading",
    "href": "slides/01-intro.html#grading",
    "title": "STAT 5011: Course Introduction",
    "section": "Grading",
    "text": "Grading\n\nHomework: 30%\nProject proposal: 20%\n\nA 20-minute presentation\n\nFinal Project: 50%\n\nA 30-minute presentation (25%)\nA final report (25%)\n\nOffice hours: Tue. 15:00-17:00"
  },
  {
    "objectID": "slides/01-intro.html#homework",
    "href": "slides/01-intro.html#homework",
    "title": "STAT 5011: Course Introduction",
    "section": "Homework",
    "text": "Homework\n\nThere will be 3 homework assignments.\nHomework includes some math problems and programming exercises.\nProgramming assignments will be done using IPython notebooks and exported to PDF.\nMath problems will be submitted as a PDF file (using LaTeX preferably).\nDO NOT:\n\nPlagiarism: copy solution from others or from the internet.\nTake photos of your computer screen.\nTake photos of your handwritten solutions."
  },
  {
    "objectID": "slides/01-intro.html#project-proposal",
    "href": "slides/01-intro.html#project-proposal",
    "title": "STAT 5011: Course Introduction",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nA group of 2-3 students\nPick a topic that you plan to solve using deep learning models, for example:\n\nimage classification/segmentation\nstock price prediction\nweather forcasting\n\nIt could be something related to your thesis research.\nThe proposal should include:\n\nDiscription of your problem\nExample dataset\nSummary of 1-2 references\n\nGive a 20-minute presentation on 10/22"
  },
  {
    "objectID": "slides/01-intro.html#final-project",
    "href": "slides/01-intro.html#final-project",
    "title": "STAT 5011: Course Introduction",
    "section": "Final Project",
    "text": "Final Project\n\nOral Presentation (25%)\n\n30-minute presentation\nFocus the model you used, the dataset, and the results\nCompare to other models\n\nWritten Report (25%)\n\nUse the template: NeurIPS\n6-page including references; one report per group\nInclude: introduction, methods, results, and conclusion\n\nMore details will be provided later."
  },
  {
    "objectID": "slides/01-intro.html#what-is-deep-learning",
    "href": "slides/01-intro.html#what-is-deep-learning",
    "title": "STAT 5011: Course Introduction",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?"
  },
  {
    "objectID": "slides/01-intro.html#what-is-dlml",
    "href": "slides/01-intro.html#what-is-dlml",
    "title": "STAT 5011: Course Introduction",
    "section": "What is DL/ML?",
    "text": "What is DL/ML?\n\nDeep learning is a subfield of machine learning that is based on deep neural networks (DNN).\nDNN is a powerful approximating class of parametric class of functions.\nML is a field of study that focuses on automatic detection/extraction of patterns from raw data.\nTo achieve this, ML uses a variety of statistical models:\n\nlinear regression, logistic regression,\ntree models,\n\\(k\\)-nearest neighbors (kNN), etc."
  },
  {
    "objectID": "slides/01-intro.html#turing-test",
    "href": "slides/01-intro.html#turing-test",
    "title": "STAT 5011: Course Introduction",
    "section": "Turing Test",
    "text": "Turing Test\n\nThe Turing test, originally called the imitation game by Alan Turing in 1950, is a test of a machine’s ability to exhibit intelligent behaviour equivalent to that of a human.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956.\n\n\n\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "slides/01-intro.html#hebbs-theory",
    "href": "slides/01-intro.html#hebbs-theory",
    "title": "STAT 5011: Course Introduction",
    "section": "Hebb’s Theory",
    "text": "Hebb’s Theory\n\nIn 1949, Donald Hebb1 proposed a theory of learning in which the connection between two neurons is strengthened if they are activated simultaneously.\nHebbian learning rule:\n\nThe connection between two neurons: \\(w_{ij} \\leftarrow w_{ij} + \\Delta w_{ij}\\)\nThe change in the connection: \\(\\Delta w_{ij} = \\eta x_i x_j\\)\nwhere \\(\\eta\\) is the learning rate, \\(x_i\\) and \\(x_j\\) are the activities of the two neurons.\n\n\nHebb, D. O. (1949). The Organization of Behavior: A Neuropsychological Theory."
  },
  {
    "objectID": "slides/01-intro.html#biological-neuron-model",
    "href": "slides/01-intro.html#biological-neuron-model",
    "title": "STAT 5011: Course Introduction",
    "section": "Biological Neuron Model",
    "text": "Biological Neuron Model\n\n\nimage/svg+xml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendrite\n\n\n\n\n\n\n\n\nSoma (cell body)\n\n\n\n\n\n\n\n\n\n\nAxon terminal\n\n\n\n\n\n\n\n\n\n\n\n\nMyelinated axon trunk\n\n\n\n\n\n\n\n\n\n\nMyelin sheat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\n\nOutputs\n\n\n\n\n\nInput points = synapses\nOutput points = synapses\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "slides/01-intro.html#artificial-neuron",
    "href": "slides/01-intro.html#artificial-neuron",
    "title": "STAT 5011: Course Introduction",
    "section": "Artificial Neuron",
    "text": "Artificial Neuron\n\nMcCulloch and Pitts (1943) proposed a simple mathematical model for neurons.\nA neuron has \\(n\\) inputs \\(x = (x_1, ... ,x_n) \\in \\R^n\\) and one output \\(y \\in \\{-1, 1\\}\\).\n\\((u * v)\\) is the inner product of two vectors, \\(b\\) is a threshold value, and \\(\\text{sign}(u)= 1\\) if \\(u &gt; 0\\) and \\(\\text{sign}(u)= -1\\) if \\(u\\leq 0\\).\nDuring the learning process, the model chooses appropriate coefficients \\(w, b\\) of the neuron."
  },
  {
    "objectID": "slides/01-intro.html#rosenblatts-perceptron-1960s",
    "href": "slides/01-intro.html#rosenblatts-perceptron-1960s",
    "title": "STAT 5011: Course Introduction",
    "section": "Rosenblatt’s Perceptron (1960s)",
    "text": "Rosenblatt’s Perceptron (1960s)\n\nRosenblatt considered a model that is a composition of several neurons.\nEach neuron has its own weight \\(w\\) and threshold \\(b\\)."
  },
  {
    "objectID": "slides/01-intro.html#perceptron-learning-algorithm-pla",
    "href": "slides/01-intro.html#perceptron-learning-algorithm-pla",
    "title": "STAT 5011: Course Introduction",
    "section": "Perceptron Learning Algorithm (PLA)",
    "text": "Perceptron Learning Algorithm (PLA)\n\nThe weights and bias between the input and the hidden layer are random numbers and kept fixed.\nLet \\((x_1,y_1),\\ldots,(x_n,y_n)\\) be the training data and \\(z_i\\) be the transformation of the input \\(x_i\\) in the hidden layer.\n\nInitialize weights: \\(w^{(0)} = 0\\).\nIf the next example of the training data \\((z_{k+1}, y_{k+1})\\) is classified correctly, i.e., \\[\n      y_{k+1}(w^{(k)}\\cdot z_{k+1}) &gt; 0,\n  \\] then \\(w^{(k + 1)} = w^{(k)}\\).\nIf the next element is classified incorrectly, i.e., \\[\n     y_{k+1}(w^{(k)}\\cdot z_{k+1}) \\leq 0,\n\\] then \\(w^{(k +1)} = w^{(k)} +y_{k+1}z_{k+1}\\)."
  },
  {
    "objectID": "slides/01-intro.html#mark-i-perceptron",
    "href": "slides/01-intro.html#mark-i-perceptron",
    "title": "STAT 5011: Course Introduction",
    "section": "Mark I Perceptron",
    "text": "Mark I Perceptron\n\n\n\nMark I Perceptron (1960)"
  },
  {
    "objectID": "slides/01-intro.html#rosenblatts-experiment",
    "href": "slides/01-intro.html#rosenblatts-experiment",
    "title": "STAT 5011: Course Introduction",
    "section": "Rosenblatt’s Experiment",
    "text": "Rosenblatt’s Experiment\n\n\n\n\n\n\n\nRosenblatt, F. (1960). Perceptron simulation experiments. Proceedings of the IRE, 48(3), pages 301-309."
  },
  {
    "objectID": "slides/01-intro.html#theoretical-analysis-of-pla",
    "href": "slides/01-intro.html#theoretical-analysis-of-pla",
    "title": "STAT 5011: Course Introduction",
    "section": "Theoretical Analysis of PLA",
    "text": "Theoretical Analysis of PLA\nIn 1962, Novikoff1 proved the first theorem about the PLA. If\n\nthe norm of the training vectors \\(z\\) is bounded by some constant \\(R\\) (\\(|z| \\leq R\\)),and\n(linear separability) the training data can be separated with margin \\(\\rho\\): \\[\n     \\sup_w \\min_i y_i(z_i \\cdot w) &gt; \\rho\n\\]\n\nThen after at most \\(N \\leq \\frac{R^2}{\\rho^2}\\) steps, the hyperplane that separates the training data will be constructed.\nNovikoff, A. B. J. (1962). On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathematical Theory of Automata, Vol. XII, pages 615–622."
  },
  {
    "objectID": "slides/01-intro.html#learning-theory",
    "href": "slides/01-intro.html#learning-theory",
    "title": "STAT 5011: Course Introduction",
    "section": "Learning Theory",
    "text": "Learning Theory\n\nNovikoff’s result and Rosenblatt’s experiment raised several questions:\n\nWhat can be learned?\nWhat is the principle for designing learning algorithms?\nHow can we assure that the algorithm is actually learning, not just memorizing?\n\nThese questions led to the development of the statistical learning theory during 70s-80s.\nImportant results include:\n\nVapnik-Chervonenkis (VC) theory (for characterizing the capacity of a model)\nProbably Approximately Correct (PAC) learning theory (for characterizing whether a model can learn from a finite sample)\nEmpirical Risk Minimization (ERM) principle (for designing learning algorithms)"
  },
  {
    "objectID": "slides/01-intro.html#revival-of-neural-networks",
    "href": "slides/01-intro.html#revival-of-neural-networks",
    "title": "STAT 5011: Course Introduction",
    "section": "Revival of Neural Networks",
    "text": "Revival of Neural Networks\n\nIn 1986, several authors independently proposed a method for simultaneously constructing the vector coefficients for all neurons of the Perceptron using the so-called back-propagation method12.\nThe idea is to replace to McCulloch-Pitts neuron model with a sigmoid approximation, i.e., \\[\n     y = S(w\\cdot x - b)\n\\] where \\(S(x)\\) is a sigmoid function (differentiable, monotonic, \\(S(-\\infty) = -1\\) and \\(S(\\infty) = 1\\)).\nThis allows us to apply gradient-based optimization methods to find the optimal weights.\n\nLe Cun, Y. (1986). Learning processes in an asymmetric threshold network, Disordered systems and biological organizations, Les Houches, France, Springer, pages 233-240.Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propagation, Parallel distributed processing: Explorations in the microstructure of cognition, Vol. I, Badford Books, Cambridge, MA., pages 318-362."
  },
  {
    "objectID": "slides/01-intro.html#example-of-sigmoid-functions",
    "href": "slides/01-intro.html#example-of-sigmoid-functions",
    "title": "STAT 5011: Course Introduction",
    "section": "Example of sigmoid functions",
    "text": "Example of sigmoid functions\n\n\n\n\nSigmoidal Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\n\n\n  \n  \n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\n\n\n\n\n\t\n\t\n\t\n\n\n\n  \n  \n  \n  \n\n\n\n\t\n\n\n\n\n\t\n\t\t\n\t\t\n\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\t\n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t\n\n\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\n\n\n\n\t\n\t\n\t\n\t  \n\t  \n\t  \n\t  \n\t\n\t\n\t  \n\t  \n\t  \n\t\n\t\n\t\n\n\n\n\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "slides/01-intro.html#universal-approximation-theorem",
    "href": "slides/01-intro.html#universal-approximation-theorem",
    "title": "STAT 5011: Course Introduction",
    "section": "Universal Approximation Theorem",
    "text": "Universal Approximation Theorem\n\nIn 1989, Cybenko1 proved the universal approximation theorem for feedforward neural networks.\nThe theorem states that\n\n\n… networks with one internal layer and an arbitrary continuous sigmoidal function can approximate continuous functions wtih arbitrary precision providing that no constraints are placed on the number of nodes or the size of the weights.\n\n\nThat is, the finite sum \\(G(x) = \\sum_{i=1}^h a_i S(w_i \\cdot x - b_i)\\), \\(x \\in D \\subseteq \\R^n\\), is dense in the space of continuous functions on \\(D\\) where \\(D\\) is compact.\n\nCybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), pages 303-314."
  },
  {
    "objectID": "slides/01-intro.html#in-the-1990s",
    "href": "slides/01-intro.html#in-the-1990s",
    "title": "STAT 5011: Course Introduction",
    "section": "In the 1990s",
    "text": "In the 1990s\n\nLe Cun (1989)1 proposed convolutional network for data with grid-like structure, e.g., images.\nHochreiter and Schmidhuber (1997)2 introduced the Long Short-Term Memory (LSTM) network to model sequential data, e.g., language and time series data.\nDue to the difficulty in training, more attention is now focused on the alternatives to neural networks, for example,\n\nsupport vector machine (SVM, Cortes and Vapnik (1995))\nkernel methods3\ngraphical models4\n\n\nLe Cun, Y. (1989). Generalization and network design strategies. Technical Report CRG-TR-89-4, University of Toronto.Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), pages 1735-1780.Schölkopf, B., & Smola, A. J. (2002). Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.Jordan, M. I. (1999). Learning in graphical models. MIT press."
  },
  {
    "objectID": "slides/01-intro.html#s---present",
    "href": "slides/01-intro.html#s---present",
    "title": "STAT 5011: Course Introduction",
    "section": "2000s - present",
    "text": "2000s - present\n\nIn 2006, Geoffrey Hinton1 showed that a kind of neural network called a deep belief network could be efficiently trained using a strategy called greedy layer-wise pretraining.\nThis wave of neural networks research popularized the use of the term deep learning to emphasize that researchers were now able to train deeper neural networks than had been possible before.\nDeep neural networks started to outperform other ML models (e.g., AlexNet (2012), VGG (2014), ResNet (2015)).\nAlso the presence of big data motivates researchers and practitioners to develop complicated models.\nIn 2023, ChatGPT broke the Turing test2.\n\nHinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural computation, 18(7), pages 1527-1554.Biever, C. (2023). ChatGPT broke the Turing test-the race is on for new ways to assess AI. Nature, 619(7971), 686-689."
  },
  {
    "objectID": "slides/01-intro.html#three-waves-of-neural-networks",
    "href": "slides/01-intro.html#three-waves-of-neural-networks",
    "title": "STAT 5011: Course Introduction",
    "section": "Three Waves of Neural Networks",
    "text": "Three Waves of Neural Networks\n\nThe first wave: 1940s-1960s\n\nFundamental concepts: artificial neuron, perceptron\nPerceptron learning algorithm\n\nThe second wave: 1980s-1990s\n\nBack-propagation algorithm\nNetwork design strategies: convolutional networks, LSTM\n\nThe third wave: 2000s-present\n\nDeep neural networks\nLarge datasets and computational resources\nLarge Language Model (LLM), e.g., ChatGPT"
  },
  {
    "objectID": "slides/01-intro.html#the-end-of-the-second-wave",
    "href": "slides/01-intro.html#the-end-of-the-second-wave",
    "title": "STAT 5011: Course Introduction",
    "section": "The end of the second wave",
    "text": "The end of the second wave\nGoodfellow et al. (2016) pointed out\n\nThe second wave of neural networks research lasted until the mid-1990s. Ventures based on neural networks and other AI technologies began to make unrealistically ambitious claims while seeking investments. When AI research did not fulfill these unreasonable expectations, investors were disappointed."
  },
  {
    "objectID": "slides/01-intro.html#an-impending-ai-doom-model-collapse",
    "href": "slides/01-intro.html#an-impending-ai-doom-model-collapse",
    "title": "STAT 5011: Course Introduction",
    "section": "An Impending AI Doom: Model Collapse",
    "text": "An Impending AI Doom: Model Collapse\n\nShumailov et al. (2023)1 showed that training on generated data can make models forget.\nThey demonstrated that training on generated data can lead to catastrophic forgetting, a phenomenon where models forget how to perform well on real data.\n\n\n\n\n\n\nShumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., & Anderson, R. (2023). The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493."
  },
  {
    "objectID": "slides/01-intro.html#other-readings",
    "href": "slides/01-intro.html#other-readings",
    "title": "STAT 5011: Course Introduction",
    "section": "Other readings",
    "text": "Other readings\n\nThe story of Frank Rosenblatt: Professor’s perceptron paved the way for AI – 60 years too soon\n\nWhat is ‘model collapse’? An expert explains the rumours about an impending AI doom.\n\n\n\n\nHome"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Deep Learning (Fall 2024)",
    "section": "",
    "text": "Week\nDate\nTopics\nSlides\nReading\n\n\n\n\n1\n9/3\nCourse Introduction\nSlide\n\n\n\n2\n9/10\nReview of Linear Models\nSlide\n\n\n\n3\n9/17\nNo class (Mid-Autumn Festival)\n\n\n\n\n4\n9/24\nMachine Learning Basics\nSlide\nDL Ch. 5\n\n\n5\n10/1\nMultilayer Perceptron\n\nD2L Ch. 5 & DL Ch. 6\n\n\n6\n10/8\nRegularization for Deep Learning\n\nDL Ch. 7\n\n\n7\n10/15\nOptimization for DL Models\n\nD2L Ch. 12 & DL Ch. 8\n\n\n8\n10/22\nProject Proposal\n\n\n\n\n9\n10/29\nImplementation of DL Models\n\nD2L Ch. 6\n\n\n10\n11/5\nConvolutional Networks\n\nD2L Ch. 7, 8 & DL Ch. 9\n\n\n11\n11/12\nRecurrent Networks\n\nD2L Ch. 9, 10 & DL Ch. 10\n\n\n12\n11/19\nHyperparameter Optimization and Tuning\n\nD2L Ch. 19 & DL Ch. 11\n\n\n13\n11/26\nGenerative Models: Autoencoders, GAN, Diffusion models\n\nD2L Ch. 20 & DL Ch. 14\n\n\n14\n12/3\nAdditional Topics: Attention Mechanisms and Gaussian Process\n\nD2L Ch. 11, 18\n\n\n15-16\n12/10-17\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "Statistical Deep Learning (Fall 2024)",
    "section": "",
    "text": "Week\nDate\nTopics\nSlides\nReading\n\n\n\n\n1\n9/3\nCourse Introduction\nSlide\n\n\n\n2\n9/10\nReview of Linear Models\nSlide\n\n\n\n3\n9/17\nNo class (Mid-Autumn Festival)\n\n\n\n\n4\n9/24\nMachine Learning Basics\nSlide\nDL Ch. 5\n\n\n5\n10/1\nMultilayer Perceptron\n\nD2L Ch. 5 & DL Ch. 6\n\n\n6\n10/8\nRegularization for Deep Learning\n\nDL Ch. 7\n\n\n7\n10/15\nOptimization for DL Models\n\nD2L Ch. 12 & DL Ch. 8\n\n\n8\n10/22\nProject Proposal\n\n\n\n\n9\n10/29\nImplementation of DL Models\n\nD2L Ch. 6\n\n\n10\n11/5\nConvolutional Networks\n\nD2L Ch. 7, 8 & DL Ch. 9\n\n\n11\n11/12\nRecurrent Networks\n\nD2L Ch. 9, 10 & DL Ch. 10\n\n\n12\n11/19\nHyperparameter Optimization and Tuning\n\nD2L Ch. 19 & DL Ch. 11\n\n\n13\n11/26\nGenerative Models: Autoencoders, GAN, Diffusion models\n\nD2L Ch. 20 & DL Ch. 14\n\n\n14\n12/3\nAdditional Topics: Attention Mechanisms and Gaussian Process\n\nD2L Ch. 11, 18\n\n\n15-16\n12/10-17\nFinal Project Presentation"
  },
  {
    "objectID": "index.html#important-dates",
    "href": "index.html#important-dates",
    "title": "Statistical Deep Learning (Fall 2024)",
    "section": "Important Dates:",
    "text": "Important Dates:\n\n9/17: No Class (Mid-Autumn Festival)\n10/22: Proposal Presentation\n12/10-17: Final Project Presentation"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Goodfellow et al. (2016) Deep Learning\nZhang et al. (2023) Dive into Deep Learning\nT. Hastie, R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning.\nC. M. Bishop (2006). Pattern Recognition and Machine Learning.\nS. Shalev-Shwartz and S. Ben-David (2014). Understanding Machine Learning: From Theory to Algorithms.\nV. Vapnik (2000). The Nature of Statistical Learning Theory\nC. C. Aggarwal (2023). Neural Networks and Deep Learning"
  },
  {
    "objectID": "resources.html#references",
    "href": "resources.html#references",
    "title": "Resources",
    "section": "",
    "text": "Goodfellow et al. (2016) Deep Learning\nZhang et al. (2023) Dive into Deep Learning\nT. Hastie, R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning.\nC. M. Bishop (2006). Pattern Recognition and Machine Learning.\nS. Shalev-Shwartz and S. Ben-David (2014). Understanding Machine Learning: From Theory to Algorithms.\nV. Vapnik (2000). The Nature of Statistical Learning Theory\nC. C. Aggarwal (2023). Neural Networks and Deep Learning"
  },
  {
    "objectID": "slides/02-lm.html#outline",
    "href": "slides/02-lm.html#outline",
    "title": "Generalized Linear Models",
    "section": "Outline",
    "text": "Outline\n\n\\[\n\\newcommand{\\mc}[1]{\\mathcal{#1}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\var}{{\\rm Var}} % Variance\n\\newcommand{\\mse}{{\\rm MSE}} % MSE\n\\newcommand{\\bias}{{\\rm Bias}} % MSE\n\\newcommand{\\cov}{{\\rm Cov}} % Covariance\n\\newcommand{\\iid}{\\stackrel{\\rm iid}{\\sim}}\n\\newcommand{\\ind}{\\stackrel{\\rm ind}{\\sim}}\n\\renewcommand{\\choose}[2]{\\binom{#1}{#2}}  % Choose\n\\newcommand{\\chooses}[2]{{}_{#1}C_{#2}}  % Small choose\n\\newcommand{\\cd}{\\stackrel{d}{\\rightarrow}}\n\\newcommand{\\cas}{\\stackrel{a.s.}{\\rightarrow}}\n\\newcommand{\\cp}{\\stackrel{p}{\\rightarrow}}\n\\newcommand{\\bin}{{\\rm Bin}}\n\\newcommand{\\ber}{{\\rm Ber}}\n\\newcommand{\\diag}{{\\rm diag}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\n\n\nClassical Linear Model\n\nOrdinary Least Squares (OLS) Estimation\nMaximum Likelihood (ML) Estimation\nPenalty and Regularization\n\nGeneralized Linear Models\n\nLogistic Regression\nMultinomial Regression\n\nNon-linear Models\n\nGeneralized Additive Models (GAM)\nProjection Pursuit Regression (PPR)"
  },
  {
    "objectID": "slides/02-lm.html#classical-linear-model",
    "href": "slides/02-lm.html#classical-linear-model",
    "title": "Generalized Linear Models",
    "section": "Classical Linear Model",
    "text": "Classical Linear Model\n\nGiven \\(p\\) covariates \\(x_1, \\ldots, x_p\\) and a response variable \\(y\\), the classical linear model assumes that the relationship between the \\(x_i\\)’s and \\(y\\) is linear: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p.\n\\]\nDenote \\(\\boldsymbol{x} = (1, x_{1}, \\ldots, x_{p})^T\\) and \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^T\\), the model can be written as \\[\ny = \\boldsymbol{x}^T\\boldsymbol{\\beta}.\n\\]\nSuppose now we have \\(n\\) samples \\((\\boldsymbol{x}_1, y_1), \\ldots (\\boldsymbol{x}_n, y_n)\\) and we believe that the linear model above is a reasonable approximation of the relationship between the \\(\\boldsymbol{x}_i\\)’s and \\(y_i\\).\nThe goal is to estimate the model parameter \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "slides/02-lm.html#ordinary-least-squares-ols-estimation",
    "href": "slides/02-lm.html#ordinary-least-squares-ols-estimation",
    "title": "Generalized Linear Models",
    "section": "Ordinary Least Squares (OLS) Estimation",
    "text": "Ordinary Least Squares (OLS) Estimation\n\nThe most common method to estimate \\(\\boldsymbol{\\beta}\\) is the ordinary least squares (OLS) estimation, that is, we find \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals: \\[\n\\hat{\\boldsymbol{\\beta}} = \\argmin_{\\boldsymbol{\\beta}} \\sum_{i=1}^n (y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2.\n\\]\nDenoting \\[\n\\boldsymbol{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad\n\\boldsymbol{X} = \\begin{bmatrix} \\boldsymbol{x}_1^T \\\\ \\vdots \\\\ \\boldsymbol{x}_n^T \\end{bmatrix},\n\\] the minimization problem can be written as \\[\n\\hat{\\boldsymbol{\\beta}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#ordinary-least-squares-ols-estimation-1",
    "href": "slides/02-lm.html#ordinary-least-squares-ols-estimation-1",
    "title": "Generalized Linear Models",
    "section": "Ordinary Least Squares (OLS) Estimation",
    "text": "Ordinary Least Squares (OLS) Estimation\n\nLet \\(L(\\boldsymbol{\\beta}) = \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2\\). This is often called the loss function.\nTaking the gradient of \\(L(\\boldsymbol{\\beta})\\) with respect to \\(\\boldsymbol{\\beta}\\) and setting it to zero, we have \\[\n\\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = -2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) = 0.\n\\]\nThe OLS estimation has a closed-form solution: \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n\\]\nTo ensure the existence of the inverse, we need to assume that \\(\\boldsymbol{X}^T\\boldsymbol{X}\\) is invertible, that is, the columns of \\(\\boldsymbol{X}\\) are linearly independent.\nTo verify that \\(\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}}\\) is indeed the minimizer, we need to show that the Hessian of \\(L(\\boldsymbol{\\beta})\\) is positive definite: \\[\n\\nabla^2_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = 2\\boldsymbol{X}^T\\boldsymbol{X} \\succ 0.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#maximum-likelihood-ml-estimation",
    "href": "slides/02-lm.html#maximum-likelihood-ml-estimation",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood (ML) Estimation",
    "text": "Maximum Likelihood (ML) Estimation\n\nAnother way to estimate \\(\\boldsymbol{\\beta}\\) is the maximum likelihood (ML) estimation.\nSuppose that the response variable \\(y\\) is normally distributed with mean \\(\\mu(\\boldsymbol{x}) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\\) and variance \\(\\sigma^2\\): \\[\ny \\mid \\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{x}^T\\boldsymbol{\\beta}, \\sigma^2).\n\\]\nAssuming the samples are i.i.d, the likelihood function is \\[\nL(\\boldsymbol{\\beta}, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right).\n\\]\nThe log-likelihood function is \\[\n\\ell(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\boldsymbol{x}_i^T\\boldsymbol{\\beta})^2 = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2.\n\\]\nThe ML estimation is \\(\\hat{\\boldsymbol{\\beta}}, \\hat{\\sigma}^2 = \\argmax_{\\boldsymbol{\\beta}, \\sigma^2} \\ell(\\boldsymbol{\\beta}, \\sigma^2)\\)."
  },
  {
    "objectID": "slides/02-lm.html#maximum-likelihood-ml-estimation-1",
    "href": "slides/02-lm.html#maximum-likelihood-ml-estimation-1",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood (ML) Estimation",
    "text": "Maximum Likelihood (ML) Estimation\n\nTaking the gradient of \\(\\ell(\\boldsymbol{\\beta}, \\sigma^2)\\) with respect to \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) and setting them to zero, we have \\[\n\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sigma^2} \\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) = 0,\n\\] and \\[\n\\frac{\\partial}{\\partial \\sigma^2} \\ell(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 = 0.\n\\]\nThe ML estimation has a closed-form solution: \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}, \\quad\n\\hat{\\sigma}^2 = \\frac{1}{n}\\|\\boldsymbol{y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}\\|_2^2.\n\\]\nThe MLE of \\(\\boldsymbol{\\beta}\\) is the same as the OLS estimation."
  },
  {
    "objectID": "slides/02-lm.html#ols-v.s.-ml-estimation",
    "href": "slides/02-lm.html#ols-v.s.-ml-estimation",
    "title": "Generalized Linear Models",
    "section": "OLS v.s. ML Estimation",
    "text": "OLS v.s. ML Estimation\n\nCompared to the OLS estimation, the ML estimation requires an additional assumption on the distribution of \\(y\\).\nIn ths case of linear regression, the normality assumption is the most common one.\nAn equivalent way to express the linear regression under the normality assumption is \\[\ny  = \\boldsymbol{x}^T\\boldsymbol{\\beta} + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2).\n\\]\nOne of the advantages of the ML estimation is that it provides a way to estimate the variance of the estimated parameter \\(\\hat{\\boldsymbol{\\beta}}\\): \\[\\begin{align*}\n\\var(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}) & = \\var((\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}) \\\\\n& = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\var(\\boldsymbol{y})\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\\\\n& = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T(\\sigma^2 I)\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\\\\n& = \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-lm.html#useful-properties-of-mle",
    "href": "slides/02-lm.html#useful-properties-of-mle",
    "title": "Generalized Linear Models",
    "section": "Useful Properties of MLE",
    "text": "Useful Properties of MLE\n\nUnder the normality assumption, the MLE has the following properties:\n\nUnbiasedness: \\(\\E(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}) = \\boldsymbol{\\beta}\\).\nNormality: \\(\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1})\\).\nPrediction Intervals: for a given \\(\\boldsymbol{x}^{\\star}\\), the predicted value is \\(y^{\\star} = \\boldsymbol{x}^{\\star T}\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}}\\) and the prediction interval is \\[\ny^{\\star} \\pm t_{n-p-1, 1-\\alpha/2} \\hat{\\sigma} \\sqrt{1 + \\boldsymbol{x}^{\\star T}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{x}^{\\star}}.\n\\]\nWe can also derive the confidence intervals and hypothesis tests for \\(c^T\\boldsymbol{\\beta}\\) for any \\(c\\)."
  },
  {
    "objectID": "slides/02-lm.html#what-if-the-samples-are-not-i.i.d",
    "href": "slides/02-lm.html#what-if-the-samples-are-not-i.i.d",
    "title": "Generalized Linear Models",
    "section": "What if the samples are not i.i.d?",
    "text": "What if the samples are not i.i.d?\n\nIf the samples are not i.i.d, we can model the joint distribution of the samples: \\[\n\\boldsymbol{y} \\mid \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{X}\\boldsymbol{\\beta}, \\sigma^2 W^{-1})\n\\] where \\(W\\) is an \\(n\\times n\\) covariance matrix describing the dependence between the samples.\nConsider the transformation \\(\\widetilde{\\boldsymbol{y}} = W^{1/2}\\boldsymbol{y}\\) and \\(\\widetilde{\\boldsymbol{X}} = W^{1/2}\\boldsymbol{X}\\). The model becomes \\[\n\\widetilde{\\boldsymbol{y}} \\mid \\widetilde{\\boldsymbol{X}} \\sim \\mathcal{N}(\\widetilde{\\boldsymbol{X}}\\boldsymbol{\\beta}, \\sigma^2 I).\n\\]\nTherefore the MLE for \\(\\boldsymbol{\\beta}\\) is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = (\\widetilde{\\boldsymbol{X}}^T\\widetilde{\\boldsymbol{X}})^{-1}\\widetilde{\\boldsymbol{X}}^T\\widetilde{\\boldsymbol{y}} = (\\boldsymbol{X}^TW\\boldsymbol{X})^{-1}\\boldsymbol{X}^TW\\boldsymbol{y},\n\\] which is called the weighted least squares estimation."
  },
  {
    "objectID": "slides/02-lm.html#penalized-likelihood-estimation",
    "href": "slides/02-lm.html#penalized-likelihood-estimation",
    "title": "Generalized Linear Models",
    "section": "Penalized Likelihood Estimation",
    "text": "Penalized Likelihood Estimation\n\nHowever, in practice, the MLE might not be the best choice.\nFor example, when \\(X\\) contains columns that are close to collinear or if the number of covariates \\(p\\) is large, computing \\((X^TX)^{-1}\\) will become numerically unstable.\nOne of the most common ways to address this issue is to add penalization or regularization.\nThe idea is to add a penalty term to the negative log-likelihood function, i.e., \\[\n-\\ell(\\boldsymbol{\\beta}, \\sigma^2) + \\lambda \\cdot \\text{pen}(\\boldsymbol{\\beta}).\n\\]\nThat is, we are looking for the \\(\\boldsymbol{\\beta}\\) that minimizes the negaive log-likelihood and the penalty.\nThe extra term \\(\\lambda\\) is a hyperparameter that controls the trade-off between the likelihood and the penalty."
  },
  {
    "objectID": "slides/02-lm.html#ridge-regression",
    "href": "slides/02-lm.html#ridge-regression",
    "title": "Generalized Linear Models",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nOne of the most common penalization methods is the Ridge regression.\nThe penalty term is the \\(L_2\\) norm of \\(\\boldsymbol{\\beta}\\): \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\|\\boldsymbol{\\beta}\\|_2^2 = \\sum_{j=0}^p \\beta_j^2.\n\\]\nThe Ridge estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2.\n\\]\nLet \\(L_{\\lambda}(\\boldsymbol{\\beta}) = \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2\\) and set the gradient to zero \\[\n\\nabla_{\\boldsymbol{\\beta}} L_{\\lambda}(\\boldsymbol{\\beta}) = -2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) + 2\\lambda \\boldsymbol{\\beta} = 0.\n\\]\nHence the Ridge estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\lambda I)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#ridge-regression-1",
    "href": "slides/02-lm.html#ridge-regression-1",
    "title": "Generalized Linear Models",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nTypically, penalization of the intercept is not desired in Ridge regression so that \\(\\beta_0\\) should be excluded from the penalty term.\nA simple way to achieve this is to center all covariates and the responses so that \\(\\bar{y} = 0\\) and \\(\\bar{\\boldsymbol{x}} = 0\\) which automatically results in \\(\\hat{\\beta}_0 = 0\\).\nA second approach is to use the following penalty term: \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\sum_{j=1}^p \\beta_j^2 = \\boldsymbol{\\beta}^TK\\boldsymbol{\\beta},\n\\] where \\(K = \\text{diag}(0, 1, \\ldots, 1)\\).\nThe Ridge estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{ridge}} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\lambda K)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#ridge-v.s.-ols",
    "href": "slides/02-lm.html#ridge-v.s.-ols",
    "title": "Generalized Linear Models",
    "section": "Ridge v.s. OLS",
    "text": "Ridge v.s. OLS\n\nThe Ridge estimator is biased and the OLS is unbiased.\nHowever, one can show that the Ridge estimator has a smaller variance than the OLS estimator.\nWhen choosing an appropriate hyperparameter \\(\\lambda\\), the Ridge estimator can have a smaller mean squared error (MSE) than the OLS estimator.\nThe Ridge estimator is shrinkage estimator that shrinks the coefficients towards zero (large value of \\(\\lambda\\) yields a stronger shrinkage).\nThe Ridge estimator is particularly useful when the covariates are collinear or when the number of covariates is large.\nThe choice of \\(\\lambda\\) is often done using cross-validation."
  },
  {
    "objectID": "slides/02-lm.html#least-absolute-shrinkage-and-selection-operator-lasso",
    "href": "slides/02-lm.html#least-absolute-shrinkage-and-selection-operator-lasso",
    "title": "Generalized Linear Models",
    "section": "Least Absolute Shrinkage and Selection Operator (LASSO)",
    "text": "Least Absolute Shrinkage and Selection Operator (LASSO)\n\nAnother common penalization method is the least absolute shrinkage and selection operator (LASSO).\nThe penalty term is the \\(L_1\\) norm of \\(\\boldsymbol{\\beta}\\): \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\sum_{j=1}^p |\\beta_j|.\n\\]\nThe LASSO estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{LASSO}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#lasso-estimation",
    "href": "slides/02-lm.html#lasso-estimation",
    "title": "Generalized Linear Models",
    "section": "LASSO Estimation",
    "text": "LASSO Estimation\n\nNote that the objective function of the LASSO estimator is not differentiable, due to the absolute value term.\nNo closed-form solution for the LASSO estimator is available. However, it can be solved using the Least Angle Regression1 or the coordinate descent2 algorithm.\nOne of the key properties of the LASSO estimator is that it produces sparse solutions, i.e., some of the estimated coefficients are exactly zero.\nThis is particularly useful for variable selection, i.e., to identify the important covariates.\n\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. The Annals of Statistics, 32(2), pages 407–499.Friedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), pages 1–22."
  },
  {
    "objectID": "slides/02-lm.html#ridge-v.s.-lasso-l_2-penalty-v.s.-l_1-penalty",
    "href": "slides/02-lm.html#ridge-v.s.-lasso-l_2-penalty-v.s.-l_1-penalty",
    "title": "Generalized Linear Models",
    "section": "Ridge v.s. LASSO (\\(L_2\\) penalty v.s. \\(L_1\\) penalty)",
    "text": "Ridge v.s. LASSO (\\(L_2\\) penalty v.s. \\(L_1\\) penalty)\n\n\n\n\n\n\n\nFigure 3.11 of ESL"
  },
  {
    "objectID": "slides/02-lm.html#elastic-net",
    "href": "slides/02-lm.html#elastic-net",
    "title": "Generalized Linear Models",
    "section": "Elastic-Net",
    "text": "Elastic-Net\n\nThe Elastic-Net1 is a combination of the Ridge and the LASSO.\nThe penalty term is a combination of the \\(L_1\\) and \\(L_2\\) norm of \\(\\boldsymbol{\\beta}\\): \\[\n\\text{pen}(\\boldsymbol{\\beta}) = \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2.\n\\]\nThe Elastic-Net estimator is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{EN}} = \\argmin_{\\boldsymbol{\\beta}} \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2.\n\\]\nThrough the choice of \\(\\lambda_1\\) and \\(\\lambda_2\\), the Elastic-Net can be used to achieve the benefits of both the ridge and the LASSO.\n\nZou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2), pages 301-320."
  },
  {
    "objectID": "slides/02-lm.html#example---sparse-features",
    "href": "slides/02-lm.html#example---sparse-features",
    "title": "Generalized Linear Models",
    "section": "Example - Sparse features",
    "text": "Example - Sparse features\n\nWe generate a synthetic dataset with 50 samples and 10 features.\nOnly 5 out of the 10 features are informative.\nWe fit the linear regression, Ridge, LASSO, and Elastic-Net models to the data.\n\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.datasets import make_regression\n\nX, y, true_coef = make_regression(n_samples = 50, n_features = 10, \n                                  n_informative = 5, noise = 5,\n                                  coef = True, random_state = 42)\n\nlm = LinearRegression().fit(X, y)\nridge = Ridge(alpha=1.0).fit(X, y)\nlasso = Lasso(alpha=1.0).fit(X, y)\nenet = ElasticNet(alpha=1.0, l1_ratio=0.5).fit(X, y)"
  },
  {
    "objectID": "slides/02-lm.html#example---sparse-features-1",
    "href": "slides/02-lm.html#example---sparse-features-1",
    "title": "Generalized Linear Models",
    "section": "Example - Sparse features",
    "text": "Example - Sparse features\n\n\n\n\n\nTrue Coef\nLinear\nRidge\nLASSO\nElastic-Net\n\n\n\n\n57.078\n56.306\n55.626\n55.459\n40.091\n\n\n0.000\n0.173\n0.471\n0.000\n1.685\n\n\n0.000\n-0.185\n0.073\n-0.000\n1.556\n\n\n35.610\n33.877\n33.447\n33.189\n25.617\n\n\n0.000\n0.702\n1.655\n0.000\n8.187\n\n\n60.577\n60.568\n59.158\n59.634\n38.185\n\n\n0.000\n1.586\n1.838\n0.650\n4.251\n\n\n64.592\n64.964\n63.242\n63.704\n37.886\n\n\n0.000\n-0.440\n-0.428\n-0.000\n-1.682\n\n\n98.652\n99.563\n96.871\n98.682\n61.042"
  },
  {
    "objectID": "slides/02-lm.html#beyond-normality",
    "href": "slides/02-lm.html#beyond-normality",
    "title": "Generalized Linear Models",
    "section": "Beyond Normality",
    "text": "Beyond Normality\n\nWhen the response variable is not real-valued, the classical linear model is not appropriate.\nFor example:\n\nBinary responses: \\(y \\in \\{0, 1\\}\\).\nCount data: \\(y \\in \\{0, 1, 2, \\ldots\\}\\).\nMultinomial responses: \\(y \\in \\{1, 2, \\ldots, K\\}\\).\n\nIn these cases, neither the OLS estimation nor the normality assumption is appropriate.\nGeneralized Linear Model (GLM) is a generalization of the classical linear model that allows for non-normal responses.\nThe key is to find a reasonable distribution to model \\(y\\)."
  },
  {
    "objectID": "slides/02-lm.html#binary-responces-logistic-regression",
    "href": "slides/02-lm.html#binary-responces-logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Binary Responces: Logistic regression",
    "text": "Binary Responces: Logistic regression\n\nWhen \\(y\\) is binary, we can use the Bernoulli distribution \\[\nY \\mid \\boldsymbol{x} \\sim \\text{Ber}(p(\\boldsymbol{x})),\n\\]\nThat is, \\(\\P(Y = 1 \\mid \\boldsymbol{x}) = p(\\boldsymbol{x})\\) and \\(\\P(Y = 0 \\mid \\boldsymbol{x}) = 1 - p(\\boldsymbol{x})\\) and the expectation is \\(\\E(Y \\mid \\boldsymbol{x}) = p(\\boldsymbol{x})\\).\nThe logistic regression model assumes \\[\np(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-\\boldsymbol{x}^T\\boldsymbol{\\beta})}.\n\\]\nEquivalently, we can write \\[\n\\log\\left(\\frac{p(\\boldsymbol{x})}{1 - p(\\boldsymbol{x})}\\right) = \\boldsymbol{x}^T\\boldsymbol{\\beta}.\n\\]\nThat is, the log-odds of the event \\(\\{Y = 1\\}\\) is linear in \\(\\boldsymbol{x}\\)."
  },
  {
    "objectID": "slides/02-lm.html#ml-estimation-for-logistic-regression",
    "href": "slides/02-lm.html#ml-estimation-for-logistic-regression",
    "title": "Generalized Linear Models",
    "section": "ML Estimation for Logistic Regression",
    "text": "ML Estimation for Logistic Regression\n\nGiven \\(n\\) samples \\((\\boldsymbol{x}_1, y_1), \\ldots (\\boldsymbol{x}_n, y_n)\\), the likelihood function is \\[\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n p(\\boldsymbol{x}_i)^{y_i} (1 - p(\\boldsymbol{x}_i))^{1 - y_i}.\n\\]\nThe log-likelihood function is \\[\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n y_i \\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)).\n\\]\nHence the MLE of \\(\\boldsymbol{\\beta}\\) is \\[\n\\hat{\\boldsymbol{\\beta}}^{\\text{MLE}} = \\argmax_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}).\n\\]\nThe negative log-likelihood is also called the cross-entropy loss, i.e., maximizing the likelihood is equivalent to minimizing the cross-entropy loss."
  },
  {
    "objectID": "slides/02-lm.html#cross-entropy-loss",
    "href": "slides/02-lm.html#cross-entropy-loss",
    "title": "Generalized Linear Models",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\n\nIn Information Theory, the cross-entropy is defined as \\[\nH(p, q) = -\\sum_{x} p(x) \\log(q(x)) = -\\E_p[\\log(q(X))],\n\\] where \\(p(x)\\) and \\(q(x)\\) are two discrete probability distributions.\nLarge value of cross-entropy indicates that the two distributions are different.\nIn the case of logistic regression, we want to measure the discrepancy between the data \\([y_i, 1-y_i]\\) and the model \\([p(\\boldsymbol{x}_i), 1 - p(\\boldsymbol{x}_i)]\\).\nHence the cross-entropy is \\[\n-\\sum_{i=1}^n y_i \\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)).\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#exponential-family",
    "href": "slides/02-lm.html#exponential-family",
    "title": "Generalized Linear Models",
    "section": "Exponential family",
    "text": "Exponential family\n\nRecall that an exponential family is a family of distributions \\[\nf(x \\mid \\theta) = h(x)\\exp(\\theta^T T(x) - \\psi(\\theta))\n\\] where \\(\\theta \\in \\R^k\\) and \\(T(x) = [T_1(x), \\ldots, T_k(x)]^T\\).\nThe parameter \\(\\theta\\) is called the natural parameter or the canonical parameter and \\(T(x)\\) is the sufficient statistic.\nTwo useful properties (from Bartlett’s identities):\n\n\\(\\E(T(X)) = \\nabla_{\\theta}\\psi(\\theta)\\)\n\\(\\var(T(X)) = \\text{Hess}(\\psi(\\theta)) = \\nabla^2_{\\theta} \\psi(\\theta)\\).\n\nThat is, the relationship between the parameter \\(\\theta\\) and the expectation \\(\\E(T(X))\\) determined by \\(\\nabla \\psi\\)."
  },
  {
    "objectID": "slides/02-lm.html#examples",
    "href": "slides/02-lm.html#examples",
    "title": "Generalized Linear Models",
    "section": "Examples",
    "text": "Examples\n\nNormal disribution: \\[\nf(x \\mid \\mu, \\sigma^2) = \\exp\\left(-\\frac{1}{2\\sigma^2} x^2 + \\frac{\\mu}{\\sigma^2}x - \\frac{\\mu^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\\right), \\quad x \\in \\R\n\\]\n\n\\(\\theta = \\left(-\\frac{1}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}\\right)\\), \\(T(x) = (-x^2, x)\\), \\(\\psi(\\theta) = -\\frac{\\theta_2^2}{4\\theta_1} - \\frac{1}{2}\\log\\left(-\\frac{\\theta_1}{\\pi}\\right)= \\frac{\\mu^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\\)\n\nBernoulli distribution: \\[\nf(x \\mid p) = p^x(1-p)^{1-x} = \\exp\\left(x\\log\\frac{p}{1-p} + \\log(1-p)\\right), \\quad x \\in \\{0, 1\\}\n\\]\n\n\\(\\theta = \\log\\frac{p}{1-p}\\), \\(T(x) = x\\), \\(\\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta})\\)\n\nPoisson distribution: \\[\nf(x \\mid \\lambda) = \\frac{\\lambda^x e^{\\lambda}}{x!}= \\frac{1}{x!}\\exp(x\\log\\lambda - \\lambda), \\quad x = 0, 1, 2, \\ldots\n\\]\n\n\\(\\theta = \\log\\lambda\\), \\(T(x) = x\\), \\(\\psi(\\theta) = \\exp(\\theta) = \\lambda\\)"
  },
  {
    "objectID": "slides/02-lm.html#generalized-linear-model-glm",
    "href": "slides/02-lm.html#generalized-linear-model-glm",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Model (GLM)",
    "text": "Generalized Linear Model (GLM)\n\nLet \\(Y\\) be univariate, \\(\\boldsymbol{x} \\in \\R^p\\), and \\(\\boldsymbol{\\beta} \\in \\R^p\\).\nA GLM is assuming \\(Y \\mid \\boldsymbol{x} \\sim F_{\\theta}\\), where \\(\\theta = \\boldsymbol{x}^T\\boldsymbol{\\beta}\\) and \\(F_\\theta\\) has the density function \\[\nf(y \\mid \\theta) = h(y)\\exp(\\theta\\cdot y - \\psi(\\theta)).\n\\]\nTherefore \\[\\begin{align*}\n\\E(Y \\mid \\boldsymbol{x}) & = \\frac{d}{d\\theta}\\psi(\\theta) = \\psi^{\\prime}(\\boldsymbol{x}^T\\boldsymbol{\\beta}).\n\\end{align*}\\]\nEquivalently, \\[\ng(\\E(Y \\mid \\boldsymbol{x})) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n\\] where \\(g\\) is the inverse of \\(\\psi^{\\prime}\\).\nThe function \\(g\\) is called the link function."
  },
  {
    "objectID": "slides/02-lm.html#logistic-regression",
    "href": "slides/02-lm.html#logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor Bernoulli distributions, we have \\[\n\\theta = \\log\\frac{p}{1-p}, \\quad \\psi(\\theta) = -\\log(1-p) = \\log(1 + e^{\\theta}).\n\\]\nThus, \\(\\psi^{\\prime}(\\theta) = \\frac{e^{\\theta}}{1 + e^{\\theta}}\\) and \\(g(p) = (\\psi^{\\prime})^{-1}(p) = \\log\\frac{p}{1-p}\\). \\(\\psi^{\\prime}\\) is called the logistic function and \\(g\\) is called the logit function.\nPutting altogether, we have \\[\ng(\\E(Y \\mid \\boldsymbol{x})) = \\log\\left(\\frac{p(\\boldsymbol{x})}{1 - p(\\boldsymbol{x})}\\right) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n\\] or equivalently \\[\n\\P(Y = 1 \\mid \\boldsymbol{x}) = \\psi^{\\prime}(\\boldsymbol{x}^T\\boldsymbol{\\beta}) = \\frac{\\exp(\\boldsymbol{x}^T\\boldsymbol{\\beta})}{1 + \\exp(\\boldsymbol{x}^T\\boldsymbol{\\beta})}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#remarks",
    "href": "slides/02-lm.html#remarks",
    "title": "Generalized Linear Models",
    "section": "Remarks",
    "text": "Remarks\n\nThe link function \\(g = (\\psi^{\\prime})^{-1}\\) is sometimes called the canonical link function, since it is derived from the canonical representation of an exponential family.\nAll we need for a link function is that it is invertible and matches the range of \\(\\E(Y \\mid \\boldsymbol{x})\\) and \\(\\boldsymbol{x}^T\\boldsymbol{\\beta}\\).\nFor example, in the Bernoulli linear model, we could have used the probit link function \\[\ng(u) = \\Phi^{-1}(u): [0, 1] \\to \\R\n\\] where \\(\\Phi\\) is the CDF of the standard normal distribution.\nThis is called the probit regression."
  },
  {
    "objectID": "slides/02-lm.html#multinomial-regression",
    "href": "slides/02-lm.html#multinomial-regression",
    "title": "Generalized Linear Models",
    "section": "Multinomial Regression",
    "text": "Multinomial Regression\n\nMultinomial regression is a generalization of Logistic regression to categorical variables with more than two categories.\nSuppose \\(Y\\) is a categorical variable with \\(K\\) categories, \\(Y \\in \\{1, 2, \\ldots, K\\}\\).\nA more useful representation is to use the one-hot encoding: \\[\nY = [0, 0, \\ldots, 1, \\ldots, 0]^T\n\\] where the \\(k\\)-th element is 1 and the rest are 0.\nThe multinomial regression model assumes \\[\nY \\mid \\boldsymbol{x} \\sim \\text{Multi}(1, [p_1(\\boldsymbol{x}), p_2(\\boldsymbol{x}), \\ldots, p_K(\\boldsymbol{x})]^T)\n\\] where \\(p_k(\\boldsymbol{x}) = \\P(Y = 1_k \\mid \\boldsymbol{x})\\) and \\(1_k\\) is the one-hot encoding of the \\(k\\)-th category."
  },
  {
    "objectID": "slides/02-lm.html#multinomial-distribution",
    "href": "slides/02-lm.html#multinomial-distribution",
    "title": "Generalized Linear Models",
    "section": "Multinomial Distribution",
    "text": "Multinomial Distribution\n\nThe probability mass function of the \\(\\text{Multi}(m,p)\\) is \\[\nf(x \\mid p) = \\frac{m!}{x_1!\\cdots x_K!}\\prod_{k=1}^K p_k^{x_k} = \\frac{m!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^K x_k\\log p_k\\right)\n\\] where \\(x = [x_1, x_2, \\ldots, x_K]^T\\), \\(\\sum_{k=1}^K x_k = m\\), and \\(\\sum_{k=1}^K p_k = 1\\).\nNote that \\(p_K = 1 - p_1 - \\ldots - p_{K-1}\\) and therefore \\[\\begin{align*}\nf(x \\mid p) & = \\frac{m!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^{K-1} x_k\\log p_k + \\left(m - \\sum_{k=1}^{K-1} x_k\\right)\\log(1 - p_1 - \\ldots - p_{K-1})\\right)\\\\\n& = \\frac{m!}{x_1!\\cdots x_K!}\\exp\\left(\\sum_{k=1}^{K-1} x_k\\log\\frac{p_k}{p_K} + m\\log(1 - p_1 - \\ldots - p_{K-1})\\right).\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-lm.html#softmax-function",
    "href": "slides/02-lm.html#softmax-function",
    "title": "Generalized Linear Models",
    "section": "Softmax Function",
    "text": "Softmax Function\n\nThe canonical parameter is \\(\\theta = [\\log\\frac{p_1}{p_K}, \\ldots, \\log\\frac{p_{K-1}}{p_K}]^T\\) and therefore \\(p_i = p_K\\exp(\\theta_i)\\).\nUsing the relationship \\(p_K = 1 - \\sum_{k=1}^{K-1} p_k\\), we have \\[\np_K = 1 - p_K\\sum_{k=1}^{K-1} \\exp(\\theta_k) \\quad \\Rightarrow \\quad p_K = \\frac{1}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}.\n\\]\nHence (assume \\(m = 1\\) for simplicity) \\[\\begin{align*}\n\\psi(\\theta) & = - \\log(1 - p_1 - \\ldots - p_{K-1})\n= - \\log(1 - p_Ke^{\\theta_1} - \\ldots - p_Ke^{\\theta_{K-1}})\\\\\n& = - \\log\\left(1 - \\frac{\\sum_{k=1}^{K-1} \\exp(\\theta_k)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}\\right)\n= \\log\\left(1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)\\right).\n\\end{align*}\\]\nTaking the derivative, we have the softmax function: \\[\n\\nabla_{\\theta}\\psi(\\theta) = \\left[\\frac{\\exp(\\theta_1)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}, \\ldots, \\frac{\\exp(\\theta_{K-1})}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_k)}\\right].\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#multinomial-regression-1",
    "href": "slides/02-lm.html#multinomial-regression-1",
    "title": "Generalized Linear Models",
    "section": "Multinomial Regression",
    "text": "Multinomial Regression\n\nThe multinomial regression model is given by \\[\\begin{align*}\n   \\theta_i & = \\boldsymbol{x}^T\\boldsymbol{\\beta}_i, \\\\\n   p_i(\\boldsymbol{x}) & = \\frac{\\exp(\\theta_i)}{1 + \\sum_{k=1}^{K-1} \\exp(\\theta_i)}, \\quad i = 1, 2, \\ldots, K-1,\n\\end{align*}\\] where \\(\\boldsymbol{\\beta}_i \\in \\R^p\\).\nIn fact, a more common representation is \\[\\begin{align*}\n   \\tilde{\\theta}_i & = \\boldsymbol{x}^T\\tilde{\\boldsymbol{\\beta}}_i, \\\\\n   p_i(\\boldsymbol{x}) & = \\frac{\\exp(\\tilde{\\theta}_i)}{\\sum_{k=1}^{K} \\exp(\\tilde{\\theta}_i)}, \\quad i = 1, 2, \\ldots, K.\n\\end{align*}\\]\nThe equivalence is due to the transformation \\(\\theta_i = \\tilde{\\theta}_i - \\tilde{\\theta}_K\\) and \\(\\boldsymbol{\\beta}_i = \\tilde{\\boldsymbol{\\beta}}_i - \\tilde{\\boldsymbol{\\beta}}_K\\). We can also write \\[\n[p_1(\\boldsymbol{x}), \\ldots, p_K(\\boldsymbol{x})] = \\texttt{softmax}(\\boldsymbol{x}^T\\boldsymbol{\\beta}_1, \\ldots, \\boldsymbol{x}^T\\boldsymbol{\\beta}_K).\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#quick-summary",
    "href": "slides/02-lm.html#quick-summary",
    "title": "Generalized Linear Models",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nA GLM is \\[\n   g(\\E(Y \\mid X = x)) = x^T\\beta \\Leftrightarrow \\E(Y \\mid X = x) = g^{-1}(x^T\\beta).\n\\]\nThe link function \\(g\\) connects the conditional expectation and the linear predictor and is chosen based on the distribution of \\(Y\\).\nExamples:\n\nLogistic regression: \\(g(p) = \\log\\left(\\frac{p}{1-p}\\right)\\), \\(g^{-1}(x) = \\frac{1}{1+e^{-x}}\\).\nLinear regression: \\(g(\\mu) = \\mu\\).\nMultinomial regression: softmax function.\nThere are other choices and the above are called the canonical link functions."
  },
  {
    "objectID": "slides/02-lm.html#beyond-linearity",
    "href": "slides/02-lm.html#beyond-linearity",
    "title": "Generalized Linear Models",
    "section": "Beyond Linearity",
    "text": "Beyond Linearity\n\nUp to now, we have assumed that a linear relationship between the features and the (transformed) conditional expectation: \\[\n   g(\\E(Y \\mid \\boldsymbol{x})) = \\boldsymbol{x}^T\\boldsymbol{\\beta}\n\\]\nHowever, this is a strong assumption and may not be appropriate in many cases.\nTo remove this assumption, we can consider \\[\n   g(\\E(Y \\mid \\boldsymbol{x})) = f(\\boldsymbol{x})\n\\] where \\(f: \\R^p \\to \\R\\) is an unknown function.\nThe problem is now to estimate the function \\(f\\).\nDepending on the restrictions on \\(f\\), we can use different methods to estimate \\(f\\)."
  },
  {
    "objectID": "slides/02-lm.html#generalized-additive-models-gam",
    "href": "slides/02-lm.html#generalized-additive-models-gam",
    "title": "Generalized Linear Models",
    "section": "Generalized Additive Models (GAM)",
    "text": "Generalized Additive Models (GAM)\n\nAn additive model assumes that the unknown function \\(f\\) is a sum of univariate functions: \\[\nf(\\boldsymbol{x}) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p).\n\\]\nTherefore, the model is \\[\ng(\\E(Y \\mid \\boldsymbol{x})) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p).\n\\]\nThe functions \\(f_j: \\R \\to \\R\\) are unknown and need to be estimated.\nNonparametric methods can be used to estimate the functions \\(f_j\\), for example, kernel smoothing, splines, etc.\nThe GAM is a generalization of the linear model that allows for non-linear relationships between the features and the conditional expectation."
  },
  {
    "objectID": "slides/02-lm.html#projection-pursuit-regression-ppr",
    "href": "slides/02-lm.html#projection-pursuit-regression-ppr",
    "title": "Generalized Linear Models",
    "section": "Projection Pursuit Regression (PPR)",
    "text": "Projection Pursuit Regression (PPR)\n\nThe projection pursuit regression (PPR)1 model assumes: \\[\nf(\\boldsymbol{x}) = \\sum_{m=1}^M f_m(\\boldsymbol{x}^T\\boldsymbol{\\omega}_m),\n\\] where \\(\\boldsymbol{\\omega}_m \\in \\R^p\\) are unknown unit vectors and \\(f_m: \\R \\to \\R\\) are unknown functions.\nThe scalar variable \\(V_m = \\boldsymbol{x}^T\\boldsymbol{\\omega}_m\\) is the projection of \\(\\boldsymbol{x}\\) onto the unit vector \\(\\boldsymbol{\\omega}_m\\), and we seek \\(\\boldsymbol{\\omega}_m\\) so that the model fits well, hence the name “projection pursuit.”\nIf \\(M\\) is taken arbitrarily large, for appropriate choice of \\(f_m\\) the PPR model can approximate any continuous function in \\(\\R^p\\) arbitrarily well, i.e., the PPR is a universal approximator.\nHowever, this model is not widely used due to the difficulty in estimating the functions \\(f_m\\).\n\nFriedman, J. H., & Tukey, J. W. (1974). A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on computers, 100(9), 881-890."
  },
  {
    "objectID": "slides/02-lm.html#the-log-likelihood-function",
    "href": "slides/02-lm.html#the-log-likelihood-function",
    "title": "Generalized Linear Models",
    "section": "The log-likelihood function",
    "text": "The log-likelihood function\nIn order to find the MLE, we need to simplify the log-likelihood function: \\[\\begin{align*}\n\\ell(\\boldsymbol{\\beta}) & = \\log \\left(\\prod_{i=1}^n p(\\boldsymbol{x}_i)^{y_i}(1-p(\\boldsymbol{x}_i))^{1-y_i}\\right) \\\\\n& = \\sum_{i=1}^n y_i \\log p(\\boldsymbol{x}_i) + (1-y_i)\\log(1-p(\\boldsymbol{x}_i))\\\\\n& = \\sum_{i=1}^n \\left[y_i \\log \\left(\\frac{p(\\boldsymbol{x}_i)}{1-p(\\boldsymbol{x}_i)}\\right) + \\log(1-p(\\boldsymbol{x}_i))\\right]\\\\\n& = \\sum_{i=1}^n \\left[y_i\\boldsymbol{x}_i^T\\boldsymbol{\\beta} - \\log(1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}))\\right].\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-lm.html#gradient-and-hessian",
    "href": "slides/02-lm.html#gradient-and-hessian",
    "title": "Generalized Linear Models",
    "section": "Gradient and Hessian",
    "text": "Gradient and Hessian\nNow we compute the gradient and the Hessian of the log-likelihood function:\n\\[\\begin{align*}\n    \\nabla \\ell(\\boldsymbol{\\beta}) & = \\sum_{i=1}^n \\left[y_i \\boldsymbol{x}_i - \\frac{\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})}{1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})}\\boldsymbol{x}_i\\right] = \\sum_{i=1}^n (y_i - p(\\boldsymbol{x}_i))\\boldsymbol{x}_i\\\\\n    & = X^T(\\boldsymbol{y}-\\mathbf{p})\\\\\n    \\nabla^2 \\ell(\\boldsymbol{\\beta}) & = -\\sum_{i=1}^n p(\\boldsymbol{x}_i)(1-p(\\boldsymbol{x}_i))\\boldsymbol{x}_i\\boldsymbol{x}_i^T = -X^TWX\n\\end{align*}\\] where \\[\n\\mathbf{p} = [p(\\boldsymbol{x}_1), \\ldots, p(\\boldsymbol{x}_n)]^T, \\quad W= \\diag(\\mathbf{p})\\diag(1-\\mathbf{p}), \\quad X = \\begin{bmatrix} \\boldsymbol{x}_1^T \\\\ \\vdots \\\\ \\boldsymbol{x}_n^T \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "slides/02-lm.html#iteratively-re-weighted-least-squares-irwls",
    "href": "slides/02-lm.html#iteratively-re-weighted-least-squares-irwls",
    "title": "Generalized Linear Models",
    "section": "Iteratively Re-Weighted Least Squares (IRWLS)",
    "text": "Iteratively Re-Weighted Least Squares (IRWLS)\nThere is no analytic solution for the MLE of the logistic regression. However, the Newton-Raphson method can be used to find the MLE. The Newton-Raphson method is an iterative method that updates the parameter \\(\\boldsymbol{\\beta}\\) as follows:\n\\[\\begin{align*}\n\\boldsymbol{\\beta}^{(t+1)} & = \\boldsymbol{\\beta}^{(t)} - \\left[\\nabla^2 \\ell(\\boldsymbol{\\beta}^{(t)})\\right]^{-1} \\nabla \\ell\\left(\\boldsymbol{\\beta}^{(t)}\\right) \\\\\n& = \\boldsymbol{\\beta}^{(t)}+\\left(X^T W^{(t)}X\\right)^{-1} X^T\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right) \\\\\n& = \\left(X^T W^{(t)} X\\right)^{-1} X^T W^{(t)}\\left[X \\boldsymbol{\\beta}^{(t)}+\\left(W^{(t)}\\right)^{-1}\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right)\\right] \\\\\n& = \\left(X^T W^{(t)} X\\right)^{-1} X^T W^{(t)} \\mathbf{z}^{(t)}\n\\end{align*}\\] where \\[\\begin{align*}\n\\mathbf{z}^{(t)} & =X \\boldsymbol{\\beta}^{(t)}+\\left(W^{(t)}\\right)^{-1}\\left(\\boldsymbol{y}-\\mathbf{p}^{(t)}\\right), \\quad \\mathbf{p}^{(t)} = [p^{(t)}(\\boldsymbol{x}_1), \\ldots, p^{(t)}(\\boldsymbol{x}_n)]^T\\\\\np^{(t)}(\\boldsymbol{x}_i) & = \\frac{\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}^{(t)})}{1+\\exp(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}^{(t)})}, \\quad\nW^{(t)} = \\diag(\\mathbf{p}^{(t)})\\diag(1-\\mathbf{p}^{(t)}).\n\\end{align*}\\]\n\n\n\nHome"
  }
]